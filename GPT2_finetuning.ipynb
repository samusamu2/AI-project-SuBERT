{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "129d7e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-14 16:10:29.664466: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747239030.641140    3700 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747239030.913912    3700 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747239033.485620    3700 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747239033.485652    3700 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747239033.485655    3700 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747239033.485657    3700 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-14 16:10:33.746467: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "model_name = 'gpt2'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f03d33c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modello caricato su: cuda\n",
      "Generazione del testo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testo Generato 1 ---\n",
      "Once upon a time, in a land far, far away, the world was not so much as one of those things that is called \"the earth.\" It had been there for thousands of years. And now it has come to pass; and we are all living on this planet with our own hands!\n",
      "The Earth's surface temperature rose by about 1 degree Celsius (3 degrees Fahrenheit) during last century alone—a record high since records began being made at least 20 million years ago. The average annual increase over these past two centuries would have taken place only if temperatures were kept constant throughout history: today, they're just below zero or even above 0°C/century — which means no warming whatsoever from human activity until 2100. But what happens\n"
     ]
    }
   ],
   "source": [
    "# Test model is working\n",
    "\n",
    "prompt_text = \"Once upon a time, in a land far, far away,\"\n",
    "\n",
    "# Imposta il pad_token se non è definito\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "# --- Device Configuration (GPU o CPU) ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(f\"Modello caricato su: {device}\")\n",
    "\n",
    "# --- Tokenizzazione dell'input ---\n",
    "input_ids = tokenizer.encode(prompt_text, return_tensors='pt').to(device)\n",
    "\n",
    "# --- Generazione del testo ---\n",
    "print(\"Generazione del testo...\")\n",
    "try:\n",
    "    output_sequences = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=150,\n",
    "        temperature=0.7,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        repetition_penalty=1.1,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    # --- Decodifica e Stampa ---\n",
    "    for i, generated_sequence in enumerate(output_sequences):\n",
    "        text = tokenizer.decode(generated_sequence, skip_special_tokens=True)\n",
    "        print(f\"\\n--- Testo Generato {i+1} ---\")\n",
    "        print(text)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Errore durante la generazione del testo: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adba3500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1905 formatted examples.\n",
      "Mean length of the texts: 103.45669291338582 words\n",
      "Percentage of texts longer than 256 words: 92.38845144356955%\n",
      "\n",
      "Example formatted text:\n",
      "Sumerian:  1(u) la₂ 1(diš) udu u₄ 2(u) 8(diš)-kam ki ab-ba-sa₆-ga-ta na-lu₅ i₃-dab₅   iti <unk> bi₂-gu₇ mu en-unu₆-gal {d}inana unu{ki}ga ba-hun  1(u) la₂ 1(diš)\n",
      "English: 9 rams, 28th day, from Abba-saga, Nalu accepted; month: “ubi-feast,” year: “Enunugal of Inanna of Uruk was installed;” (total:) 9 (rams).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# --- 2. Load and Prepare Your Dataset ---\n",
    "# Assume you have your data as two lists: `sumerian_texts` and `english_translations`\n",
    "# Example:\n",
    "# sumerian_texts = [\"transliteration 1\", \"transliteration 2\", ...]\n",
    "# english_translations = [\"translation 1\", \"translation 2\", ...]\n",
    "\n",
    "train_data = pd.read_csv('datasets/SumTablets_English_train.csv')\n",
    "test_data = pd.read_csv('datasets/SumTablets_English_train.csv')\n",
    "\n",
    "# Format the data for GPT-2:\n",
    "# We'll combine Sumerian and English with a separator.\n",
    "# GPT-2 will learn to generate the English part after seeing \"English: \".\n",
    "# The <|endoftext|> token is GPT-2's standard end-of-sequence token.\n",
    "formatted_texts = []\n",
    "for index, row in train_data.iterrows():\n",
    "    sumerian_texts = row['transliteration']\n",
    "    english_translations = row['translation']\n",
    "    if isinstance(sumerian_texts, str) and isinstance(english_translations, str):\n",
    "        sumerian_texts = sumerian_texts.replace('\\n', ' ')\n",
    "        english_translations = english_translations.replace('\\n', ' ')\n",
    "        formatted_texts.append(f\"Sumerian: {sumerian_texts}\\nEnglish: {english_translations}\")\n",
    "print(f\"Loaded {len(formatted_texts)} formatted examples.\")\n",
    "\n",
    "lengths = [len(text.split()) for text in formatted_texts]\n",
    "mean_length = np.mean(lengths)\n",
    "print(f\"Mean length of the texts: {mean_length} words\")\n",
    "print(f\"Percentage of texts longer than 256 words: {(1 - np.count_nonzero(np.array(lengths) > 256)/len(lengths)) * 100}%\")\n",
    "\n",
    "\n",
    "if formatted_texts:\n",
    "    print(f\"\\nExample formatted text:\\n{formatted_texts[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "170cf36a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set tokenizer.pad_token to tokenizer.eos_token (<|endoftext|>)\n",
      "Split dataset into 1714 training samples and 191 validation samples.\n",
      "Set model.config.pad_token_id to 50256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3700/3751395191.py:134: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fine-tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4280' max='4280' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4280/4280 09:48, Epoch 19/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.947600</td>\n",
       "      <td>1.719530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.462900</td>\n",
       "      <td>1.376470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.410300</td>\n",
       "      <td>1.232802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.111700</td>\n",
       "      <td>1.154847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.081500</td>\n",
       "      <td>1.111231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.993400</td>\n",
       "      <td>1.084200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.061500</td>\n",
       "      <td>1.062282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.969400</td>\n",
       "      <td>1.046695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.896900</td>\n",
       "      <td>1.036477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.998600</td>\n",
       "      <td>1.029258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.772000</td>\n",
       "      <td>1.026710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.885300</td>\n",
       "      <td>1.022487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.856900</td>\n",
       "      <td>1.023207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.779200</td>\n",
       "      <td>1.021312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.804000</td>\n",
       "      <td>1.026785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.829600</td>\n",
       "      <td>1.023736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.738700</td>\n",
       "      <td>1.026905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.722400</td>\n",
       "      <td>1.027287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.745800</td>\n",
       "      <td>1.032367</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning completed.\n",
      "Saving model to ./sumerian_gpt2_finetuned\n",
      "Model and tokenizer saved to ./sumerian_gpt2_finetuned\n",
      "\n",
      "--- Inference Example ---\n",
      "Prompt for generation: 'Sumerian: dingir inana za-me-en English:'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sumerian Input: dingir inana za-me-en\n",
      "Generated English: Basket(-of)-tablets?: xxxxxx are here. Year following “xox₆ was destroyed” (Amar.-Suen 7). Conveyor: ... . Month(s) :  ox, barley rations of the year; month \"Oxen.\" Ur Murguur received. Rushesdam erected. Seal(ed tablet ARAD-)Damascus7th day completed. Sealed tablets ADAR-(?). Total: 417 years. Scribe and administrator of Apisal temple administrators. Basalt king household chief responsible for distribution to him. URU@gula {d}šara₄ Šulgi{ki}haverum scribes 1/2 cubits Willow seed grain loan from Dadaga did Sumer receive 12 months extra A campaign by Sasanian troops against Samarudamar took place that is known as the Great Stele Wall Simanjan built 2nd wall between us and Zabala The Amarna priestess Agadei fashioned a shield with bronze shekels It has been said before but I am not speaking it out because there exists no god among men like me In order protect my people\n",
      "\n",
      "Script finished.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from transformers import (\n",
    "    GPT2Tokenizer,\n",
    "    GPT2LMHeadModel,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "import pandas as pd\n",
    "\n",
    "# --- 1. Configuration & Parameters ---\n",
    "# Model choice: you can start with 'gpt2' and try 'gpt2-medium' if you have more resources\n",
    "MODEL_NAME = 'gpt2'\n",
    "OUTPUT_DIR = './sumerian_gpt2_finetuned' # Directory to save the fine-tuned model\n",
    "LOG_DIR = './logs'                     # Directory for training logs\n",
    "\n",
    "# Training hyperparameters (adjust these based on your dataset size and resources)\n",
    "NUM_EPOCHS = 20                        # Number of training epochs\n",
    "BATCH_SIZE_PER_DEVICE = 4              # Batch size for training and evaluation (adjust based on GPU memory)\n",
    "LEARNING_RATE = 3e-5                   # Learning rate\n",
    "WARMUP_STEPS = 200                     # Number of warmup steps for learning rate scheduler\n",
    "WEIGHT_DECAY = 0.01                    # Weight decay\n",
    "MAX_LENGTH = 256                       # Maximum sequence length for tokenizer (adjust based on your data)\n",
    "TRAIN_VALID_SPLIT = 0.1                # Proportion of data to use for validation\n",
    "\n",
    "\n",
    "# --- 3. Initialize Tokenizer ---\n",
    "# Load GPT-2 tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# GPT-2 doesn't have a pad token by default. We'll use the eos_token as the pad_token.\n",
    "# This is a common practice.\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(f\"Set tokenizer.pad_token to tokenizer.eos_token ({tokenizer.eos_token})\")\n",
    "\n",
    "# --- 4. Create a PyTorch Dataset ---\n",
    "class SumerianEnglishDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = texts\n",
    "        self.max_length = max_length\n",
    "        self.encodings = []\n",
    "        for text in texts:\n",
    "            # Tokenize the combined text\n",
    "            # truncation=True ensures that sequences longer than max_length are cut.\n",
    "            # padding='max_length' pads shorter sequences to max_length.\n",
    "            # return_tensors='pt' returns PyTorch tensors.\n",
    "            encoding = self.tokenizer(\n",
    "                text,\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                padding=\"max_length\", # Ensure all sequences have the same length for batching\n",
    "                return_attention_mask=True,\n",
    "                return_tensors='pt'  # Explicitly specify to return PyTorch tensors\n",
    "            )\n",
    "            # For language modeling, the 'labels' are typically the same as 'input_ids'.\n",
    "            # The model will learn to predict the next token.\n",
    "            # The DataCollatorForLanguageModeling will handle shifting labels for us.\n",
    "            self.encodings.append({\n",
    "                \"input_ids\": encoding[\"input_ids\"].squeeze(), # Remove batch dimension if present\n",
    "                \"attention_mask\": encoding[\"attention_mask\"].squeeze()\n",
    "            })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.encodings[idx]\n",
    "        # The labels are the input_ids themselves for language modeling.\n",
    "        # The model is trained to predict the next token in the sequence.\n",
    "        # The DataCollatorForLanguageModeling will shift them appropriately.\n",
    "        return {\"input_ids\": item[\"input_ids\"], \"attention_mask\": item[\"attention_mask\"], \"labels\": item[\"input_ids\"].clone()}\n",
    "\n",
    "# Create the full dataset\n",
    "full_dataset = SumerianEnglishDataset(formatted_texts, tokenizer, MAX_LENGTH)\n",
    "\n",
    "# Split into training and validation sets\n",
    "if TRAIN_VALID_SPLIT > 0:\n",
    "    num_train = int((1 - TRAIN_VALID_SPLIT) * len(full_dataset))\n",
    "    num_valid = len(full_dataset) - num_train\n",
    "    train_dataset, eval_dataset = random_split(full_dataset, [num_train, num_valid])\n",
    "    print(f\"Split dataset into {len(train_dataset)} training samples and {len(eval_dataset)} validation samples.\")\n",
    "else:\n",
    "    train_dataset = full_dataset\n",
    "    eval_dataset = None # No validation\n",
    "    print(f\"Using all {len(train_dataset)} samples for training. No validation set.\")\n",
    "\n",
    "\n",
    "# --- 5. Initialize Model ---\n",
    "# Load GPT-2 model with a language modeling head\n",
    "# model = GPT2LMHeadModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Resize token embeddings if you added special tokens (not strictly necessary here as we used eos_token as pad_token)\n",
    "# model.resize_token_embeddings(len(tokenizer)) # Uncomment if you explicitly added new tokens\n",
    "\n",
    "# Set the pad_token_id in the model configuration (important for generation and padding)\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "print(f\"Set model.config.pad_token_id to {tokenizer.pad_token_id}\")\n",
    "\n",
    "\n",
    "# --- 6. Data Collator ---\n",
    "# The DataCollatorForLanguageModeling will automatically create batches and\n",
    "# shift the input_ids to create labels for causal language modeling (predicting the next token).\n",
    "# It also handles padding. `mlm=False` means we are doing Causal Language Modeling (CLM), not Masked Language Modeling (MLM).\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # Causal Language Modeling for GPT-2\n",
    ")\n",
    "\n",
    "# --- 7. Define Training Arguments ---\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,                  # Directory to save model checkpoints and outputs\n",
    "    num_train_epochs=NUM_EPOCHS,            # Total number of training epochs\n",
    "    per_device_train_batch_size=BATCH_SIZE_PER_DEVICE, # Batch size per device during training\n",
    "    per_device_eval_batch_size=BATCH_SIZE_PER_DEVICE,  # Batch size for evaluation\n",
    "    warmup_steps=WARMUP_STEPS,              # Number of warmup steps for learning rate scheduler\n",
    "    weight_decay=WEIGHT_DECAY,              # Strength of weight decay\n",
    "    logging_dir=LOG_DIR,                    # Directory for storing logs\n",
    "    logging_steps=10,                       # Log every X updates steps\n",
    "    eval_strategy=\"epoch\" if eval_dataset else \"no\", # Evaluate at the end of each epoch if eval_dataset exists\n",
    "    save_strategy=\"epoch\",                  # Save a checkpoint at the end of each epoch\n",
    "    load_best_model_at_end=True if eval_dataset else False, # Load the best model found during training (based on eval loss)\n",
    "    fp16=torch.cuda.is_available(),         # Use 16-bit (mixed) precision training if a GPU is available\n",
    "    # report_to=\"tensorboard\",              # You can integrate with TensorBoard, WandB, etc.\n",
    "    save_total_limit=2,                     # Limit the total amount of checkpoints. Deletes the older checkpoints.\n",
    "    gradient_accumulation_steps=2,          # Gradient accumulation steps (if you want to simulate larger batch sizes)\n",
    "    lr_scheduler_type=\"linear\",             # Learning rate scheduler type\n",
    ")\n",
    "\n",
    "# --- 8. Initialize Trainer ---\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# --- 9. Start Fine-tuning ---\n",
    "print(\"Starting fine-tuning...\")\n",
    "try:\n",
    "    trainer.train()\n",
    "    print(\"Fine-tuning completed.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during training: {e}\")\n",
    "    # Potentially save the current state if an error occurs mid-training\n",
    "    # model.save_pretrained(f\"{OUTPUT_DIR}_interrupted\")\n",
    "    # tokenizer.save_pretrained(f\"{OUTPUT_DIR}_interrupted\")\n",
    "    # print(f\"Interrupted model saved to {OUTPUT_DIR}_interrupted\")\n",
    "    raise e\n",
    "\n",
    "\n",
    "# --- 10. Save the Fine-tuned Model and Tokenizer ---\n",
    "print(f\"Saving model to {OUTPUT_DIR}\")\n",
    "trainer.save_model(OUTPUT_DIR) # Saves the model state_dict and configuration\n",
    "tokenizer.save_pretrained(OUTPUT_DIR) # Saves the tokenizer\n",
    "print(f\"Model and tokenizer saved to {OUTPUT_DIR}\")\n",
    "\n",
    "\n",
    "# --- 11. Inference Example (How to use the fine-tuned model) ---\n",
    "print(\"\\n--- Inference Example ---\")\n",
    "# Load the fine-tuned model and tokenizer\n",
    "fine_tuned_model = GPT2LMHeadModel.from_pretrained(OUTPUT_DIR)\n",
    "fine_tuned_tokenizer = GPT2Tokenizer.from_pretrained(OUTPUT_DIR)\n",
    "\n",
    "# Ensure the pad token is set for the loaded tokenizer (it should be saved, but good to double check)\n",
    "if fine_tuned_tokenizer.pad_token is None:\n",
    "    fine_tuned_tokenizer.pad_token = fine_tuned_tokenizer.eos_token\n",
    "    fine_tuned_model.config.pad_token_id = fine_tuned_tokenizer.eos_token_id\n",
    "\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "fine_tuned_model.to(device)\n",
    "fine_tuned_model.eval() # Set the model to evaluation mode\n",
    "\n",
    "# Example Sumerian transliteration to translate\n",
    "sumerian_prompt = \"dingir inana za-me-en\" # \"Goddess Inana, you are\"\n",
    "\n",
    "# Format the prompt exactly as done during training, up to the point where generation should start\n",
    "prompt_for_generation = f\"Sumerian: {sumerian_prompt.strip()} English:\"\n",
    "print(f\"Prompt for generation: '{prompt_for_generation}'\")\n",
    "\n",
    "# Tokenize the prompt\n",
    "input_ids = fine_tuned_tokenizer.encode(prompt_for_generation, return_tensors='pt').to(device)\n",
    "\n",
    "# Generate text\n",
    "# Adjust generation parameters as needed\n",
    "# max_new_tokens is often preferred over max_length for more control over the generated part\n",
    "# For this example, we'll use max_length relative to the prompt.\n",
    "output_sequences = fine_tuned_model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_length=MAX_LENGTH, # Max length of prompt + generated text\n",
    "    # max_new_tokens=50, # Alternative: specify only the number of new tokens to generate\n",
    "    temperature=0.7,          # Controls randomness. Lower is more deterministic.\n",
    "    top_k=50,                 # Considers the top K most probable tokens at each step.\n",
    "    top_p=0.95,               # Nucleus sampling: considers tokens with cumulative probability >= P.\n",
    "    repetition_penalty=1.2,   # Penalizes repetition.\n",
    "    num_return_sequences=1,   # Number of different sequences to generate.\n",
    "    pad_token_id=fine_tuned_tokenizer.eos_token_id # Crucial for generation\n",
    ")\n",
    "\n",
    "# Decode and print the generated text\n",
    "for generated_sequence in output_sequences:\n",
    "    full_text = fine_tuned_tokenizer.decode(generated_sequence, skip_special_tokens=False) # Keep special tokens initially for inspection\n",
    "    # Extract only the generated English part\n",
    "    # This depends on your prompt format. We look for text after \"English: \"\n",
    "    generated_english = full_text.split(prompt_for_generation)[-1]\n",
    "    # Remove the <|endoftext|> token if present at the end\n",
    "    generated_english = generated_english.replace(fine_tuned_tokenizer.eos_token, \"\").strip()\n",
    "\n",
    "    print(f\"Sumerian Input: {sumerian_prompt}\")\n",
    "    print(f\"Generated English: {generated_english}\")\n",
    "    # For more detailed inspection:\n",
    "    # print(f\"Full generated sequence: {full_text}\")\n",
    "\n",
    "print(\"\\nScript finished.\")\n",
    "\n",
    "# To run this script:\n",
    "# 1. Save it as a Python file (e.g., `finetune_sumerian_gpt2.py`).\n",
    "# 2. Make sure you have your Sumerian and English data ready and update `load_your_data()`.\n",
    "# 3. Install the necessary libraries: pip install torch transformers datasets (datasets is not used here but often useful)\n",
    "# 4. Run from your terminal: python finetune_sumerian_gpt2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394f831c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['events.out.tfevents.1747150577.Samu_PC.17328.0', 'events.out.tfevents.1747151032.labP1WE54.4513.0', 'events.out.tfevents.1747152201.labP1WE54.10247.0']\n",
      "No log files found in the specified directory.\n"
     ]
    }
   ],
   "source": [
    "# plot training and validation loss\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Load the training logs\n",
    "log_dir = 'logs'\n",
    "log_files = [f for f in os.listdir(log_dir) if f.endswith('.json')]\n",
    "if not log_files:\n",
    "    print(\"No log files found in the specified directory.\")\n",
    "else:\n",
    "    log_file = os.path.join(log_dir, log_files[0])\n",
    "    with open(log_file, 'r') as f:\n",
    "        logs = json.load(f)\n",
    "\n",
    "    # Extract training and validation loss\n",
    "    train_loss = []\n",
    "    eval_loss = []\n",
    "    for log in logs:\n",
    "        if 'loss' in log:\n",
    "            train_loss.append(log['loss'])\n",
    "        if 'eval_loss' in log:\n",
    "            eval_loss.append(log['eval_loss'])\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(np.arange(len(train_loss)), train_loss, label='Training Loss')\n",
    "    plt.plot(np.arange(len(eval_loss)), eval_loss, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss Over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb9adc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generazione del testo...\n",
      "Testo di input: Sumerian:  1(u) la₂ 1(diš) udu u₄ 2(u) 8(diš)-kam ki ab-ba-sa₆-ga-ta na-lu₅ i₃-dab₅   iti <unk> bi₂-gu₇ mu en-unu₆-gal {d}inana unu{ki}ga ba-hun  1(u) la₂ 1(diš) \n",
      "English:\n",
      "Traduzione effettiva: 9 rams, 28th day, from Abba-saga, Nalu accepted; month: “ubi-feast,” year: “Enunugal of Inanna of Uruk was installed;” (total:) 9 (rams).\n",
      "Testo Generato: Sumerian:  1(u) la₂ 1(diš) udu u₄ 2(u) 8(diš)-kam ki ab-ba-sa₆-ga-ta na-lu₅ i₃-dab₅   iti <unk> bi₂-gu₇ mu en-unu₆-gal {d}inana unu{ki}ga ba-hun  1(u) la₂ 1(diš) \n",
      "English: 10 sheep, 28th day; from Abbasaga Nalu accepted; month “Ubi-feast,” year after: “En\u0015unugal of Inanna in Uruk was installed;” (total): 11. (total:)...). [...] 20th day, out(-of-)BHIgina priestess of Unu-Inanna; extra month „Big festival,” Year following: “The high-priestess of Inanna by means ... was chosen.” En(-En-(Šašanī) praised. Month: “Great Festival,” Year: ‟Ansanne is the queen.” 15th day, inside palace. Apisal court administrator. Booked-outs: xxx xxx xxx xxx xxx xxx xxx\n",
      "---\n",
      "Generazione del testo...\n",
      "Testo di input: Sumerian:  3(diš) 1/2(diš) gin₂ 1(u) 5(diš) še ku₃-babbar ur₅-še₃ ur{d}en-lil₂-la₂-ta lugal-sa₆-ga u₃ ur{d}šu-mah šu ba-ti  iti ku₃ <unk> u₄ 2(u) 2(diš) ba-zal  mu si-ma-num₂{ki} ba-hul \n",
      "English:\n",
      "Traduzione effettiva: 3 1/2 shekels 15 grains of silver, for interest, from Ur-Enlila, Lugal-saga, and Ur-Šumaḫ, received. month: “KuŠIM,” the 22th day passed, year: “Simanum was destroyed.”\n",
      "Testo Generato: Sumerian:  3(diš) 1/2(diš) gin₂ 1(u) 5(diš) še ku₃-babbar ur₅-še₃ ur{d}en-lil₂-la₂-ta lugal-sa₆-ga u₃ ur{d}šu-mah šu ba-ti  iti ku₃ <unk> u₄ 2(u) 2(diš) ba-zal  mu si-ma-num₂{ki} ba-hul \n",
      "English: 3 1/3 shekels silver, 15 grains of barley (according to the account booked out), from Ur-Enlila did Lugal-saga receive; month “kisnudĝum” and year following: \"Simanum was destroyed.\" Month ‘Kuŋuga,” Year after: Simana. Shu-Mamaḫgal founded. 372 bales at that time. Seal of Uram Ḫurta. Ilgi(-priestess) of Umma. Speaker of Siskur. Watchman of Ku-Shuba. 26th day passed. In Karaborum. Lu-Suen recieved. Sealed document of Ur-Šūmah\n",
      "---\n",
      "Generazione del testo...\n",
      "Testo di input: Sumerian:  6(diš) murgu₂ peš 2(geš₂) 4(u) 5(diš) {geš}umbin ma₂ 6(diš) {geš}u₃-suh₅ {geš}a-da-še₃ ki lu₂-kal-la-ta  mar-sa-aš kišib₃ lu₂-sa₆-i₃-zu mu hu-uh₂-nu-ri{ki} ba-hul  \n",
      "English:\n",
      "Traduzione effettiva: 6 date palm spines, 165 boat ribs(?), 6 pine trees for ada-planks, from Lukalla, to the boat house, under seal of Lu-sa-izu. year: “Ḫuḫnuri was destroyed.”\n",
      "Testo Generato: Sumerian:  6(diš) murgu₂ peš 2(geš₂) 4(u) 5(diš) {geš}umbin ma₂ 6(diš) {geš}u₃-suh₅ {geš}a-da-še₃ ki lu₂-kal-la-ta  mar-sa-aš kišib₃ lu₂-sa₆-i₃-zu mu hu-uh₂-nu-ri{ki} ba-hul  \n",
      "English: 36 copper, silver (weights), its weight is 26 gur; from Lu'akalla. under seal of Lukalla Month “Harvest” Year after: \"The wall was erected.\" Ilumuzu destroyed The Wall for the 9th time . Urbilum and Hurtnuri were destoyed by it All that exists in heaven must be repaid In this place there is no god Ending a destructive campaign Believed to be responsible For Lullubidzida's fate During which period did Irestina become king? ... My beloved sister! May she not dwell inside my clutches\"(?) Uruk 3/4(diš)-kam šuniŋin 1/3(diš)-bar naga kar�\n",
      "---\n",
      "Generazione del testo...\n",
      "Testo di input: Sumerian:  1/3(diš) kuš gu₄ 1(diš) sa gu₄ 2(diš) kuš udu a i₃-ri₂-na 1(u) gin₂ še-gin₂  ...  mu si-ma-num₂{ki} \n",
      "English:\n",
      "Traduzione effettiva: 1/3 oxen hide, 1 (bundle of) oxen sinews, 2 sheep skins “(soaked) with madder,” 10 shekels of glue, year: “Simanum.”\n",
      "Testo Generato: Sumerian:  1/3(diš) kuš gu₄ 1(diš) sa gu₄ 2(diš) kuš udu a i₃-ri₂-na 1(u) gin₂ še-gin₂  ...  mu si-ma-num₂{ki} \n",
      "English: 11 oxen, 10 sheep goats, grass(-fed), 20th day; from Abbasaga did Ninsha receive month “Piglet feast”. Month “Simanum.\" Year after: \"The throne was fashioned\" for Simanum in Uruk by the scribe SIAb. Sister of Gina. Seal of Ningina. (total:)- 12 months. Sealed tablet of Ninginana. Mama and Childers are here. The year following: \"Simanal is destroyed\"(?) to be replaced by Shu-Mama.-Six male laborers at Willowmium were found.(?). All that they possessed before being transferred out into the lands where it would have been carried off). Grand Total: 13 years. From Ningina(’s account). To [son] Sila... . May she bring justice! Tahi'ea may he give her!\". Tigartisanus minaretsuna {d}utul Anzuimudya son of Ur-Utaba received. Within A�\n",
      "---\n",
      "Generazione del testo...\n",
      "Testo di input: Sumerian:  da-da u₃-na-a-du₁₁ 3(gešʾu) sa gi <unk> giri₃-ni-i₃-sa₆-ra  he₂-na-ab-šum₂-mu    {d}...gi nita kal-ga lugal uri₅{ki}ma lugal an ub-da limmu₂-ba  ... ensi₂ umma{ki} <unk> zu \n",
      "English:\n",
      "Traduzione effettiva: To Dada, say: “1800 bundles of fire-reeds, to(?) Girini-isa, let him give.” Šulgi, strong man, the king of Ur, king of the four corners: Ur-Lisi, governor, of Umma, is your servant.\n",
      "Testo Generato: Sumerian:  da-da u₃-na-a-du₁₁ 3(gešʾu) sa gi <unk> giri₃-ni-i₃-sa₆-ra  he₂-na-ab-šum₂-mu    {d}...gi nita kal-ga lugal uri₅{ki}ma lugal an ub-da limmu₂-ba  ... ensi₂ umma{ki} <unk> zu \n",
      "English: To Dada speak! 30 bundles of reed, (for the “house”). From Gudani. Sealed tablet of Iliya'qurī. Year following: ‟The throne was fashioned for Umani by means known to man.\" Ur-Ishkalla is credited with this seal. Lugal-Amar-Suen, strong king, king in Umma, king of Lagaš. URU-KA-gina, governor of Umma, your servant. Amar-Suen, strong king,. . . . zu’sipa, [your] greatest barge-bearer. Belongs to you. Alakkan, scribe, son of Anubanda. SUZANĝEbex, courrier rider,\n",
      "---\n",
      "Generazione del testo...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerazione del testo...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 15\u001b[0m     output_sequences \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.95\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepetition_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTesto di input: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt_text\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraduzione effettiva: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menglish_translations\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/transformers/generation/utils.py:2465\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[0m\n\u001b[1;32m   2457\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2458\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2459\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2460\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2461\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2462\u001b[0m     )\n\u001b[1;32m   2464\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2465\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2466\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2467\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2468\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2469\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2470\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2471\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2472\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2473\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2475\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2476\u001b[0m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[1;32m   2477\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2478\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2479\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   2480\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2481\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2482\u001b[0m     )\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/transformers/generation/utils.py:3434\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3432\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3433\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3434\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3436\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3437\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3438\u001b[0m     outputs,\n\u001b[1;32m   3439\u001b[0m     model_kwargs,\n\u001b[1;32m   3440\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3441\u001b[0m )\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:1062\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1060\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1062\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1063\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1064\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1065\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1066\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1068\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1069\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1070\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1071\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1072\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1073\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1074\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1075\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1077\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1079\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:922\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    910\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    911\u001b[0m         block\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    912\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    919\u001b[0m         output_attentions,\n\u001b[1;32m    920\u001b[0m     )\n\u001b[1;32m    921\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 922\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    933\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    934\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:441\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    439\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    440\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_2(hidden_states)\n\u001b[0;32m--> 441\u001b[0m feed_forward_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;66;03m# residual connection\u001b[39;00m\n\u001b[1;32m    443\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m feed_forward_hidden_states\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:368\u001b[0m, in \u001b[0;36mGPT2MLP.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: Optional[Tuple[torch\u001b[38;5;241m.\u001b[39mFloatTensor]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor:\n\u001b[0;32m--> 368\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_fc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    369\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(hidden_states)\n\u001b[1;32m    370\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_proj(hidden_states)\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/transformers/pytorch_utils.py:118\u001b[0m, in \u001b[0;36mConv1D.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    117\u001b[0m     size_out \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnf,)\n\u001b[0;32m--> 118\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddmm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(size_out)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# load model from a checkpoint\n",
    "model = GPT2LMHeadModel.from_pretrained(f\"{OUTPUT_DIR}\").to(device)\n",
    "\n",
    "for index, row in test_data.iloc[:30,:].iterrows():\n",
    "    sumerian_texts = row['transliteration'].replace('\\n', ' ')\n",
    "    english_translations = row['translation'].replace('\\n', ' ')\n",
    "    prompt_text = f\"Sumerian: {sumerian_texts} \\nEnglish:\"\n",
    "\n",
    "    # --- Tokenizzazione dell'input ---\n",
    "    input_ids = tokenizer.encode(prompt_text, return_tensors='pt').to(device)\n",
    "\n",
    "    # --- Generazione del testo ---\n",
    "    print(\"Generazione del testo...\")\n",
    "    try:\n",
    "        output_sequences = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_length=200,\n",
    "            temperature=0.7,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            repetition_penalty=1.1,\n",
    "            num_return_sequences=1,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "        print(f\"Testo di input: {prompt_text}\")\n",
    "        print(f\"Traduzione effettiva: {english_translations}\")\n",
    "        \n",
    "        # --- Decodifica e Stampa ---\n",
    "        for i, generated_sequence in enumerate(output_sequences):\n",
    "            text = tokenizer.decode(generated_sequence, skip_special_tokens=True)\n",
    "            print('Testo Generato:', text)\n",
    "            print('---')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Errore durante la generazione del testo: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "57d05237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking if the destination folder 'sumerian_gpt2_finetuned' already exists on OneDrive...\n",
      "Moving 'sumerian_gpt2_finetuned' to OneDrive: 'onedrive:AI-project/sumerian_gpt2_finetuned'...\n",
      "rclone command: rclone move sumerian_gpt2_finetuned onedrive:AI-project/sumerian_gpt2_finetuned --create-empty-src-dirs -P\n",
      "SUCCESS: Folder 'sumerian_gpt2_finetuned' moved successfully to OneDrive.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rclone import move_folder_to_onedrive\n",
    "\n",
    "move_folder_to_onedrive('sumerian_gpt2_finetuned')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azureml_py310_sdkv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
