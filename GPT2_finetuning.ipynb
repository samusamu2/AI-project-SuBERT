{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "129d7e18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0032ff99fba748c396b69c44e82d4298",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3568ed80fdf646e9b7ca3633af79cef7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c011e46ae3fc4ed7aaed3fba946428eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0712a772df2444978a9e54e8577b7e13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3637e630ae9f480da2b3e263be512752",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73df0df0a569464cbd21d77ca3f7ef4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57cbfcfe8a1448639be9739343e423df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "model_name = 'gpt2'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03d33c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stras\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\generation\\configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\stras\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\generation\\configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modello caricato su: cpu\n",
      "Generazione del testo...\n",
      "\n",
      "--- Testo Generato 1 ---\n",
      "Once upon a time, in a land far, far away, the world was not so much as one of those things that is called \"the earth.\" It had been there for thousands of years. And now it has come to pass; and we are all living on this planet with our own hands!\n",
      "The Earth's surface temperature rose by about 1 degree Celsius (3 degrees Fahrenheit) during last century alone—a record high since records began being made at least 20 million years ago. The average annual increase over these past two centuries would have taken place only if temperatures were kept constant throughout history: today, they're just below zero or even above 0°C/century — which means no warming whatsoever from human activity until 2100. But what happens\n"
     ]
    }
   ],
   "source": [
    "# Test model is working\n",
    "\n",
    "prompt_text = \"Once upon a time, in a land far, far away,\"\n",
    "\n",
    "# Imposta il pad_token se non è definito\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "# --- Device Configuration (GPU o CPU) ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(f\"Modello caricato su: {device}\")\n",
    "\n",
    "# --- Tokenizzazione dell'input ---\n",
    "input_ids = tokenizer.encode(prompt_text, return_tensors='pt').to(device)\n",
    "\n",
    "# --- Generazione del testo ---\n",
    "print(\"Generazione del testo...\")\n",
    "try:\n",
    "    output_sequences = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=150,\n",
    "        temperature=0.7,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        repetition_penalty=1.1,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    # --- Decodifica e Stampa ---\n",
    "    for i, generated_sequence in enumerate(output_sequences):\n",
    "        text = tokenizer.decode(generated_sequence, skip_special_tokens=True)\n",
    "        print(f\"\\n--- Testo Generato {i+1} ---\")\n",
    "        print(text)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Errore durante la generazione del testo: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "170cf36a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1905 formatted examples.\n",
      "Example formatted text: Sumerian: 1(u) la₂ 1(diš) udu\n",
      "u₄ 2(u) 8(diš)-kam\n",
      "ki ab-ba-sa₆-ga-ta\n",
      "na-lu₅ i₃-dab₅\n",
      "\n",
      "\n",
      "iti <unk> bi₂-gu₇\n",
      "mu en-unu₆-gal {d}inana unu{ki}ga ba-hun\n",
      "\n",
      "1(u) la₂ 1(diš) English: 9 rams,\n",
      "28th day,\n",
      "from Abba-saga,\n",
      "Nalu accepted;\n",
      "month: “ubi-feast,”\n",
      "year: “Enunugal of Inanna of Uruk was installed;”\n",
      "(total:) 9 (rams).\n",
      "Set tokenizer.pad_token to tokenizer.eos_token (<|endoftext|>)\n",
      "Split dataset into 1714 training samples and 191 validation samples.\n",
      "Set model.config.pad_token_id to 50256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stras\\AppData\\Local\\Temp\\ipykernel_17328\\2791006142.py:155: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fine-tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='1287' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   4/1287 00:19 < 3:23:36, 0.11 it/s, Epoch 0.01/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 167\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting fine-tuning...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 167\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFine-tuning completed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\trainer.py:2245\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2243\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2246\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2250\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\trainer.py:2560\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2553\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2554\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2555\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2556\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[0;32m   2557\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2558\u001b[0m )\n\u001b[0;32m   2559\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2560\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2563\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2564\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2565\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2566\u001b[0m ):\n\u001b[0;32m   2567\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2568\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\trainer.py:3782\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   3779\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED:\n\u001b[0;32m   3780\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscale_wrt_gas\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 3782\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3784\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\accelerate\\accelerator.py:2454\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   2452\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[0;32m   2453\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2454\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from transformers import (\n",
    "    GPT2Tokenizer,\n",
    "    GPT2LMHeadModel,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "import pandas as pd\n",
    "\n",
    "# --- 1. Configuration & Parameters ---\n",
    "# Model choice: you can start with 'gpt2' and try 'gpt2-medium' if you have more resources\n",
    "MODEL_NAME = 'gpt2'\n",
    "OUTPUT_DIR = './sumerian_gpt2_finetuned' # Directory to save the fine-tuned model\n",
    "LOG_DIR = './logs'                     # Directory for training logs\n",
    "\n",
    "# Training hyperparameters (adjust these based on your dataset size and resources)\n",
    "NUM_EPOCHS = 3                         # Number of training epochs\n",
    "BATCH_SIZE_PER_DEVICE = 4              # Batch size for training and evaluation (adjust based on GPU memory)\n",
    "LEARNING_RATE = 5e-5                   # Learning rate\n",
    "WARMUP_STEPS = 100                     # Number of warmup steps for learning rate scheduler\n",
    "WEIGHT_DECAY = 0.01                    # Weight decay\n",
    "MAX_LENGTH = 256                       # Maximum sequence length for tokenizer (adjust based on your data)\n",
    "TRAIN_VALID_SPLIT = 0.1                # Proportion of data to use for validation\n",
    "\n",
    "# --- 2. Load and Prepare Your Dataset ---\n",
    "# Assume you have your data as two lists: `sumerian_texts` and `english_translations`\n",
    "# Example:\n",
    "# sumerian_texts = [\"transliteration 1\", \"transliteration 2\", ...]\n",
    "# english_translations = [\"translation 1\", \"translation 2\", ...]\n",
    "\n",
    "train_data = pd.read_csv('datasets/SumTablets_English_train.csv')\n",
    "test_data = pd.read_csv('datasets/SumTablets_English_train.csv')\n",
    "\n",
    "\n",
    "# Format the data for GPT-2:\n",
    "# We'll combine Sumerian and English with a separator.\n",
    "# GPT-2 will learn to generate the English part after seeing \"English: \".\n",
    "# The <|endoftext|> token is GPT-2's standard end-of-sequence token.\n",
    "formatted_texts = []\n",
    "for index, row in train_data.iterrows():\n",
    "    sumerian_texts = row['transliteration']\n",
    "    english_translations = row['translation']\n",
    "    if isinstance(sumerian_texts, str) and isinstance(english_translations, str):\n",
    "        formatted_texts.append(f\"Sumerian: {sumerian_texts.strip()} English: {english_translations.strip()}\")\n",
    "print(f\"Loaded {len(formatted_texts)} formatted examples.\")\n",
    "\n",
    "if formatted_texts:\n",
    "    print(f\"Example formatted text: {formatted_texts[0]}\")\n",
    "\n",
    "# --- 3. Initialize Tokenizer ---\n",
    "# Load GPT-2 tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# GPT-2 doesn't have a pad token by default. We'll use the eos_token as the pad_token.\n",
    "# This is a common practice.\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(f\"Set tokenizer.pad_token to tokenizer.eos_token ({tokenizer.eos_token})\")\n",
    "\n",
    "# --- 4. Create a PyTorch Dataset ---\n",
    "class SumerianEnglishDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = texts\n",
    "        self.max_length = max_length\n",
    "        self.encodings = []\n",
    "        for text in texts:\n",
    "            # Tokenize the combined text\n",
    "            # truncation=True ensures that sequences longer than max_length are cut.\n",
    "            # padding='max_length' pads shorter sequences to max_length.\n",
    "            # return_tensors='pt' returns PyTorch tensors.\n",
    "            encoding = self.tokenizer(\n",
    "                text,\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                padding=\"max_length\", # Ensure all sequences have the same length for batching\n",
    "                return_attention_mask=True,\n",
    "                return_tensors='pt'  # Explicitly specify to return PyTorch tensors\n",
    "            )\n",
    "            # For language modeling, the 'labels' are typically the same as 'input_ids'.\n",
    "            # The model will learn to predict the next token.\n",
    "            # The DataCollatorForLanguageModeling will handle shifting labels for us.\n",
    "            self.encodings.append({\n",
    "                \"input_ids\": encoding[\"input_ids\"].squeeze(), # Remove batch dimension if present\n",
    "                \"attention_mask\": encoding[\"attention_mask\"].squeeze()\n",
    "            })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.encodings[idx]\n",
    "        # The labels are the input_ids themselves for language modeling.\n",
    "        # The model is trained to predict the next token in the sequence.\n",
    "        # The DataCollatorForLanguageModeling will shift them appropriately.\n",
    "        return {\"input_ids\": item[\"input_ids\"], \"attention_mask\": item[\"attention_mask\"], \"labels\": item[\"input_ids\"].clone()}\n",
    "\n",
    "# Create the full dataset\n",
    "full_dataset = SumerianEnglishDataset(formatted_texts, tokenizer, MAX_LENGTH)\n",
    "\n",
    "# Split into training and validation sets\n",
    "if TRAIN_VALID_SPLIT > 0:\n",
    "    num_train = int((1 - TRAIN_VALID_SPLIT) * len(full_dataset))\n",
    "    num_valid = len(full_dataset) - num_train\n",
    "    train_dataset, eval_dataset = random_split(full_dataset, [num_train, num_valid])\n",
    "    print(f\"Split dataset into {len(train_dataset)} training samples and {len(eval_dataset)} validation samples.\")\n",
    "else:\n",
    "    train_dataset = full_dataset\n",
    "    eval_dataset = None # No validation\n",
    "    print(f\"Using all {len(train_dataset)} samples for training. No validation set.\")\n",
    "\n",
    "\n",
    "# --- 5. Initialize Model ---\n",
    "# Load GPT-2 model with a language modeling head\n",
    "# model = GPT2LMHeadModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Resize token embeddings if you added special tokens (not strictly necessary here as we used eos_token as pad_token)\n",
    "# model.resize_token_embeddings(len(tokenizer)) # Uncomment if you explicitly added new tokens\n",
    "\n",
    "# Set the pad_token_id in the model configuration (important for generation and padding)\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "print(f\"Set model.config.pad_token_id to {tokenizer.pad_token_id}\")\n",
    "\n",
    "\n",
    "# --- 6. Data Collator ---\n",
    "# The DataCollatorForLanguageModeling will automatically create batches and\n",
    "# shift the input_ids to create labels for causal language modeling (predicting the next token).\n",
    "# It also handles padding. `mlm=False` means we are doing Causal Language Modeling (CLM), not Masked Language Modeling (MLM).\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # Causal Language Modeling for GPT-2\n",
    ")\n",
    "\n",
    "# --- 7. Define Training Arguments ---\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,                  # Directory to save model checkpoints and outputs\n",
    "    num_train_epochs=NUM_EPOCHS,            # Total number of training epochs\n",
    "    per_device_train_batch_size=BATCH_SIZE_PER_DEVICE, # Batch size per device during training\n",
    "    per_device_eval_batch_size=BATCH_SIZE_PER_DEVICE,  # Batch size for evaluation\n",
    "    warmup_steps=WARMUP_STEPS,              # Number of warmup steps for learning rate scheduler\n",
    "    weight_decay=WEIGHT_DECAY,              # Strength of weight decay\n",
    "    logging_dir=LOG_DIR,                    # Directory for storing logs\n",
    "    logging_steps=10,                       # Log every X updates steps\n",
    "    eval_strategy=\"epoch\" if eval_dataset else \"no\", # Evaluate at the end of each epoch if eval_dataset exists\n",
    "    save_strategy=\"epoch\",                  # Save a checkpoint at the end of each epoch\n",
    "    load_best_model_at_end=True if eval_dataset else False, # Load the best model found during training (based on eval loss)\n",
    "    fp16=torch.cuda.is_available(),         # Use 16-bit (mixed) precision training if a GPU is available\n",
    "    # report_to=\"tensorboard\",              # You can integrate with TensorBoard, WandB, etc.\n",
    ")\n",
    "\n",
    "# --- 8. Initialize Trainer ---\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# --- 9. Start Fine-tuning ---\n",
    "print(\"Starting fine-tuning...\")\n",
    "try:\n",
    "    trainer.train()\n",
    "    print(\"Fine-tuning completed.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during training: {e}\")\n",
    "    # Potentially save the current state if an error occurs mid-training\n",
    "    # model.save_pretrained(f\"{OUTPUT_DIR}_interrupted\")\n",
    "    # tokenizer.save_pretrained(f\"{OUTPUT_DIR}_interrupted\")\n",
    "    # print(f\"Interrupted model saved to {OUTPUT_DIR}_interrupted\")\n",
    "    raise e\n",
    "\n",
    "\n",
    "# --- 10. Save the Fine-tuned Model and Tokenizer ---\n",
    "print(f\"Saving model to {OUTPUT_DIR}\")\n",
    "trainer.save_model(OUTPUT_DIR) # Saves the model state_dict and configuration\n",
    "tokenizer.save_pretrained(OUTPUT_DIR) # Saves the tokenizer\n",
    "print(f\"Model and tokenizer saved to {OUTPUT_DIR}\")\n",
    "\n",
    "\n",
    "# --- 11. Inference Example (How to use the fine-tuned model) ---\n",
    "print(\"\\n--- Inference Example ---\")\n",
    "# Load the fine-tuned model and tokenizer\n",
    "fine_tuned_model = GPT2LMHeadModel.from_pretrained(OUTPUT_DIR)\n",
    "fine_tuned_tokenizer = GPT2Tokenizer.from_pretrained(OUTPUT_DIR)\n",
    "\n",
    "# Ensure the pad token is set for the loaded tokenizer (it should be saved, but good to double check)\n",
    "if fine_tuned_tokenizer.pad_token is None:\n",
    "    fine_tuned_tokenizer.pad_token = fine_tuned_tokenizer.eos_token\n",
    "    fine_tuned_model.config.pad_token_id = fine_tuned_tokenizer.eos_token_id\n",
    "\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "fine_tuned_model.to(device)\n",
    "fine_tuned_model.eval() # Set the model to evaluation mode\n",
    "\n",
    "# Example Sumerian transliteration to translate\n",
    "sumerian_prompt = \"dingir inana za-me-en\" # \"Goddess Inana, you are\"\n",
    "\n",
    "# Format the prompt exactly as done during training, up to the point where generation should start\n",
    "prompt_for_generation = f\"Sumerian: {sumerian_prompt.strip()} English:\"\n",
    "print(f\"Prompt for generation: '{prompt_for_generation}'\")\n",
    "\n",
    "# Tokenize the prompt\n",
    "input_ids = fine_tuned_tokenizer.encode(prompt_for_generation, return_tensors='pt').to(device)\n",
    "\n",
    "# Generate text\n",
    "# Adjust generation parameters as needed\n",
    "# max_new_tokens is often preferred over max_length for more control over the generated part\n",
    "# For this example, we'll use max_length relative to the prompt.\n",
    "output_sequences = fine_tuned_model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_length=MAX_LENGTH, # Max length of prompt + generated text\n",
    "    # max_new_tokens=50, # Alternative: specify only the number of new tokens to generate\n",
    "    temperature=0.7,          # Controls randomness. Lower is more deterministic.\n",
    "    top_k=50,                 # Considers the top K most probable tokens at each step.\n",
    "    top_p=0.95,               # Nucleus sampling: considers tokens with cumulative probability >= P.\n",
    "    repetition_penalty=1.2,   # Penalizes repetition.\n",
    "    num_return_sequences=1,   # Number of different sequences to generate.\n",
    "    pad_token_id=fine_tuned_tokenizer.eos_token_id # Crucial for generation\n",
    ")\n",
    "\n",
    "# Decode and print the generated text\n",
    "for generated_sequence in output_sequences:\n",
    "    full_text = fine_tuned_tokenizer.decode(generated_sequence, skip_special_tokens=False) # Keep special tokens initially for inspection\n",
    "    # Extract only the generated English part\n",
    "    # This depends on your prompt format. We look for text after \"English: \"\n",
    "    generated_english = full_text.split(prompt_for_generation)[-1]\n",
    "    # Remove the <|endoftext|> token if present at the end\n",
    "    generated_english = generated_english.replace(fine_tuned_tokenizer.eos_token, \"\").strip()\n",
    "\n",
    "    print(f\"Sumerian Input: {sumerian_prompt}\")\n",
    "    print(f\"Generated English: {generated_english}\")\n",
    "    # For more detailed inspection:\n",
    "    # print(f\"Full generated sequence: {full_text}\")\n",
    "\n",
    "print(\"\\nScript finished.\")\n",
    "\n",
    "# To run this script:\n",
    "# 1. Save it as a Python file (e.g., `finetune_sumerian_gpt2.py`).\n",
    "# 2. Make sure you have your Sumerian and English data ready and update `load_your_data()`.\n",
    "# 3. Install the necessary libraries: pip install torch transformers datasets (datasets is not used here but often useful)\n",
    "# 4. Run from your terminal: python finetune_sumerian_gpt2.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
