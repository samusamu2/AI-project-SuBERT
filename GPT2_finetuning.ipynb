{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "129d7e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 16:02:16.171231: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747152137.154339   10247 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747152137.428631   10247 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747152140.258324   10247 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747152140.258366   10247 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747152140.258369   10247 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747152140.258372   10247 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-13 16:02:20.550862: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "model_name = 'gpt2'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f03d33c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modello caricato su: cuda\n",
      "Generazione del testo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testo Generato 1 ---\n",
      "Once upon a time, in a land far, far away, the world was not so much as one of those things that is called \"the earth.\" It had been there for thousands of years. And now it has come to pass; and we are all living on this planet with our own hands!\n",
      "The Earth's surface temperature rose by about 1 degree Celsius (3 degrees Fahrenheit) during last century alone—a record high since records began being made at least 20 million years ago. The average annual increase over these past two centuries would have taken place only if temperatures were kept constant throughout history: today, they're just below zero or even above 0°C/century — which means no warming whatsoever from human activity until 2100. But what happens\n"
     ]
    }
   ],
   "source": [
    "# Test model is working\n",
    "\n",
    "prompt_text = \"Once upon a time, in a land far, far away,\"\n",
    "\n",
    "# Imposta il pad_token se non è definito\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "# --- Device Configuration (GPU o CPU) ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(f\"Modello caricato su: {device}\")\n",
    "\n",
    "# --- Tokenizzazione dell'input ---\n",
    "input_ids = tokenizer.encode(prompt_text, return_tensors='pt').to(device)\n",
    "\n",
    "# --- Generazione del testo ---\n",
    "print(\"Generazione del testo...\")\n",
    "try:\n",
    "    output_sequences = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=150,\n",
    "        temperature=0.7,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        repetition_penalty=1.1,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    # --- Decodifica e Stampa ---\n",
    "    for i, generated_sequence in enumerate(output_sequences):\n",
    "        text = tokenizer.decode(generated_sequence, skip_special_tokens=True)\n",
    "        print(f\"\\n--- Testo Generato {i+1} ---\")\n",
    "        print(text)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Errore durante la generazione del testo: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "170cf36a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1905 formatted examples.\n",
      "Example formatted text: Sumerian: 1(u) la₂ 1(diš) udu\n",
      "u₄ 2(u) 8(diš)-kam\n",
      "ki ab-ba-sa₆-ga-ta\n",
      "na-lu₅ i₃-dab₅\n",
      "\n",
      "\n",
      "iti <unk> bi₂-gu₇\n",
      "mu en-unu₆-gal {d}inana unu{ki}ga ba-hun\n",
      "\n",
      "1(u) la₂ 1(diš) English: 9 rams,\n",
      "28th day,\n",
      "from Abba-saga,\n",
      "Nalu accepted;\n",
      "month: “ubi-feast,”\n",
      "year: “Enunugal of Inanna of Uruk was installed;”\n",
      "(total:) 9 (rams).\n",
      "Set tokenizer.pad_token to tokenizer.eos_token (<|endoftext|>)\n",
      "Split dataset into 1714 training samples and 191 validation samples.\n",
      "Set model.config.pad_token_id to 50256\n",
      "Starting fine-tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10247/1176954419.py:155: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9449' max='21450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 9449/21450 17:20 < 22:01, 9.08 it/s, Epoch 22.02/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.043800</td>\n",
       "      <td>0.856657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.912000</td>\n",
       "      <td>0.850089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.793200</td>\n",
       "      <td>0.850662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.784000</td>\n",
       "      <td>0.846976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.836200</td>\n",
       "      <td>0.850845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.753500</td>\n",
       "      <td>0.852763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.640400</td>\n",
       "      <td>0.869439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.680900</td>\n",
       "      <td>0.865067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.680200</td>\n",
       "      <td>0.869354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.595700</td>\n",
       "      <td>0.884560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.494500</td>\n",
       "      <td>0.903524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.493700</td>\n",
       "      <td>0.912063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.451900</td>\n",
       "      <td>0.933314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.442500</td>\n",
       "      <td>0.935757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.496500</td>\n",
       "      <td>0.943482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.532500</td>\n",
       "      <td>0.959158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.978015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.430500</td>\n",
       "      <td>0.987302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.399500</td>\n",
       "      <td>0.986449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.346600</td>\n",
       "      <td>1.012642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.392300</td>\n",
       "      <td>1.021060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.473800</td>\n",
       "      <td>1.044097</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 167\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting fine-tuning...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 167\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFine-tuning completed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/transformers/trainer.py:2245\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2243\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2246\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2250\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/transformers/trainer.py:2565\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2559\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m   2560\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs, num_items_in_batch)\n\u001b[1;32m   2562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2563\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2564\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m-> 2565\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2566\u001b[0m ):\n\u001b[1;32m   2567\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2568\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   2569\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from transformers import (\n",
    "    GPT2Tokenizer,\n",
    "    GPT2LMHeadModel,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "import pandas as pd\n",
    "\n",
    "# --- 1. Configuration & Parameters ---\n",
    "# Model choice: you can start with 'gpt2' and try 'gpt2-medium' if you have more resources\n",
    "MODEL_NAME = 'gpt2'\n",
    "OUTPUT_DIR = './sumerian_gpt2_finetuned' # Directory to save the fine-tuned model\n",
    "LOG_DIR = './logs'                     # Directory for training logs\n",
    "\n",
    "# Training hyperparameters (adjust these based on your dataset size and resources)\n",
    "NUM_EPOCHS = 50                        # Number of training epochs\n",
    "BATCH_SIZE_PER_DEVICE = 4              # Batch size for training and evaluation (adjust based on GPU memory)\n",
    "LEARNING_RATE = 5e-5                   # Learning rate\n",
    "WARMUP_STEPS = 100                     # Number of warmup steps for learning rate scheduler\n",
    "WEIGHT_DECAY = 0.01                    # Weight decay\n",
    "MAX_LENGTH = 512                       # Maximum sequence length for tokenizer (adjust based on your data)\n",
    "TRAIN_VALID_SPLIT = 0.1                # Proportion of data to use for validation\n",
    "\n",
    "# --- 2. Load and Prepare Your Dataset ---\n",
    "# Assume you have your data as two lists: `sumerian_texts` and `english_translations`\n",
    "# Example:\n",
    "# sumerian_texts = [\"transliteration 1\", \"transliteration 2\", ...]\n",
    "# english_translations = [\"translation 1\", \"translation 2\", ...]\n",
    "\n",
    "train_data = pd.read_csv('datasets/SumTablets_English_train.csv')\n",
    "test_data = pd.read_csv('datasets/SumTablets_English_train.csv')\n",
    "\n",
    "\n",
    "# Format the data for GPT-2:\n",
    "# We'll combine Sumerian and English with a separator.\n",
    "# GPT-2 will learn to generate the English part after seeing \"English: \".\n",
    "# The <|endoftext|> token is GPT-2's standard end-of-sequence token.\n",
    "formatted_texts = []\n",
    "for index, row in train_data.iterrows():\n",
    "    sumerian_texts = row['transliteration']\n",
    "    english_translations = row['translation']\n",
    "    if isinstance(sumerian_texts, str) and isinstance(english_translations, str):\n",
    "        formatted_texts.append(f\"Sumerian: {sumerian_texts.strip()} English: {english_translations.strip()}\")\n",
    "print(f\"Loaded {len(formatted_texts)} formatted examples.\")\n",
    "\n",
    "if formatted_texts:\n",
    "    print(f\"Example formatted text: {formatted_texts[0]}\")\n",
    "\n",
    "# --- 3. Initialize Tokenizer ---\n",
    "# Load GPT-2 tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# GPT-2 doesn't have a pad token by default. We'll use the eos_token as the pad_token.\n",
    "# This is a common practice.\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(f\"Set tokenizer.pad_token to tokenizer.eos_token ({tokenizer.eos_token})\")\n",
    "\n",
    "# --- 4. Create a PyTorch Dataset ---\n",
    "class SumerianEnglishDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = texts\n",
    "        self.max_length = max_length\n",
    "        self.encodings = []\n",
    "        for text in texts:\n",
    "            # Tokenize the combined text\n",
    "            # truncation=True ensures that sequences longer than max_length are cut.\n",
    "            # padding='max_length' pads shorter sequences to max_length.\n",
    "            # return_tensors='pt' returns PyTorch tensors.\n",
    "            encoding = self.tokenizer(\n",
    "                text,\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                padding=\"max_length\", # Ensure all sequences have the same length for batching\n",
    "                return_attention_mask=True,\n",
    "                return_tensors='pt'  # Explicitly specify to return PyTorch tensors\n",
    "            )\n",
    "            # For language modeling, the 'labels' are typically the same as 'input_ids'.\n",
    "            # The model will learn to predict the next token.\n",
    "            # The DataCollatorForLanguageModeling will handle shifting labels for us.\n",
    "            self.encodings.append({\n",
    "                \"input_ids\": encoding[\"input_ids\"].squeeze(), # Remove batch dimension if present\n",
    "                \"attention_mask\": encoding[\"attention_mask\"].squeeze()\n",
    "            })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.encodings[idx]\n",
    "        # The labels are the input_ids themselves for language modeling.\n",
    "        # The model is trained to predict the next token in the sequence.\n",
    "        # The DataCollatorForLanguageModeling will shift them appropriately.\n",
    "        return {\"input_ids\": item[\"input_ids\"], \"attention_mask\": item[\"attention_mask\"], \"labels\": item[\"input_ids\"].clone()}\n",
    "\n",
    "# Create the full dataset\n",
    "full_dataset = SumerianEnglishDataset(formatted_texts, tokenizer, MAX_LENGTH)\n",
    "\n",
    "# Split into training and validation sets\n",
    "if TRAIN_VALID_SPLIT > 0:\n",
    "    num_train = int((1 - TRAIN_VALID_SPLIT) * len(full_dataset))\n",
    "    num_valid = len(full_dataset) - num_train\n",
    "    train_dataset, eval_dataset = random_split(full_dataset, [num_train, num_valid])\n",
    "    print(f\"Split dataset into {len(train_dataset)} training samples and {len(eval_dataset)} validation samples.\")\n",
    "else:\n",
    "    train_dataset = full_dataset\n",
    "    eval_dataset = None # No validation\n",
    "    print(f\"Using all {len(train_dataset)} samples for training. No validation set.\")\n",
    "\n",
    "\n",
    "# --- 5. Initialize Model ---\n",
    "# Load GPT-2 model with a language modeling head\n",
    "# model = GPT2LMHeadModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Resize token embeddings if you added special tokens (not strictly necessary here as we used eos_token as pad_token)\n",
    "# model.resize_token_embeddings(len(tokenizer)) # Uncomment if you explicitly added new tokens\n",
    "\n",
    "# Set the pad_token_id in the model configuration (important for generation and padding)\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "print(f\"Set model.config.pad_token_id to {tokenizer.pad_token_id}\")\n",
    "\n",
    "\n",
    "# --- 6. Data Collator ---\n",
    "# The DataCollatorForLanguageModeling will automatically create batches and\n",
    "# shift the input_ids to create labels for causal language modeling (predicting the next token).\n",
    "# It also handles padding. `mlm=False` means we are doing Causal Language Modeling (CLM), not Masked Language Modeling (MLM).\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # Causal Language Modeling for GPT-2\n",
    ")\n",
    "\n",
    "# --- 7. Define Training Arguments ---\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,                  # Directory to save model checkpoints and outputs\n",
    "    num_train_epochs=NUM_EPOCHS,            # Total number of training epochs\n",
    "    per_device_train_batch_size=BATCH_SIZE_PER_DEVICE, # Batch size per device during training\n",
    "    per_device_eval_batch_size=BATCH_SIZE_PER_DEVICE,  # Batch size for evaluation\n",
    "    warmup_steps=WARMUP_STEPS,              # Number of warmup steps for learning rate scheduler\n",
    "    weight_decay=WEIGHT_DECAY,              # Strength of weight decay\n",
    "    logging_dir=LOG_DIR,                    # Directory for storing logs\n",
    "    logging_steps=10,                       # Log every X updates steps\n",
    "    eval_strategy=\"epoch\" if eval_dataset else \"no\", # Evaluate at the end of each epoch if eval_dataset exists\n",
    "    save_strategy=\"epoch\",                  # Save a checkpoint at the end of each epoch\n",
    "    load_best_model_at_end=True if eval_dataset else False, # Load the best model found during training (based on eval loss)\n",
    "    fp16=torch.cuda.is_available(),         # Use 16-bit (mixed) precision training if a GPU is available\n",
    "    # report_to=\"tensorboard\",              # You can integrate with TensorBoard, WandB, etc.\n",
    ")\n",
    "\n",
    "# --- 8. Initialize Trainer ---\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# --- 9. Start Fine-tuning ---\n",
    "print(\"Starting fine-tuning...\")\n",
    "try:\n",
    "    trainer.train()\n",
    "    print(\"Fine-tuning completed.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during training: {e}\")\n",
    "    # Potentially save the current state if an error occurs mid-training\n",
    "    # model.save_pretrained(f\"{OUTPUT_DIR}_interrupted\")\n",
    "    # tokenizer.save_pretrained(f\"{OUTPUT_DIR}_interrupted\")\n",
    "    # print(f\"Interrupted model saved to {OUTPUT_DIR}_interrupted\")\n",
    "    raise e\n",
    "\n",
    "\n",
    "# --- 10. Save the Fine-tuned Model and Tokenizer ---\n",
    "print(f\"Saving model to {OUTPUT_DIR}\")\n",
    "trainer.save_model(OUTPUT_DIR) # Saves the model state_dict and configuration\n",
    "tokenizer.save_pretrained(OUTPUT_DIR) # Saves the tokenizer\n",
    "print(f\"Model and tokenizer saved to {OUTPUT_DIR}\")\n",
    "\n",
    "\n",
    "# --- 11. Inference Example (How to use the fine-tuned model) ---\n",
    "print(\"\\n--- Inference Example ---\")\n",
    "# Load the fine-tuned model and tokenizer\n",
    "fine_tuned_model = GPT2LMHeadModel.from_pretrained(OUTPUT_DIR)\n",
    "fine_tuned_tokenizer = GPT2Tokenizer.from_pretrained(OUTPUT_DIR)\n",
    "\n",
    "# Ensure the pad token is set for the loaded tokenizer (it should be saved, but good to double check)\n",
    "if fine_tuned_tokenizer.pad_token is None:\n",
    "    fine_tuned_tokenizer.pad_token = fine_tuned_tokenizer.eos_token\n",
    "    fine_tuned_model.config.pad_token_id = fine_tuned_tokenizer.eos_token_id\n",
    "\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "fine_tuned_model.to(device)\n",
    "fine_tuned_model.eval() # Set the model to evaluation mode\n",
    "\n",
    "# Example Sumerian transliteration to translate\n",
    "sumerian_prompt = \"dingir inana za-me-en\" # \"Goddess Inana, you are\"\n",
    "\n",
    "# Format the prompt exactly as done during training, up to the point where generation should start\n",
    "prompt_for_generation = f\"Sumerian: {sumerian_prompt.strip()} English:\"\n",
    "print(f\"Prompt for generation: '{prompt_for_generation}'\")\n",
    "\n",
    "# Tokenize the prompt\n",
    "input_ids = fine_tuned_tokenizer.encode(prompt_for_generation, return_tensors='pt').to(device)\n",
    "\n",
    "# Generate text\n",
    "# Adjust generation parameters as needed\n",
    "# max_new_tokens is often preferred over max_length for more control over the generated part\n",
    "# For this example, we'll use max_length relative to the prompt.\n",
    "output_sequences = fine_tuned_model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_length=MAX_LENGTH, # Max length of prompt + generated text\n",
    "    # max_new_tokens=50, # Alternative: specify only the number of new tokens to generate\n",
    "    temperature=0.7,          # Controls randomness. Lower is more deterministic.\n",
    "    top_k=50,                 # Considers the top K most probable tokens at each step.\n",
    "    top_p=0.95,               # Nucleus sampling: considers tokens with cumulative probability >= P.\n",
    "    repetition_penalty=1.2,   # Penalizes repetition.\n",
    "    num_return_sequences=1,   # Number of different sequences to generate.\n",
    "    pad_token_id=fine_tuned_tokenizer.eos_token_id # Crucial for generation\n",
    ")\n",
    "\n",
    "# Decode and print the generated text\n",
    "for generated_sequence in output_sequences:\n",
    "    full_text = fine_tuned_tokenizer.decode(generated_sequence, skip_special_tokens=False) # Keep special tokens initially for inspection\n",
    "    # Extract only the generated English part\n",
    "    # This depends on your prompt format. We look for text after \"English: \"\n",
    "    generated_english = full_text.split(prompt_for_generation)[-1]\n",
    "    # Remove the <|endoftext|> token if present at the end\n",
    "    generated_english = generated_english.replace(fine_tuned_tokenizer.eos_token, \"\").strip()\n",
    "\n",
    "    print(f\"Sumerian Input: {sumerian_prompt}\")\n",
    "    print(f\"Generated English: {generated_english}\")\n",
    "    # For more detailed inspection:\n",
    "    # print(f\"Full generated sequence: {full_text}\")\n",
    "\n",
    "print(\"\\nScript finished.\")\n",
    "\n",
    "# To run this script:\n",
    "# 1. Save it as a Python file (e.g., `finetune_sumerian_gpt2.py`).\n",
    "# 2. Make sure you have your Sumerian and English data ready and update `load_your_data()`.\n",
    "# 3. Install the necessary libraries: pip install torch transformers datasets (datasets is not used here but often useful)\n",
    "# 4. Run from your terminal: python finetune_sumerian_gpt2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394f831c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['events.out.tfevents.1747150577.Samu_PC.17328.0', 'events.out.tfevents.1747151032.labP1WE54.4513.0', 'events.out.tfevents.1747152201.labP1WE54.10247.0']\n",
      "No log files found in the specified directory.\n"
     ]
    }
   ],
   "source": [
    "# plot training and validation loss\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Load the training logs\n",
    "log_dir = 'logs'\n",
    "log_files = [f for f in os.listdir(log_dir) if f.endswith('.json')]\n",
    "if not log_files:\n",
    "    print(\"No log files found in the specified directory.\")\n",
    "else:\n",
    "    log_file = os.path.join(log_dir, log_files[0])\n",
    "    with open(log_file, 'r') as f:\n",
    "        logs = json.load(f)\n",
    "\n",
    "    # Extract training and validation loss\n",
    "    train_loss = []\n",
    "    eval_loss = []\n",
    "    for log in logs:\n",
    "        if 'loss' in log:\n",
    "            train_loss.append(log['loss'])\n",
    "        if 'eval_loss' in log:\n",
    "            eval_loss.append(log['eval_loss'])\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(np.arange(len(train_loss)), train_loss, label='Training Loss')\n",
    "    plt.plot(np.arange(len(eval_loss)), eval_loss, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss Over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4bb9adc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generazione del testo...\n",
      "Testo di input: Sumerian: \n",
      "1(u) la₂ 1(diš) udu\n",
      "u₄ 2(u) 8(diš)-kam\n",
      "ki ab-ba-sa₆-ga-ta\n",
      "na-lu₅ i₃-dab₅\n",
      "\n",
      "\n",
      "iti <unk> bi₂-gu₇\n",
      "mu en-unu₆-gal {d}inana unu{ki}ga ba-hun\n",
      "\n",
      "1(u) la₂ 1(diš)\n",
      "Traduzione effettiva: 9 rams,\n",
      "28th day,\n",
      "from Abba-saga,\n",
      "Nalu accepted;\n",
      "month: “ubi-feast,”\n",
      "year: “Enunugal of Inanna of Uruk was installed;”\n",
      "(total:) 9 (rams).\n",
      "\n",
      "--- Testo Generato 1 ---\n",
      " 9 rams,\n",
      "28th day;\n",
      "from Abbasaga\n",
      "Nalu accepted;\n",
      "month: “U\n",
      "Generazione del testo...\n",
      "Testo di input: Sumerian: \n",
      "3(diš) 1/2(diš) gin₂ 1(u) 5(diš) še ku₃-babbar\n",
      "ur₅-še₃ ur{d}en-lil₂-la₂-ta\n",
      "lugal-sa₆-ga\n",
      "u₃ ur{d}šu-mah\n",
      "šu ba-ti\n",
      "\n",
      "iti ku₃ <unk>\n",
      "u₄ 2(u) 2(diš) ba-zal\n",
      "\n",
      "mu si-ma-num₂{ki} ba-hul\n",
      "Traduzione effettiva: 3 1/2 shekels 15 grains of silver,\n",
      "for interest, from Ur-Enlila,\n",
      "Lugal-saga,\n",
      "and Ur-Šumaḫ,\n",
      "received.\n",
      "month: “KuŠIM,”\n",
      "the 22th day passed,\n",
      "year: “Simanum was destroyed.”\n",
      "\n",
      "--- Testo Generato 1 ---\n",
      "Sumerian: \n",
      "3(diš) 1/2(diš) gin₂ 1(u) 5(diš) še ku₃-babbar\n",
      "ur₅-še₃ ur{d}en-lil₂-la₂-ta\n",
      "lugal-sa₆-ga\n",
      "u₃ ur{d}šu-mah\n",
      "šu ba-ti\n",
      "\n",
      "iti ku₃ <unk>\n",
      "u₄ 2(u) 2(diš) ba-zal\n",
      "\n",
      "mu si-ma-num₂{ki} ba-hul English\n",
      "Generazione del testo...\n",
      "Testo di input: Sumerian: \n",
      "6(diš) murgu₂ peš\n",
      "2(geš₂) 4(u) 5(diš) {geš}umbin ma₂\n",
      "6(diš) {geš}u₃-suh₅ {geš}a-da-še₃\n",
      "ki lu₂-kal-la-ta\n",
      "\n",
      "mar-sa-aš\n",
      "kišib₃ lu₂-sa₆-i₃-zu\n",
      "mu hu-uh₂-nu-ri{ki} ba-hul\n",
      "\n",
      "Traduzione effettiva: 6 date palm spines,\n",
      "165 boat ribs(?),\n",
      "6 pine trees for ada-planks,\n",
      "from Lukalla,\n",
      "to the boat house,\n",
      "under seal of Lu-sa-izu.\n",
      "year: “Ḫuḫnuri was destroyed.”\n",
      "\n",
      "--- Testo Generato 1 ---\n",
      "Sumerian: \n",
      "6(diš) murgu₂ peš\n",
      "2(geš₂) 4(u) 5(diš) {geš}umbin ma₂\n",
      "6(diš) {geš}u₃-suh₅ {geš}a-da-še₃\n",
      "ki lu₂-kal-la-ta\n",
      "\n",
      "mar-sa-aš\n",
      "kišib₃ lu₂-sa₆-i₃-zu\n",
      "mu hu-uh₂-nu-ri{ki} ba-hul\n",
      "lugal-\n",
      "Generazione del testo...\n",
      "Testo di input: Sumerian: \n",
      "1/3(diš) kuš gu₄\n",
      "1(diš) sa gu₄\n",
      "2(diš) kuš udu a i₃-ri₂-na\n",
      "1(u) gin₂ še-gin₂\n",
      "\n",
      "...\n",
      "\n",
      "mu si-ma-num₂{ki}\n",
      "Traduzione effettiva: 1/3 oxen hide,\n",
      "1 (bundle of) oxen sinews,\n",
      "2 sheep skins “(soaked) with madder,”\n",
      "10 shekels of glue,\n",
      "year: “Simanum.”\n",
      "\n",
      "--- Testo Generato 1 ---\n",
      " 1/3 oxen hide,\n",
      "1 (bundle of) oxen sinews,\n",
      "2 sheep skins “(soaked with?) emmer,”\n",
      "10 shekels poplar leaves—that were left behind;\n",
      "year: “Siman\n",
      "Generazione del testo...\n",
      "Testo di input: Sumerian: \n",
      "da-da\n",
      "u₃-na-a-du₁₁\n",
      "3(gešʾu) sa gi <unk>\n",
      "giri₃-ni-i₃-sa₆-ra\n",
      "\n",
      "he₂-na-ab-šum₂-mu\n",
      "\n",
      "\n",
      "\n",
      "{d}...gi\n",
      "nita kal-ga\n",
      "lugal uri₅{ki}ma\n",
      "lugal an ub-da limmu₂-ba\n",
      "\n",
      "...\n",
      "ensi₂\n",
      "umma{ki}\n",
      "<unk> zu\n",
      "Traduzione effettiva: To Dada,\n",
      "say:\n",
      "“1800 bundles of fire-reeds,\n",
      "to(?) Girini-isa,\n",
      "let him give.”\n",
      "Šulgi,\n",
      "strong man,\n",
      "the king of Ur,\n",
      "king of the four corners:\n",
      "Ur-Lisi,\n",
      "governor,\n",
      "of Umma,\n",
      "is your servant.\n",
      "\n",
      "--- Testo Generato 1 ---\n",
      "  “House:”\n",
      "Generazione del testo...\n",
      "Errore durante la generazione del testo: Input length of input_ids is 165, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.\n",
      "Generazione del testo...\n",
      "Errore durante la generazione del testo: Input length of input_ids is 372, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.\n",
      "Generazione del testo...\n",
      "Testo di input: Sumerian: \n",
      "1(u) 4(diš) gu₄ geš\n",
      "ki na-sa₆-ta\n",
      "e₂-u₆-e i₃-dab₅\n",
      "\n",
      "iti ki-siki{d}nin-a-zu\n",
      "\n",
      "Traduzione effettiva: 14 plough oxen\n",
      "from Nasa\n",
      "E’u’e received;\n",
      "month “Ki-siki of Ninazu.”\n",
      "\n",
      "--- Testo Generato 1 ---\n",
      " 14 plough oxen\n",
      "from Nasa\n",
      "E’u’e received;\n",
      "month “Ki-sikig Ninazu,”\n",
      "year: “The Nanna priest was installed.”\n",
      "14 plough oxens,\n",
      "from Nasa(\n",
      "Generazione del testo...\n",
      "Testo di input: Sumerian: \n",
      "1(diš) am gu₄\n",
      "1(diš) amar <unk> am\n",
      "u₄ 2(u) 1(diš)-kam\n",
      "ki ab-ba-sa₆-ga-ta\n",
      "lu₂-dingir-ra\n",
      "\n",
      "i₃-dab₅\n",
      "\n",
      "iti šu-eš₅-ša\n",
      "mu {d}amar{d}suen lugal-e ur-bi₂-lum{ki} mu-hul\n",
      "\n",
      "2(diš)\n",
      "Traduzione effettiva: 1 wild ox,\n",
      "1 wild heifer calf,\n",
      "21st day,\n",
      "from Abbasaga\n",
      "did Lu-dingir\n",
      "receive;\n",
      "month “šu’eša,”\n",
      "year: “Amar-Suen, the king, Urbilum destroyed.”\n",
      "(total:) 2 (cattle).\n",
      "\n",
      "--- Testo Generato 1 ---\n",
      " 1 wild ox,\n",
      "1 wild heifer calf,\n",
      "21st day\n",
      "Generazione del testo...\n",
      "Errore durante la generazione del testo: Input length of input_ids is 158, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.\n",
      "Generazione del testo...\n",
      "Testo di input: Sumerian: \n",
      "2(u) {geš}il₂\n",
      "kun-zi-da a-pi₄-sal₄{ki}še₃\n",
      "geme₂ uš-bar-e {geš}il₂\n",
      "...ib₂-il₂\n",
      "ki ku₃-ga-ni-ta\n",
      "\n",
      "šeš-sag₁₀...ba-ti\n",
      "\n",
      "mu {d}amar-suen lugal\n",
      "\n",
      "šeš...\n",
      "dub...\n",
      "dumu lugal-gu₃...\n",
      "Traduzione effettiva: 20 corvée baskets:\n",
      "to the weir of Apisal,\n",
      "did the weaving female laborers the corvée baskets\n",
      "... carry;\n",
      "from Kugani\n",
      "did Šeš-saga receive;\n",
      "year: “Amar-Suen is king.”\n",
      "Šeš-saga,\n",
      "scribe,\n",
      "son of Lugal-Gudea.\n",
      "\n",
      "--- Testo Generato 1 ---\n",
      " xxx\n",
      "xxx\n",
      "xxx\n",
      "basket\n",
      "Generazione del testo...\n",
      "Testo di input: Sumerian: \n",
      "1(diš) udu-nita₂ bar-gal₂ ba-uš₂\n",
      "ki lu₂{d}utu-ta\n",
      "kišib₃ lu₂-kal-la\n",
      "\n",
      "iti pa₄-u₂-e\n",
      "\n",
      "mu hu-uh₂-nu-ri{ki} ba-hul\n",
      "\n",
      "lu₂-kal-la\n",
      "dub-sar\n",
      "dumu ur-e₁₁-e šuš₃\n",
      "Traduzione effettiva: 1 ram, with fleece, slaughtered,\n",
      "from Lu-Utu.\n",
      "under seal of Lukalla;\n",
      "month “Pa’ue,”\n",
      "year: “Ḫuḫnuri was destroyed.”\n",
      "Lukalla,\n",
      "scribe,\n",
      "son of Ur-E’e, chief livestock administrator.\n",
      "\n",
      "--- Testo Generato 1 ---\n",
      " 1 ram, with fleece, slaughtered;\n",
      "from\n",
      "Generazione del testo...\n",
      "Errore durante la generazione del testo: Input length of input_ids is 246, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.\n",
      "Generazione del testo...\n",
      "Errore durante la generazione del testo: Input length of input_ids is 185, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.\n",
      "Generazione del testo...\n",
      "Errore durante la generazione del testo: Input length of input_ids is 165, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.\n",
      "Generazione del testo...\n",
      "Errore durante la generazione del testo: Input length of input_ids is 461, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.\n",
      "Generazione del testo...\n",
      "Errore durante la generazione del testo: Input length of input_ids is 213, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.\n",
      "Generazione del testo...\n",
      "Testo di input: Sumerian: \n",
      "1(u) 5(diš) guruš u₄ 1(diš)-še₃\n",
      "ka-giri₃-da\n",
      "bar-la₂ dub-la₂{d}utu gub-ba\n",
      "ugula lugal-iti-da\n",
      "\n",
      "kišib₃ šeš-a-ni\n",
      "\n",
      "mu {d}šu{d}suen lugal-e na-ru₂-mah mu-du₃\n",
      "\n",
      "šeš-a-ni\n",
      "dub-sar\n",
      "dumu da-da\n",
      "Traduzione effettiva: (1) 15 male laborer days\n",
      "on duty\n",
      "at the basin of the Dubla-Utu (canal) stationed;\n",
      "foreman: Lugal-itida,\n",
      "under seal of Šešani;\n",
      "year: “Šū-Suen, the king, Big Stele erected.”\n",
      "Šešani,\n",
      "scribe,\n",
      "son of Dada.\n",
      "\n",
      "--- Testo Generato 1 ---\n",
      " (1/5) 15\n",
      "Generazione del testo...\n",
      "Errore durante la generazione del testo: Input length of input_ids is 162, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.\n",
      "Generazione del testo...\n",
      "Testo di input: Sumerian: \n",
      "5(geš₂) 2(u) 5(diš) udu\n",
      "4(u) 4(diš) sila₄\n",
      "2(geš₂) 2(u) 4(diš) maš₂-gal\n",
      "1(u) 1(diš) maš₂\n",
      "u₄ 3(diš)-kam\n",
      "ki ab-ba-sa₆-ga-ta\n",
      "Traduzione effettiva: 337 rams,\n",
      "34 male lambs,\n",
      "147 billy goats,\n",
      "11 male kids,\n",
      "3rd day,\n",
      "from Abbasaga\n",
      "did Inta’e’a\n",
      "accept;\n",
      "month “Festival-of-Šulgi,”\n",
      "year: “Šašrum was destroyed.”\n",
      "(total :) 529 small cattle.\n",
      "\n",
      "--- Testo Generato 1 ---\n",
      "Sumerian: \n",
      "5(geš₂) 2(u) 5(diš) udu\n",
      "4(u) 4(diš) sila₄\n",
      "2(geš₂) 2(u) 4(diš) maš₂-gal\n",
      "1(u) 1(diš) maš₂\n",
      "u₄ 3(diš)-kam\n",
      "ki ab-ba-sa₆-ga-ta\n",
      "\n",
      "na-lu₅ i₃-dab₅\n",
      "iti ezem{d}nin...\n",
      "mu en {d}nanna ba-hun\n",
      "\n",
      " (ŋe\n",
      "Generazione del testo...\n",
      "Testo di input: Sumerian: \n",
      "2(diš) sila₄ ga\n",
      "1(diš) kir₁₁ ga\n",
      "u₃-tu-da e₂-udu-ka\n",
      "\n",
      "en-dingir-mu i₃-dab₅\n",
      "u₄ 1(u) la₂ 1(diš)-kam\n",
      "\n",
      "iti šu-eš-ša\n",
      "mu en-mah-gal-an-na en {d}nanna ba-hun\n",
      "Traduzione effettiva: 2 male lambs, suckling,\n",
      "1 female lamb, suckling,\n",
      "new-borns in the sheephouse,\n",
      "Endingirmu accepted;\n",
      "9th day;\n",
      "month: “šuešša,”\n",
      "year: “Enmaḫgalana, en-priestess of Nanna, was installed.”\n",
      "\n",
      "--- Testo Generato 1 ---\n",
      " \n",
      "2 suckling lambs,\n",
      "1 suckling female lamb,\n",
      "new-borns;\n",
      "the 9th day\n",
      "\n",
      "Generazione del testo...\n",
      "Testo di input: Sumerian: \n",
      "1(geš₂) 2(u) 4(barig) še gur\n",
      "še geš e₃-a\n",
      "a-ša₃ u-gir₄{ki}\n",
      "ki ur{d}šara₂-ta\n",
      "\n",
      "kišib₃ {d}šul-gi-mu-dah\n",
      "\n",
      "iti nesag\n",
      "mu si-mu-ru-um{ki} ba-hul\n",
      "\n",
      "{d}šara₂-kam\n",
      "dub-sar\n",
      "{d}šul-gi-mu-dah <unk> zu\n",
      "Traduzione effettiva: 80 gur 4 barig barley,\n",
      "barley winnowed with a stick,\n",
      "field of Ugir,\n",
      "from Ur-Šara\n",
      "Seal of Šulgi-mudaḫ\n",
      "Month: nesag\n",
      "year: “Simurum was destroyed.”\n",
      "Šara-kam,\n",
      "scribe:\n",
      "Šulgi-mudaḫ, your servant.\n",
      "\n",
      "--- Testo Generato 1 ---\n",
      "Sumerian: \n",
      "1(geš₂) 2(u) 4(barig) še gur\n",
      "še geš e₃-a\n",
      "a-ša₃ u-gir₄{ki}\n",
      "ki ur{d}šara₂-ta\n",
      "\n",
      "kišib₃ {d}šul-gi-mu-dah\n",
      "\n",
      "iti nesag\n",
      "mu si-mu-ru-um{ki} ba-hul\n",
      "\n",
      "{d}šara₂-kam\n",
      "dub-sar\n",
      "{d}šul-gi-mu-dah <unk> zu English\n",
      "Generazione del testo...\n",
      "Testo di input: Sumerian: \n",
      "1(gešʾu) 3(u) 3(diš) ad₆(|LU₂.LAGAB×U|) udu maš₂ hi-a\n",
      "ki na-ra-am-i₃-li₂-ta\n",
      "ur-nigar{gar}\n",
      "\n",
      "šu ba-ti\n",
      "\n",
      "...da₃-gu₇\n",
      "...us₂-sa ki-maš{ki}...ur₅-ti{ki} ba-hul\n",
      "Traduzione effettiva: 633 carcasses, various sheep and goats,\n",
      "from Naram-ilī\n",
      "did Ur-nigar\n",
      "receive;\n",
      "month “Gezelle-feast,”\n",
      "year after: “Kimaš and Ḫurti were destroyed.”\n",
      "\n",
      "--- Testo Generato 1 ---\n",
      " 93 goats, various (animals),\n",
      "from Nasa’ili\n",
      "did Ur-niĝ\n",
      "Generazione del testo...\n",
      "Testo di input: Sumerian: \n",
      "pisan dub-ba\n",
      "gaba-ri kišib₃ lu₂ nig₂-dab₅-ke₄-ne\n",
      "ugu₂-a ga₂-ga₂-de₃\n",
      "kišib₃-bi ur{d}nun-gal\n",
      "...\n",
      "\n",
      "...\n",
      "...gal₂\n",
      "giri₃...{d}ba-ba₆\n",
      "u₃...zi\n",
      "iti ezem{d}šu{d}suen\n",
      "...{d}...\n",
      "Traduzione effettiva: Basket-of-tablets:\n",
      "copies, sealed documents, men of the takes,\n",
      "in the debits to be placed’s\n",
      "the sealed documents of Ur-Nungal,\n",
      "...\n",
      "...\n",
      "are here;\n",
      "via ...-Baba,\n",
      "and ...-zi,\n",
      "month “Festival-of-Šu-Suen,”\n",
      "year: “... .”\n",
      "\n",
      "--- Testo Generato 1 ---\n",
      " Basket-of-tablets:\n",
      "xxx\n",
      "\n",
      "Generazione del testo...\n",
      "Errore durante la generazione del testo: Input length of input_ids is 279, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.\n",
      "Generazione del testo...\n",
      "Errore durante la generazione del testo: Input length of input_ids is 296, but `max_length` is set to 150. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.\n",
      "Generazione del testo...\n",
      "Testo di input: Sumerian: \n",
      "pisan dub-ba\n",
      "...<unk> <unk>\n",
      "...<unk> ku₆ še\n",
      "...{d}inanna-ta <unk>\n",
      "...ab\n",
      "...<unk>\n",
      "...\n",
      "\n",
      "...\n",
      "...ba\n",
      "...{d}suen še...<unk> {geš}eb\n",
      "...gal₂\n",
      "...1(diš) mu\n",
      "...ka₃-li₂-šar₃-ri₂\n",
      "...geš-i₃ a-ga-de₃{ki}\n",
      "Traduzione effettiva: Basket-of-tablets:\n",
      "xxx\n",
      "xxx\n",
      "xxx\n",
      "xxx\n",
      "xxx\n",
      "\n",
      "--- Testo Generato 1 ---\n",
      "Sumerian: \n",
      "pisan dub-ba\n",
      "...<unk> <unk>\n",
      "...<unk> ku₆ še\n",
      "...{d}inanna-ta <unk>\n",
      "...ab\n",
      "...<unk>\n",
      "...\n",
      "\n",
      "...\n",
      "...ba\n",
      "...{d}suen še...<unk> {geš}eb\n",
      "...gal₂\n",
      "...1(diš) mu\n",
      "...ka₃-li₂-šar₃-ri₂\n",
      "...geš-i₃ a-ga-de₃{ki}\n",
      "...bi ugula lugal-ku₃-zu\n",
      "Generazione del testo...\n",
      "Testo di input: Sumerian: \n",
      "pisan dub-ba\n",
      "im di-til-la\n",
      "{d}šara₂-kam ensi₂-ka\n",
      "\n",
      "i₃-gal₂\n",
      "mu ša-aš-ru{ki} ba-hul\n",
      "Traduzione effettiva: Basket-of-tablets:\n",
      "tablets, completed legal cases\n",
      "of Šarakam, the governor,\n",
      "are here;\n",
      "year: “Šašru was destroyed.”\n",
      "\n",
      "--- Testo Generato 1 ---\n",
      " Basket-of-tablets:\n",
      "xxx\n",
      "xxx\n",
      "xxx\n",
      "xxx\n",
      "xxx\n",
      "xxx\n",
      "xxx\n",
      "xxx\n",
      "xxx\n",
      "xxx\n",
      "xxx\n",
      "xxx\n",
      "xxx\n",
      "xxx\n",
      "xxx\n",
      "xxx\n",
      "xxx\n",
      "xxx\n",
      "xxx\n",
      "xxx\n",
      "xxx\n",
      "xxx\n",
      "xxx\n",
      "xxx\n",
      "xxx\n",
      "xxx\n",
      "xxx\n",
      "xxx\n",
      "xxx\n",
      "xxx\n",
      "xxx\n",
      "xxx\n",
      "xxx\n",
      "xxx\n",
      "xxx\n",
      "xxx\n",
      "xxx\n",
      "\n",
      "Generazione del testo...\n",
      "Testo di input: Sumerian: \n",
      "pisan dub-ba\n",
      "sag nig₂-gur₁₁\n",
      "ma-ni\n",
      "ša₃ gir₂-su{ki}\n",
      "i₃-gal₂\n",
      "\n",
      "\n",
      "mu ki-maš{ki} ba-hul\n",
      "Traduzione effettiva: Basket-of-tablets:\n",
      "debits,\n",
      "Mani,\n",
      "in Girsu,\n",
      "are here;\n",
      "year: “Kimaš was destroyed.”\n",
      "\n",
      "--- Testo Generato 1 ---\n",
      " Basket-of...\n",
      "debits,\n",
      "Mani,\n",
      "in Girsu,\n",
      "are here;\n",
      "year: “Kimaš was destroyed.”\n",
      "Kimaš,\n",
      "scribe,\n",
      "son of Kimaš.\n",
      "(The account of) Kešgi.\n",
      "Year: “Kimaš was destroyed.”\n",
      "Kimaš,\n",
      "\n",
      "Generazione del testo...\n",
      "Testo di input: Sumerian: \n",
      "pisan dub-ba\n",
      "nig₂-ka₉ ak\n",
      "i-din-er₃-ra\n",
      "iti še-sag₁₁-ku₅-ta\n",
      "iti ezem{d}me-ki-gal₂-še₃\n",
      "...1(u) 2(diš)-kam\n",
      "\n",
      "mu {d}šu{d}suen lugal uri₅{ki}ma-ke₄ ma-da za-ab-ša-li{ki} mu-hul\n",
      "i₃-gal₂\n",
      "Traduzione effettiva: Basket-of-tablets:\n",
      "accounts of\n",
      "Idin-Erra,\n",
      "from month “Harvest”\n",
      "to month “Festival-of-Mekigal,”\n",
      "(a period of) 12 months,\n",
      "year: “Šu-Suen, king of Ur, the lands of Zabšali destroyed,”\n",
      "are here.\n",
      "\n",
      "--- Testo Generato 1 ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load model from a checkpoint\n",
    "model = GPT2LMHeadModel.from_pretrained(f\"{OUTPUT_DIR}/checkpoint-6006\").to(device)\n",
    "\n",
    "for index, row in test_data.iloc[:30,:].iterrows():\n",
    "    prompt_text = f\"Sumerian: {row['transliteration']}\"\n",
    "\n",
    "    # --- Tokenizzazione dell'input ---\n",
    "    input_ids = tokenizer.encode(prompt_text, return_tensors='pt').to(device)\n",
    "\n",
    "    # --- Generazione del testo ---\n",
    "    print(\"Generazione del testo...\")\n",
    "    try:\n",
    "        output_sequences = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_length=150,\n",
    "            temperature=0.7,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            repetition_penalty=1.1,\n",
    "            num_return_sequences=1,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "        print(f\"Testo di input: {prompt_text}\")\n",
    "        print(f\"Traduzione effettiva: {row['translation']}\")\n",
    "        \n",
    "        # --- Decodifica e Stampa ---\n",
    "        for i, generated_sequence in enumerate(output_sequences):\n",
    "            text = tokenizer.decode(generated_sequence, skip_special_tokens=True)\n",
    "            print(f\"\\n--- Testo Generato {i+1} ---\")\n",
    "            print(text.split(\"English:\")[-1])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Errore durante la generazione del testo: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57d05237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking if the destination folder 'sumerian_gpt2_finetuned' already exists on OneDrive...\n",
      "Conflict detected: 'sumerian_gpt2_finetuned' already exists. Using 'sumerian_gpt2_finetuned_1' instead.\n",
      "Moving 'sumerian_gpt2_finetuned' to OneDrive: 'onedrive:AI-project/sumerian_gpt2_finetuned_1'...\n",
      "rclone command: rclone move sumerian_gpt2_finetuned onedrive:AI-project/sumerian_gpt2_finetuned_1 --create-empty-src-dirs -P\n",
      "SUCCESS: Folder 'sumerian_gpt2_finetuned' moved successfully to OneDrive.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rclone import move_folder_to_onedrive\n",
    "\n",
    "move_folder_to_onedrive('sumerian_gpt2_finetuned')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azureml_py310_sdkv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
