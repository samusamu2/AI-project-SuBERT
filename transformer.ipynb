{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa22e854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Source Input Shape: torch.Size([32, 60])\n",
      "Target Input Shape: torch.Size([32, 55])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (32) must match the size of tensor b (60) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 600\u001b[0m\n\u001b[0;32m    596\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget Input Shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtgt_input\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# (BATCH_SIZE, TGT_SEQ_LEN)\u001b[39;00m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;66;03m# --- Forward Pass ---\u001b[39;00m\n\u001b[0;32m    599\u001b[0m \u001b[38;5;66;03m# The model expects src/tgt padding indices to create masks internally\u001b[39;00m\n\u001b[1;32m--> 600\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_pad_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPAD_IDX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_pad_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPAD_IDX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    602\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput Shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# Should be (BATCH_SIZE, TGT_SEQ_LEN, TGT_VOCAB_SIZE)\u001b[39;00m\n\u001b[0;32m    604\u001b[0m \u001b[38;5;66;03m# --- Inference Example (Greedy Decoding - very basic) ---\u001b[39;00m\n\u001b[0;32m    605\u001b[0m \u001b[38;5;66;03m# During inference, we generate the target sequence token by token.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[1], line 533\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, src, tgt, src_pad_idx, tgt_pad_idx)\u001b[0m\n\u001b[0;32m    528\u001b[0m tgt_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_encoder(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtgt_embedding(tgt)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_model))\n\u001b[0;32m    529\u001b[0m \u001b[38;5;66;03m# Shape after pos_encoder: (seq_len, batch_size, d_model)\u001b[39;00m\n\u001b[0;32m    530\u001b[0m \n\u001b[0;32m    531\u001b[0m \u001b[38;5;66;03m# 3. Encoder\u001b[39;00m\n\u001b[0;32m    532\u001b[0m \u001b[38;5;66;03m# memory shape: (src_seq_len, batch_size, d_model)\u001b[39;00m\n\u001b[1;32m--> 533\u001b[0m memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_padding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;66;03m# 4. Decoder\u001b[39;00m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;66;03m# decoder_output shape: (tgt_seq_len, batch_size, d_model)\u001b[39;00m\n\u001b[0;32m    537\u001b[0m decoder_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(tgt_emb, memory,\n\u001b[0;32m    538\u001b[0m                               tgt_mask\u001b[38;5;241m=\u001b[39mtgt_mask, \u001b[38;5;66;03m# Look-ahead mask\u001b[39;00m\n\u001b[0;32m    539\u001b[0m                               tgt_key_padding_mask\u001b[38;5;241m=\u001b[39mtgt_padding_mask, \u001b[38;5;66;03m# Target padding mask\u001b[39;00m\n\u001b[0;32m    540\u001b[0m                               memory_key_padding_mask\u001b[38;5;241m=\u001b[39msrc_padding_mask) \u001b[38;5;66;03m# Source padding mask\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[1], line 352\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[1;34m(self, src, mask, src_key_padding_mask)\u001b[0m\n\u001b[0;32m    350\u001b[0m output \u001b[38;5;241m=\u001b[39m src\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m--> 352\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    355\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(output)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[1], line 240\u001b[0m, in \u001b[0;36mEncoderLayer.forward\u001b[1;34m(self, src, src_mask, src_key_padding_mask)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;124;03mForward pass for the Encoder Layer.\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;124;03m    Tensor: Output tensor from the encoder layer, shape (src_seq_len, batch_size, d_model).\u001b[39;00m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    234\u001b[0m \u001b[38;5;66;03m# 1. Multi-Head Self-Attention + Add & Norm\u001b[39;00m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;66;03m# The mask passed here should combine src_mask and src_key_padding_mask if needed.\u001b[39;00m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;66;03m# For self-attention, query, key, and value are all 'src'.\u001b[39;00m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;66;03m# Note: PyTorch's MultiheadAttention expects mask shape (N, S) for key padding\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;66;03m#       and (L, S) or (N*num_heads, L, S) for attn_mask.\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;66;03m#       Our custom implementation handles padding mask broadcasting.\u001b[39;00m\n\u001b[1;32m--> 240\u001b[0m attn_output, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Pass padding mask\u001b[39;00m\n\u001b[0;32m    241\u001b[0m src \u001b[38;5;241m=\u001b[39m src \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(attn_output) \u001b[38;5;66;03m# Residual connection\u001b[39;00m\n\u001b[0;32m    242\u001b[0m src \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(src) \u001b[38;5;66;03m# Layer normalization\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[1], line 155\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[1;34m(self, query, key, value, mask)\u001b[0m\n\u001b[0;32m    150\u001b[0m V \u001b[38;5;241m=\u001b[39m V\u001b[38;5;241m.\u001b[39mview(seq_len_v, batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnhead, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_k)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m    152\u001b[0m \u001b[38;5;66;03m# 3. Scaled dot-product attention\u001b[39;00m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;66;03m# attn_output shape: (batch_size, nhead, seq_len_q, d_k)\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;66;03m# attn_weights shape: (batch_size, nhead, seq_len_q, seq_len_k)\u001b[39;00m\n\u001b[1;32m--> 155\u001b[0m attn_output, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_scaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mV\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;66;03m# 4. Concatenate heads and project\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;66;03m# (batch_size, nhead, seq_len_q, d_k) -> (seq_len_q, batch_size, nhead, d_k) -> (seq_len_q, batch_size, d_model)\u001b[39;00m\n\u001b[0;32m    159\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(seq_len_q, batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_model)\n",
      "Cell \u001b[1;32mIn[1], line 109\u001b[0m, in \u001b[0;36mMultiHeadAttention._scaled_dot_product_attention\u001b[1;34m(self, Q, K, V, mask)\u001b[0m\n\u001b[0;32m    107\u001b[0m          mask \u001b[38;5;241m=\u001b[39m mask\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;66;03m# Apply the mask by setting masked positions to a very small number (-inf)\u001b[39;00m\n\u001b[1;32m--> 109\u001b[0m     attn_scores \u001b[38;5;241m=\u001b[39m \u001b[43mattn_scores\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasked_fill\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1e9\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Use -1e9 instead of float('-inf') for stability\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;66;03m# Apply softmax to get attention weights\u001b[39;00m\n\u001b[0;32m    112\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(attn_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# Shape: (batch_size, nhead, seq_len_q, seq_len_k)\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (32) must match the size of tensor b (60) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torch import Tensor\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Injects positional information into the input embeddings.\n",
    "    This helps the model understand the order of tokens in a sequence,\n",
    "    as the self-attention mechanism itself is permutation-invariant.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model (int): The dimensionality of the embeddings (must be even).\n",
    "            dropout (float): Dropout probability.\n",
    "            max_len (int): The maximum possible length of the input sequences.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if d_model % 2 != 0:\n",
    "            raise ValueError(f\"d_model must be even, got {d_model}\")\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Create a positional encoding matrix of shape (max_len, d_model)\n",
    "        position = torch.arange(max_len).unsqueeze(1) # Shape: (max_len, 1)\n",
    "        # Calculate the division term for the sinusoidal functions\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)) # Shape: (d_model / 2)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        # Apply sine to even indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        # Apply cosine to odd indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        # Add a batch dimension and register as a buffer (not a model parameter)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1) # Shape: (max_len, 1, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): Input embeddings of shape (seq_len, batch_size, d_model).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Embeddings with added positional information, same shape as input.\n",
    "        \"\"\"\n",
    "        # Add positional encoding to the input embeddings\n",
    "        # x.size(0) is the sequence length of the current batch\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the Multi-Head Attention mechanism as described in\n",
    "    \"Attention Is All You Need\". Allows the model to jointly attend\n",
    "    to information from different representation subspaces at different positions.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, nhead: int, dropout: float = 0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model (int): Total dimension of the model.\n",
    "            nhead (int): Number of parallel attention heads. d_model must be divisible by nhead.\n",
    "            dropout (float): Dropout probability.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if d_model % nhead != 0:\n",
    "            raise ValueError(f\"d_model ({d_model}) must be divisible by nhead ({nhead})\")\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "        self.d_k = d_model // nhead # Dimension of each head\n",
    "\n",
    "        # Linear layers for Query, Key, Value, and the final output projection\n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_o = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = math.sqrt(self.d_k)\n",
    "\n",
    "    def _scaled_dot_product_attention(self, Q: Tensor, K: Tensor, V: Tensor, mask: Optional[Tensor] = None) -> Tuple[Tensor, Tensor]:\n",
    "        \"\"\"\n",
    "        Computes the scaled dot-product attention.\n",
    "\n",
    "        Args:\n",
    "            Q (Tensor): Queries, shape (batch_size, nhead, seq_len_q, d_k)\n",
    "            K (Tensor): Keys, shape (batch_size, nhead, seq_len_k, d_k)\n",
    "            V (Tensor): Values, shape (batch_size, nhead, seq_len_v, d_k) (seq_len_k == seq_len_v)\n",
    "            mask (Optional[Tensor]): Mask to prevent attention to certain positions.\n",
    "                                     Shape (batch_size, 1, seq_len_q, seq_len_k) or (seq_len_q, seq_len_k)\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Tensor, Tensor]: Output tensor and attention weights.\n",
    "                                    Output shape: (batch_size, nhead, seq_len_q, d_k)\n",
    "                                    Attention weights shape: (batch_size, nhead, seq_len_q, seq_len_k)\n",
    "        \"\"\"\n",
    "        # Calculate attention scores (QK^T / sqrt(d_k))\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale # Shape: (batch_size, nhead, seq_len_q, seq_len_k)\n",
    "\n",
    "        # Apply mask if provided (e.g., for padding or future tokens)\n",
    "        if mask is not None:\n",
    "            # Ensure mask has compatible dimensions for broadcasting\n",
    "            if mask.dim() == 2: # Sequence mask (tgt_len, src_len) -> (1, 1, tgt_len, src_len)\n",
    "                 mask = mask.unsqueeze(0).unsqueeze(0)\n",
    "            elif mask.dim() == 3: # Padding mask (batch_size, 1, src_len) -> (batch_size, 1, 1, src_len)\n",
    "                 mask = mask.unsqueeze(2)\n",
    "            # Apply the mask by setting masked positions to a very small number (-inf)\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9) # Use -1e9 instead of float('-inf') for stability\n",
    "\n",
    "        # Apply softmax to get attention weights\n",
    "        attn_weights = torch.softmax(attn_scores, dim=-1) # Shape: (batch_size, nhead, seq_len_q, seq_len_k)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Multiply weights by values (Weighted sum)\n",
    "        output = torch.matmul(attn_weights, V) # Shape: (batch_size, nhead, seq_len_q, d_k)\n",
    "\n",
    "        return output, attn_weights\n",
    "\n",
    "    def forward(self, query: Tensor, key: Tensor, value: Tensor, mask: Optional[Tensor] = None) -> Tuple[Tensor, Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass for Multi-Head Attention.\n",
    "\n",
    "        Args:\n",
    "            query (Tensor): Query tensor, shape (seq_len_q, batch_size, d_model).\n",
    "            key (Tensor): Key tensor, shape (seq_len_k, batch_size, d_model).\n",
    "            value (Tensor): Value tensor, shape (seq_len_v, batch_size, d_model) (seq_len_k == seq_len_v).\n",
    "            mask (Optional[Tensor]): Mask, shape depends on use case (padding or look-ahead).\n",
    "                                     Typical shapes: (tgt_seq_len, src_seq_len) for decoder self-attention mask,\n",
    "                                     (batch_size, src_seq_len) for encoder padding mask.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Tensor, Tensor]: The attention output and the attention weights.\n",
    "                                    Output shape: (seq_len_q, batch_size, d_model)\n",
    "                                    Attention weights shape: (batch_size, nhead, seq_len_q, seq_len_k)\n",
    "        \"\"\"\n",
    "        seq_len_q, batch_size, _ = query.size()\n",
    "        seq_len_k, _, _ = key.size()\n",
    "        seq_len_v, _, _ = value.size() # Should be same as seq_len_k\n",
    "\n",
    "        # 1. Linear projections\n",
    "        Q = self.W_q(query) # Shape: (seq_len_q, batch_size, d_model)\n",
    "        K = self.W_k(key)   # Shape: (seq_len_k, batch_size, d_model)\n",
    "        V = self.W_v(value) # Shape: (seq_len_v, batch_size, d_model)\n",
    "\n",
    "        # 2. Reshape for multi-head attention\n",
    "        # (seq_len, batch_size, d_model) -> (seq_len, batch_size, nhead, d_k) -> (batch_size, nhead, seq_len, d_k)\n",
    "        Q = Q.view(seq_len_q, batch_size, self.nhead, self.d_k).permute(1, 2, 0, 3)\n",
    "        K = K.view(seq_len_k, batch_size, self.nhead, self.d_k).permute(1, 2, 0, 3)\n",
    "        V = V.view(seq_len_v, batch_size, self.nhead, self.d_k).permute(1, 2, 0, 3)\n",
    "\n",
    "        # 3. Scaled dot-product attention\n",
    "        # attn_output shape: (batch_size, nhead, seq_len_q, d_k)\n",
    "        # attn_weights shape: (batch_size, nhead, seq_len_q, seq_len_k)\n",
    "        attn_output, attn_weights = self._scaled_dot_product_attention(Q, K, V, mask=mask)\n",
    "\n",
    "        # 4. Concatenate heads and project\n",
    "        # (batch_size, nhead, seq_len_q, d_k) -> (seq_len_q, batch_size, nhead, d_k) -> (seq_len_q, batch_size, d_model)\n",
    "        attn_output = attn_output.permute(2, 0, 1, 3).contiguous().view(seq_len_q, batch_size, self.d_model)\n",
    "\n",
    "        # 5. Final linear projection\n",
    "        output = self.W_o(attn_output) # Shape: (seq_len_q, batch_size, d_model)\n",
    "\n",
    "        return output, attn_weights\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the two-layer feed-forward network applied to each position separately and identically.\n",
    "    FFN(x) = max(0, xW_1 + b_1)W_2 + b_2\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model (int): Input and output dimension.\n",
    "            d_ff (int): Inner dimension of the feed-forward layer.\n",
    "            dropout (float): Dropout probability.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.activation = nn.ReLU() # Or GELU\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): Input tensor, shape (seq_len, batch_size, d_model).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output tensor, same shape as input.\n",
    "        \"\"\"\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Represents one layer of the Transformer Encoder.\n",
    "    Consists of a self-attention mechanism followed by a position-wise feed-forward network.\n",
    "    Includes residual connections and layer normalization.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, nhead: int, d_ff: int, dropout: float = 0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model (int): The number of expected features in the input.\n",
    "            nhead (int): The number of heads in the multiheadattention models.\n",
    "            d_ff (int): The dimension of the feedforward network model.\n",
    "            dropout (float): The dropout value.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, nhead, dropout=dropout)\n",
    "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout=dropout)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for the Encoder Layer.\n",
    "\n",
    "        Args:\n",
    "            src (Tensor): Input sequence to the encoder layer, shape (src_seq_len, batch_size, d_model).\n",
    "            src_mask (Optional[Tensor]): Mask for the source sequence (rarely used in encoder).\n",
    "            src_key_padding_mask (Optional[Tensor]): Mask for padding tokens in the source sequence,\n",
    "                                                      shape (batch_size, src_seq_len).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output tensor from the encoder layer, shape (src_seq_len, batch_size, d_model).\n",
    "        \"\"\"\n",
    "        # 1. Multi-Head Self-Attention + Add & Norm\n",
    "        # The mask passed here should combine src_mask and src_key_padding_mask if needed.\n",
    "        # For self-attention, query, key, and value are all 'src'.\n",
    "        # Note: PyTorch's MultiheadAttention expects mask shape (N, S) for key padding\n",
    "        #       and (L, S) or (N*num_heads, L, S) for attn_mask.\n",
    "        #       Our custom implementation handles padding mask broadcasting.\n",
    "        attn_output, _ = self.self_attn(src, src, src, mask=src_key_padding_mask) # Pass padding mask\n",
    "        src = src + self.dropout1(attn_output) # Residual connection\n",
    "        src = self.norm1(src) # Layer normalization\n",
    "\n",
    "        # 2. Feed Forward + Add & Norm\n",
    "        ff_output = self.feed_forward(src)\n",
    "        src = src + self.dropout2(ff_output) # Residual connection\n",
    "        src = self.norm2(src) # Layer normalization\n",
    "\n",
    "        return src\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Represents one layer of the Transformer Decoder.\n",
    "    Consists of self-attention, encoder-decoder attention, and a feed-forward network.\n",
    "    Includes residual connections and layer normalization.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, nhead: int, d_ff: int, dropout: float = 0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model (int): The number of expected features in the input.\n",
    "            nhead (int): The number of heads in the multiheadattention models.\n",
    "            d_ff (int): The dimension of the feedforward network model.\n",
    "            dropout (float): The dropout value.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, nhead, dropout=dropout)\n",
    "        self.multihead_attn = MultiHeadAttention(d_model, nhead, dropout=dropout) # Encoder-Decoder Attention\n",
    "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout=dropout)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, tgt: Tensor, memory: Tensor,\n",
    "                tgt_mask: Optional[Tensor] = None,\n",
    "                memory_mask: Optional[Tensor] = None,\n",
    "                tgt_key_padding_mask: Optional[Tensor] = None,\n",
    "                memory_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for the Decoder Layer.\n",
    "\n",
    "        Args:\n",
    "            tgt (Tensor): Target sequence input to the decoder layer, shape (tgt_seq_len, batch_size, d_model).\n",
    "            memory (Tensor): Output from the encoder (memory), shape (src_seq_len, batch_size, d_model).\n",
    "            tgt_mask (Optional[Tensor]): Mask to prevent attending to future tokens in the target sequence,\n",
    "                                         shape (tgt_seq_len, tgt_seq_len).\n",
    "            memory_mask (Optional[Tensor]): Mask for the encoder output (rarely used).\n",
    "            tgt_key_padding_mask (Optional[Tensor]): Mask for padding tokens in the target sequence,\n",
    "                                                      shape (batch_size, tgt_seq_len).\n",
    "            memory_key_padding_mask (Optional[Tensor]): Mask for padding tokens in the source sequence\n",
    "                                                         (passed from encoder), shape (batch_size, src_seq_len).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output tensor from the decoder layer, shape (tgt_seq_len, batch_size, d_model).\n",
    "        \"\"\"\n",
    "        # 1. Masked Multi-Head Self-Attention (on target sequence) + Add & Norm\n",
    "        # Combine tgt_mask (look-ahead) and tgt_key_padding_mask\n",
    "        # Our attention implementation expects the combined mask.\n",
    "        # If using PyTorch's MultiheadAttention: pass tgt_mask to attn_mask, tgt_key_padding_mask to key_padding_mask\n",
    "        self_attn_output, _ = self.self_attn(tgt, tgt, tgt, mask=tgt_mask) # Pass look-ahead mask\n",
    "        tgt = tgt + self.dropout1(self_attn_output) # Residual connection\n",
    "        tgt = self.norm1(tgt) # Layer normalization\n",
    "\n",
    "        # 2. Multi-Head Encoder-Decoder Attention + Add & Norm\n",
    "        # Query=tgt, Key=memory, Value=memory\n",
    "        # Mask comes from the source padding (memory_key_padding_mask)\n",
    "        enc_dec_attn_output, _ = self.multihead_attn(tgt, memory, memory, mask=memory_key_padding_mask) # Pass src padding mask\n",
    "        tgt = tgt + self.dropout2(enc_dec_attn_output) # Residual connection\n",
    "        tgt = self.norm2(tgt) # Layer normalization\n",
    "\n",
    "        # 3. Feed Forward + Add & Norm\n",
    "        ff_output = self.feed_forward(tgt)\n",
    "        tgt = tgt + self.dropout3(ff_output) # Residual connection\n",
    "        tgt = self.norm3(tgt) # Layer normalization\n",
    "\n",
    "        return tgt\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    The Transformer Encoder stack.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder_layer: nn.Module, num_layers: int, norm: Optional[nn.Module] = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            encoder_layer (nn.Module): An instance of the EncoderLayer.\n",
    "            num_layers (int): The number of sub-encoder-layers in the encoder.\n",
    "            norm (Optional[nn.Module]): An optional layer normalization module.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([encoder_layer for _ in range(num_layers)])\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = norm\n",
    "\n",
    "    def forward(self, src: Tensor, mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        \"\"\"\n",
    "        Pass the input through the encoder layers.\n",
    "\n",
    "        Args:\n",
    "            src (Tensor): The sequence to the encoder, shape (src_seq_len, batch_size, d_model).\n",
    "            mask (Optional[Tensor]): The mask for the src sequence (rarely used).\n",
    "            src_key_padding_mask (Optional[Tensor]): The mask for the src keys per batch,\n",
    "                                                      shape (batch_size, src_seq_len).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output tensor from the encoder, shape (src_seq_len, batch_size, d_model).\n",
    "        \"\"\"\n",
    "        output = src\n",
    "        for mod in self.layers:\n",
    "            output = mod(output, src_mask=mask, src_key_padding_mask=src_key_padding_mask)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    The Transformer Decoder stack.\n",
    "    \"\"\"\n",
    "    def __init__(self, decoder_layer: nn.Module, num_layers: int, norm: Optional[nn.Module] = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            decoder_layer (nn.Module): An instance of the DecoderLayer.\n",
    "            num_layers (int): The number of sub-decoder-layers in the decoder.\n",
    "            norm (Optional[nn.Module]): An optional layer normalization module.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([decoder_layer for _ in range(num_layers)])\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = norm\n",
    "\n",
    "    def forward(self, tgt: Tensor, memory: Tensor,\n",
    "                tgt_mask: Optional[Tensor] = None,\n",
    "                memory_mask: Optional[Tensor] = None,\n",
    "                tgt_key_padding_mask: Optional[Tensor] = None,\n",
    "                memory_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        \"\"\"\n",
    "        Pass the inputs (and mask) through the decoder layer stack.\n",
    "\n",
    "        Args:\n",
    "            tgt (Tensor): The sequence to the decoder, shape (tgt_seq_len, batch_size, d_model).\n",
    "            memory (Tensor): The sequence from the last layer of the encoder, shape (src_seq_len, batch_size, d_model).\n",
    "            tgt_mask (Optional[Tensor]): The mask for the tgt sequence, shape (tgt_seq_len, tgt_seq_len).\n",
    "            memory_mask (Optional[Tensor]): The mask for the memory sequence (rarely used).\n",
    "            tgt_key_padding_mask (Optional[Tensor]): The mask for the tgt keys per batch,\n",
    "                                                      shape (batch_size, tgt_seq_len).\n",
    "            memory_key_padding_mask (Optional[Tensor]): The mask for the memory keys per batch,\n",
    "                                                         shape (batch_size, src_seq_len).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output tensor from the decoder, shape (tgt_seq_len, batch_size, d_model).\n",
    "        \"\"\"\n",
    "        output = tgt\n",
    "        for mod in self.layers:\n",
    "            output = mod(output, memory,\n",
    "                         tgt_mask=tgt_mask,\n",
    "                         memory_mask=memory_mask,\n",
    "                         tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                         memory_key_padding_mask=memory_key_padding_mask)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    The main Transformer model, combining Encoder and Decoder.\n",
    "    Based on the paper \"Attention Is All You Need\".\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int = 512, nhead: int = 8, num_encoder_layers: int = 6,\n",
    "                 num_decoder_layers: int = 6, d_ff: int = 2048, dropout: float = 0.1,\n",
    "                 activation: str = \"relu\", # activation is used in PositionwiseFeedForward, not directly here\n",
    "                 src_vocab_size: int = 10000, tgt_vocab_size: int = 10000,\n",
    "                 max_seq_len: int = 512):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model (int): The number of expected features in the encoder/decoder inputs.\n",
    "            nhead (int): The number of heads in the multiheadattention models.\n",
    "            num_encoder_layers (int): The number of sub-encoder-layers in the encoder.\n",
    "            num_decoder_layers (int): The number of sub-decoder-layers in the decoder.\n",
    "            d_ff (int): The dimension of the feedforward network model.\n",
    "            dropout (float): The dropout value.\n",
    "            activation (str): The activation function of encoder/decoder intermediate layer, relu or gelu. (Not directly used here, passed to layers)\n",
    "            src_vocab_size (int): Size of the source vocabulary.\n",
    "            tgt_vocab_size (int): Size of the target vocabulary.\n",
    "            max_seq_len (int): Maximum sequence length for positional encoding.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # --- Embeddings and Positional Encoding ---\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout, max_len=max_seq_len)\n",
    "\n",
    "        # --- Encoder ---\n",
    "        encoder_layer = EncoderLayer(d_model, nhead, d_ff, dropout)\n",
    "        encoder_norm = nn.LayerNorm(d_model)\n",
    "        self.encoder = Encoder(encoder_layer, num_encoder_layers, encoder_norm)\n",
    "\n",
    "        # --- Decoder ---\n",
    "        decoder_layer = DecoderLayer(d_model, nhead, d_ff, dropout)\n",
    "        decoder_norm = nn.LayerNorm(d_model)\n",
    "        self.decoder = Decoder(decoder_layer, num_decoder_layers, decoder_norm)\n",
    "\n",
    "        # --- Final Output Layer ---\n",
    "        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "        # --- Initialization ---\n",
    "        self._reset_parameters()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        \"\"\"Initiate parameters in the transformer model.\"\"\"\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz: int) -> Tensor:\n",
    "        \"\"\"\n",
    "        Generates a square mask for the sequence. The masked positions are filled with float('-inf').\n",
    "        Unmasked positions are filled with float(0.0).\n",
    "        Used for the decoder's self-attention to prevent looking ahead.\n",
    "        Shape: (sz, sz)\n",
    "        \"\"\"\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def _create_padding_mask(self, sequence: Tensor, pad_idx: int) -> Tensor:\n",
    "        \"\"\"\n",
    "        Creates a mask for padding tokens.\n",
    "        Args:\n",
    "            sequence (Tensor): Input sequence tensor, shape (batch_size, seq_len).\n",
    "            pad_idx (int): Index of the padding token.\n",
    "        Returns:\n",
    "            Tensor: Padding mask, shape (batch_size, 1, seq_len). Returns 0 where padded, 1 otherwise.\n",
    "                    (Note: Attention mechanisms often expect mask==0 for positions to *ignore*)\n",
    "                    Let's return shape (batch_size, seq_len) where True means padding.\n",
    "                    The attention layer will handle broadcasting.\n",
    "        \"\"\"\n",
    "        # return (sequence == pad_idx).unsqueeze(1) # Shape: (batch_size, 1, seq_len)\n",
    "        return (sequence == pad_idx) # Shape: (batch_size, seq_len)\n",
    "\n",
    "\n",
    "    def forward(self, src: Tensor, tgt: Tensor,\n",
    "                src_pad_idx: int = 0, tgt_pad_idx: int = 0) -> Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the Transformer model.\n",
    "\n",
    "        Args:\n",
    "            src (Tensor): Source sequence tensor, shape (batch_size, src_seq_len).\n",
    "                          Contains token indices.\n",
    "            tgt (Tensor): Target sequence tensor, shape (batch_size, tgt_seq_len).\n",
    "                          Contains token indices. Typically starts with <SOS> token.\n",
    "            src_pad_idx (int): Index of the padding token in the source vocabulary.\n",
    "            tgt_pad_idx (int): Index of the padding token in the target vocabulary.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output tensor, shape (batch_size, tgt_seq_len, tgt_vocab_size).\n",
    "                    Represents the probability distribution over the target vocabulary for each position.\n",
    "        \"\"\"\n",
    "        # 1. Create Masks\n",
    "        src_seq_len = src.shape[1]\n",
    "        tgt_seq_len = tgt.shape[1]\n",
    "\n",
    "        # Look-ahead mask for target self-attention (prevents attending to future tokens)\n",
    "        # Shape: (tgt_seq_len, tgt_seq_len)\n",
    "        tgt_mask = self._generate_square_subsequent_mask(tgt_seq_len).to(tgt.device)\n",
    "\n",
    "        # Padding masks (identify padding tokens)\n",
    "        # Shape: (batch_size, src_seq_len) and (batch_size, tgt_seq_len)\n",
    "        # True where padded, False otherwise.\n",
    "        src_padding_mask = self._create_padding_mask(src, src_pad_idx)\n",
    "        tgt_padding_mask = self._create_padding_mask(tgt, tgt_pad_idx)\n",
    "\n",
    "\n",
    "        # 2. Embeddings and Positional Encoding\n",
    "        # Input shape: (batch_size, seq_len)\n",
    "        # Embedding output shape: (batch_size, seq_len, d_model)\n",
    "        # Transformer layers expect (seq_len, batch_size, d_model), so transpose.\n",
    "        src_emb = self.pos_encoder(self.src_embedding(src).transpose(0, 1) * math.sqrt(self.d_model))\n",
    "        tgt_emb = self.pos_encoder(self.tgt_embedding(tgt).transpose(0, 1) * math.sqrt(self.d_model))\n",
    "        # Shape after pos_encoder: (seq_len, batch_size, d_model)\n",
    "\n",
    "        # 3. Encoder\n",
    "        # memory shape: (src_seq_len, batch_size, d_model)\n",
    "        memory = self.encoder(src_emb, src_key_padding_mask=src_padding_mask)\n",
    "\n",
    "        # 4. Decoder\n",
    "        # decoder_output shape: (tgt_seq_len, batch_size, d_model)\n",
    "        decoder_output = self.decoder(tgt_emb, memory,\n",
    "                                      tgt_mask=tgt_mask, # Look-ahead mask\n",
    "                                      tgt_key_padding_mask=tgt_padding_mask, # Target padding mask\n",
    "                                      memory_key_padding_mask=src_padding_mask) # Source padding mask\n",
    "\n",
    "        # 5. Final Linear Layer\n",
    "        # Input shape: (tgt_seq_len, batch_size, d_model)\n",
    "        # Output shape: (tgt_seq_len, batch_size, tgt_vocab_size)\n",
    "        output = self.fc_out(decoder_output)\n",
    "\n",
    "        # Transpose back to (batch_size, tgt_seq_len, tgt_vocab_size) for standard loss calculation\n",
    "        return output.transpose(0, 1)\n",
    "\n",
    "\n",
    "# --- Example Usage ---\n",
    "if __name__ == '__main__':\n",
    "    # Hyperparameters (example values)\n",
    "    SRC_VOCAB_SIZE = 5000\n",
    "    TGT_VOCAB_SIZE = 6000\n",
    "    D_MODEL = 512       # Embedding dimension, must be divisible by NHEAD\n",
    "    NHEAD = 8           # Number of attention heads\n",
    "    NUM_ENCODER_LAYERS = 3 # Number of encoder layers\n",
    "    NUM_DECODER_LAYERS = 3 # Number of decoder layers\n",
    "    D_FF = 2048         # Dimension of the feed-forward layer\n",
    "    MAX_SEQ_LEN = 100   # Maximum sequence length\n",
    "    DROPOUT = 0.1\n",
    "    BATCH_SIZE = 32\n",
    "    SRC_SEQ_LEN = 60    # Example source sequence length\n",
    "    TGT_SEQ_LEN = 55    # Example target sequence length (e.g., during training with teacher forcing)\n",
    "    PAD_IDX = 0         # Example padding index\n",
    "\n",
    "    # Device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Create the model\n",
    "    transformer_model = Transformer(\n",
    "        d_model=D_MODEL,\n",
    "        nhead=NHEAD,\n",
    "        num_encoder_layers=NUM_ENCODER_LAYERS,\n",
    "        num_decoder_layers=NUM_DECODER_LAYERS,\n",
    "        d_ff=D_FF,\n",
    "        dropout=DROPOUT,\n",
    "        src_vocab_size=SRC_VOCAB_SIZE,\n",
    "        tgt_vocab_size=TGT_VOCAB_SIZE,\n",
    "        max_seq_len=MAX_SEQ_LEN\n",
    "    ).to(device)\n",
    "\n",
    "    # Generate dummy input data\n",
    "    # Source sequence (batch_size, src_seq_len)\n",
    "    src_input = torch.randint(1, SRC_VOCAB_SIZE, (BATCH_SIZE, SRC_SEQ_LEN), device=device)\n",
    "    # Target sequence (batch_size, tgt_seq_len) - for training (teacher forcing)\n",
    "    tgt_input = torch.randint(1, TGT_VOCAB_SIZE, (BATCH_SIZE, TGT_SEQ_LEN), device=device)\n",
    "\n",
    "    # Add some padding for demonstration\n",
    "    src_input[0, -10:] = PAD_IDX # Pad last 10 tokens of first sequence in batch\n",
    "    tgt_input[1, -5:] = PAD_IDX  # Pad last 5 tokens of second sequence in batch\n",
    "\n",
    "    print(f\"Source Input Shape: {src_input.shape}\") # (BATCH_SIZE, SRC_SEQ_LEN)\n",
    "    print(f\"Target Input Shape: {tgt_input.shape}\") # (BATCH_SIZE, TGT_SEQ_LEN)\n",
    "\n",
    "    # --- Forward Pass ---\n",
    "    # The model expects src/tgt padding indices to create masks internally\n",
    "    output = transformer_model(src_input, tgt_input, src_pad_idx=PAD_IDX, tgt_pad_idx=PAD_IDX)\n",
    "\n",
    "    print(f\"Output Shape: {output.shape}\") # Should be (BATCH_SIZE, TGT_SEQ_LEN, TGT_VOCAB_SIZE)\n",
    "\n",
    "    # --- Inference Example (Greedy Decoding - very basic) ---\n",
    "    # During inference, we generate the target sequence token by token.\n",
    "    print(\"\\n--- Basic Inference Example (Greedy) ---\")\n",
    "    transformer_model.eval() # Set model to evaluation mode\n",
    "\n",
    "    # Start with a <SOS> token (assuming index 1)\n",
    "    tgt_sequence = torch.ones((BATCH_SIZE, 1), dtype=torch.long, device=device) * 1 # Shape: (BATCH_SIZE, 1)\n",
    "\n",
    "    max_output_len = 20 # Maximum length of the generated sequence\n",
    "\n",
    "    with torch.no_grad(): # No need to track gradients during inference\n",
    "        # Encode the source sequence once\n",
    "        src_padding_mask_inf = transformer_model._create_padding_mask(src_input, PAD_IDX)\n",
    "        src_emb_inf = transformer_model.pos_encoder(transformer_model.src_embedding(src_input).transpose(0, 1) * math.sqrt(D_MODEL))\n",
    "        memory_inf = transformer_model.encoder(src_emb_inf, src_key_padding_mask=src_padding_mask_inf)\n",
    "        # memory_inf shape: (src_seq_len, batch_size, d_model)\n",
    "\n",
    "        for _ in range(max_output_len):\n",
    "            tgt_seq_len_inf = tgt_sequence.shape[1]\n",
    "            tgt_mask_inf = transformer_model._generate_square_subsequent_mask(tgt_seq_len_inf).to(device)\n",
    "            tgt_padding_mask_inf = transformer_model._create_padding_mask(tgt_sequence, PAD_IDX)\n",
    "\n",
    "            # Embed the current target sequence\n",
    "            tgt_emb_inf = transformer_model.pos_encoder(transformer_model.tgt_embedding(tgt_sequence).transpose(0, 1) * math.sqrt(D_MODEL))\n",
    "\n",
    "            # Decode using the current target sequence and encoder memory\n",
    "            decoder_output_inf = transformer_model.decoder(\n",
    "                tgt_emb_inf, memory_inf,\n",
    "                tgt_mask=tgt_mask_inf,\n",
    "                tgt_key_padding_mask=tgt_padding_mask_inf,\n",
    "                memory_key_padding_mask=src_padding_mask_inf\n",
    "            ) # Shape: (tgt_seq_len, batch_size, d_model)\n",
    "\n",
    "            # Get the output for the *last* token only\n",
    "            last_token_output = transformer_model.fc_out(decoder_output_inf[-1, :, :]) # Shape: (batch_size, tgt_vocab_size)\n",
    "\n",
    "            # Find the token with the highest probability (greedy decoding)\n",
    "            next_token = last_token_output.argmax(dim=-1) # Shape: (batch_size)\n",
    "\n",
    "            # Append the predicted token to the target sequence\n",
    "            tgt_sequence = torch.cat([tgt_sequence, next_token.unsqueeze(1)], dim=1) # Shape: (batch_size, current_len + 1)\n",
    "\n",
    "            # Optional: Stop if all sequences in the batch generated an <EOS> token (e.g., index 2)\n",
    "            # if (next_token == 2).all():\n",
    "            #     break\n",
    "\n",
    "    print(f\"Generated Target Sequence Shape: {tgt_sequence.shape}\") # (BATCH_SIZE, generated_len)\n",
    "    print(f\"Example Generated Sequence (first in batch):\\n{tgt_sequence[0]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ecc585b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Source Input Shape: torch.Size([16, 20])\n",
      "Target Input Shape: torch.Size([16, 18])\n",
      "Output Shape (Training): torch.Size([16, 18, 1200])\n",
      "\n",
      "--- Basic Inference Example (Greedy) ---\n",
      "Generated Target Sequence Shape: torch.Size([16, 16])\n",
      "Example Generated Sequence (first in batch):\n",
      "tensor([   1,  965,  415, 1104,  491,  848,  106,  436,   89, 1122,  415,  950,\n",
      "         698, 1161,  890, 1046])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torch import Tensor\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "# Note: Positional Encoding is omitted for simplicity in this primitive version.\n",
    "# This means the model has no inherent understanding of word order.\n",
    "\n",
    "class SimpleAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    A very basic single-head scaled dot-product attention mechanism.\n",
    "    Simplified version without multi-head complexity.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model (int): Dimension of the model.\n",
    "            dropout (float): Dropout probability.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        # Use d_model directly as d_k since it's single-head\n",
    "        self.d_k = d_model\n",
    "\n",
    "        # Linear layers for Query, Key, Value\n",
    "        # Note: In a truly minimal version, these could even be omitted,\n",
    "        # using the input directly, but we keep them for closer analogy.\n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "        # No final output projection Wo needed as we don't concatenate heads\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = math.sqrt(self.d_k)\n",
    "\n",
    "    def forward(self, query: Tensor, key: Tensor, value: Tensor, mask: Optional[Tensor] = None) -> Tuple[Tensor, Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass for Simple Attention.\n",
    "\n",
    "        Args:\n",
    "            query (Tensor): Query tensor, shape (seq_len_q, batch_size, d_model).\n",
    "            key (Tensor): Key tensor, shape (seq_len_k, batch_size, d_model).\n",
    "            value (Tensor): Value tensor, shape (seq_len_v, batch_size, d_model) (seq_len_k == seq_len_v).\n",
    "            mask (Optional[Tensor]): Mask, shape depends on use case.\n",
    "                                     Boolean tensor: True indicates position should be masked.\n",
    "                                     Expected shapes: (tgt_seq_len, src_seq_len) or (batch_size, src_seq_len).\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Tensor, Tensor]: The attention output and the attention weights.\n",
    "                                    Output shape: (seq_len_q, batch_size, d_model)\n",
    "                                    Attention weights shape: (batch_size, seq_len_q, seq_len_k)\n",
    "        \"\"\"\n",
    "        seq_len_q, batch_size, _ = query.size()\n",
    "        seq_len_k, _, _ = key.size()\n",
    "\n",
    "        # 1. Linear projections\n",
    "        Q = self.W_q(query) # Shape: (seq_len_q, batch_size, d_model)\n",
    "        K = self.W_k(key)   # Shape: (seq_len_k, batch_size, d_model)\n",
    "        V = self.W_v(value) # Shape: (seq_len_v, batch_size, d_model)\n",
    "\n",
    "        # Transpose batch and sequence length dimensions for attention calculation\n",
    "        # (seq_len, batch_size, d_model) -> (batch_size, seq_len, d_model)\n",
    "        Q = Q.transpose(0, 1)\n",
    "        K = K.transpose(0, 1)\n",
    "        V = V.transpose(0, 1)\n",
    "\n",
    "        # 2. Scaled dot-product attention\n",
    "        # (batch_size, seq_len_q, d_model) @ (batch_size, d_model, seq_len_k) -> (batch_size, seq_len_q, seq_len_k)\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
    "\n",
    "        # Apply mask if provided\n",
    "        if mask is not None:\n",
    "            # Ensure mask has compatible dimensions for broadcasting to attn_scores (batch_size, seq_len_q, seq_len_k)\n",
    "            if mask.dim() == 2:\n",
    "                # Case 1: Look-ahead mask (tgt_len, tgt_len) or (seq_len_q, seq_len_k)\n",
    "                if mask.shape[0] == seq_len_q and mask.shape[1] == seq_len_k:\n",
    "                    # Unsqueeze batch dimension\n",
    "                    mask = mask.unsqueeze(0) # Shape: (1, seq_len_q, seq_len_k)\n",
    "                # Case 2: Padding mask (batch_size, src_len) or (batch_size, seq_len_k)\n",
    "                elif mask.shape[0] == batch_size and mask.shape[1] == seq_len_k:\n",
    "                    # Unsqueeze query sequence length dimension\n",
    "                    mask = mask.unsqueeze(1) # Shape: (batch_size, 1, seq_len_k)\n",
    "                else:\n",
    "                     # Handle potential mismatch or unexpected shape\n",
    "                     raise ValueError(f\"Mask shape {mask.shape} incompatible with attention scores shape {attn_scores.shape}\")\n",
    "\n",
    "            elif mask.dim() == 3:\n",
    "                 # Assume mask is already broadcastable, e.g., (batch_size, 1, seq_len_k) or (batch_size, seq_len_q, seq_len_k)\n",
    "                 # No unsqueezing needed if dimensions match or broadcast correctly.\n",
    "                 pass # Keep mask as is\n",
    "            elif mask.dim() == 1: # Less common, maybe (src_len)?\n",
    "                 # Needs careful handling depending on intent. Let's assume it's (seq_len_k)\n",
    "                 # Needs expansion to (1, 1, seq_len_k) for broadcasting\n",
    "                 if mask.shape[0] == seq_len_k:\n",
    "                     mask = mask.unsqueeze(0).unsqueeze(0) # Shape: (1, 1, seq_len_k)\n",
    "                 else:\n",
    "                     raise ValueError(f\"Mask shape {mask.shape} incompatible with attention scores shape {attn_scores.shape}\")\n",
    "\n",
    "            # Check for final broadcast compatibility before applying\n",
    "            try:\n",
    "                # Apply the mask (True values in mask indicate positions to ignore)\n",
    "                # Use a large negative number for masked positions before softmax\n",
    "                attn_scores = attn_scores.masked_fill(mask, -1e9)\n",
    "            except RuntimeError as e:\n",
    "                print(f\"Error applying mask. Attn scores shape: {attn_scores.shape}, Mask shape after potential unsqueeze: {mask.shape}\")\n",
    "                raise e\n",
    "\n",
    "\n",
    "        # Apply softmax to get attention weights\n",
    "        attn_weights = torch.softmax(attn_scores, dim=-1) # Shape: (batch_size, seq_len_q, seq_len_k)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Multiply weights by values\n",
    "        # (batch_size, seq_len_q, seq_len_k) @ (batch_size, seq_len_v, d_model) -> (batch_size, seq_len_q, d_model)\n",
    "        output = torch.matmul(attn_weights, V)\n",
    "\n",
    "        # Transpose back to (seq_len_q, batch_size, d_model)\n",
    "        output = output.transpose(0, 1)\n",
    "\n",
    "        # Return attention weights in (batch_size, seq_len_q, seq_len_k) format\n",
    "        return output, attn_weights\n",
    "\n",
    "# Optional: Simplified FeedForward (could even be removed entirely)\n",
    "class SimpleFeedForward(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self.linear2(self.dropout(self.activation(self.linear1(x))))\n",
    "\n",
    "class SimpleEncoderLayer(nn.Module):\n",
    "    \"\"\" Simplified Encoder Layer with single attention and optional feedforward \"\"\"\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = SimpleAttention(d_model, dropout=dropout)\n",
    "        self.feed_forward = SimpleFeedForward(d_model, d_ff, dropout=dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout) # Single dropout for simplicity\n",
    "\n",
    "    def forward(self, src: Tensor, src_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        # Self Attention + Add & Norm\n",
    "        attn_output, _ = self.self_attn(src, src, src, mask=src_padding_mask)\n",
    "        src = self.norm1(src + self.dropout(attn_output)) # Residual connection\n",
    "\n",
    "        # Feed Forward + Add & Norm\n",
    "        ff_output = self.feed_forward(src)\n",
    "        src = self.norm2(src + self.dropout(ff_output)) # Residual connection\n",
    "        return src\n",
    "\n",
    "class SimpleDecoderLayer(nn.Module):\n",
    "    \"\"\" Simplified Decoder Layer \"\"\"\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = SimpleAttention(d_model, dropout=dropout)\n",
    "        self.cross_attn = SimpleAttention(d_model, dropout=dropout) # Encoder-Decoder Attention\n",
    "        self.feed_forward = SimpleFeedForward(d_model, d_ff, dropout=dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout) # Single dropout for simplicity\n",
    "\n",
    "    def forward(self, tgt: Tensor, memory: Tensor,\n",
    "                tgt_mask: Optional[Tensor] = None, # Look-ahead mask\n",
    "                memory_padding_mask: Optional[Tensor] = None, # Src padding mask\n",
    "                tgt_padding_mask: Optional[Tensor] = None) -> Tensor: # Tgt padding mask\n",
    "\n",
    "        # Masked Self-Attention (on target) + Add & Norm\n",
    "        # Combine look-ahead and target padding mask if needed (simplified here)\n",
    "        # Note: SimpleAttention expects boolean mask where True means MASK\n",
    "        # Need to combine tgt_mask (float -inf/0) and tgt_padding_mask (bool True/False)\n",
    "        # For simplicity, we only pass the look-ahead mask here. Proper handling is more complex.\n",
    "        self_attn_output, _ = self.self_attn(tgt, tgt, tgt, mask=tgt_mask)\n",
    "        tgt = self.norm1(tgt + self.dropout(self_attn_output))\n",
    "\n",
    "        # Encoder-Decoder Attention + Add & Norm\n",
    "        # Query=tgt, Key=memory, Value=memory\n",
    "        cross_attn_output, _ = self.cross_attn(tgt, memory, memory, mask=memory_padding_mask)\n",
    "        tgt = self.norm2(tgt + self.dropout(cross_attn_output))\n",
    "\n",
    "        # Feed Forward + Add & Norm\n",
    "        ff_output = self.feed_forward(tgt)\n",
    "        tgt = self.norm3(tgt + self.dropout(ff_output))\n",
    "        return tgt\n",
    "\n",
    "class SimpleTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    A very simplified Transformer model.\n",
    "    - No positional encoding\n",
    "    - Single-layer Encoder & Decoder\n",
    "    - Simple single-head attention\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int = 128, d_ff: int = 256, dropout: float = 0.1,\n",
    "                 src_vocab_size: int = 1000, tgt_vocab_size: int = 1200,\n",
    "                 pad_idx: int = 0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model (int): Embedding dimension.\n",
    "            d_ff (int): Feedforward inner dimension.\n",
    "            dropout (float): Dropout rate.\n",
    "            src_vocab_size (int): Size of source vocabulary.\n",
    "            tgt_vocab_size (int): Size of target vocabulary.\n",
    "            pad_idx (int): Index for padding token.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.pad_idx = pad_idx\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # --- Embeddings ---\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size, d_model, padding_idx=pad_idx)\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model, padding_idx=pad_idx)\n",
    "        # --- NO Positional Encoding ---\n",
    "\n",
    "        # --- Single Encoder Layer ---\n",
    "        self.encoder_layer = SimpleEncoderLayer(d_model, d_ff, dropout)\n",
    "        # --- Single Decoder Layer ---\n",
    "        self.decoder_layer = SimpleDecoderLayer(d_model, d_ff, dropout)\n",
    "\n",
    "        # --- Final Output Layer ---\n",
    "        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "        # --- Initialization ---\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        \"\"\"Initiate parameters.\"\"\"\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz: int, device: torch.device) -> Tensor:\n",
    "        \"\"\"\n",
    "        Generates a boolean mask where True means position should be masked.\n",
    "        Shape: (sz, sz)\n",
    "        \"\"\"\n",
    "        mask = torch.triu(torch.ones(sz, sz, device=device), diagonal=1)\n",
    "        return mask.bool() # True for upper triangle (masked positions)\n",
    "\n",
    "    def _create_padding_mask(self, sequence: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Creates a boolean mask for padding tokens (True where padded).\n",
    "        Shape: (batch_size, seq_len)\n",
    "        \"\"\"\n",
    "        return (sequence == self.pad_idx)\n",
    "\n",
    "    def encode(self, src: Tensor) -> Tensor:\n",
    "        \"\"\" Encodes the source sequence. \"\"\"\n",
    "        src_padding_mask = self._create_padding_mask(src) # (batch_size, src_seq_len)\n",
    "        # Embeddings: (batch_size, src_seq_len) -> (batch_size, src_seq_len, d_model)\n",
    "        # Transpose for Encoder Layer: -> (src_seq_len, batch_size, d_model)\n",
    "        src_emb = self.src_embedding(src).transpose(0, 1) * math.sqrt(self.d_model)\n",
    "        # NO Positional Encoding\n",
    "        # Encoder expects mask as (batch_size, src_seq_len)\n",
    "        memory = self.encoder_layer(src_emb, src_padding_mask=src_padding_mask)\n",
    "        return memory, src_padding_mask\n",
    "\n",
    "    def decode(self, tgt: Tensor, memory: Tensor, memory_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "         \"\"\" Decodes the target sequence given memory. \"\"\"\n",
    "         tgt_seq_len = tgt.shape[1]\n",
    "         # Create masks\n",
    "         # Look-ahead mask (True means mask): (tgt_seq_len, tgt_seq_len)\n",
    "         tgt_mask = self._generate_square_subsequent_mask(tgt_seq_len, tgt.device)\n",
    "         # Target padding mask (True means mask): (batch_size, tgt_seq_len)\n",
    "         tgt_padding_mask = self._create_padding_mask(tgt)\n",
    "\n",
    "         # Embeddings: (batch_size, tgt_seq_len) -> (batch_size, tgt_seq_len, d_model)\n",
    "         # Transpose for Decoder Layer: -> (tgt_seq_len, batch_size, d_model)\n",
    "         tgt_emb = self.tgt_embedding(tgt).transpose(0, 1) * math.sqrt(self.d_model)\n",
    "         # NO Positional Encoding\n",
    "\n",
    "         # Decoder Layer\n",
    "         # Pass necessary masks: look-ahead, source padding, target padding\n",
    "         decoder_output = self.decoder_layer(\n",
    "             tgt_emb, memory,\n",
    "             tgt_mask=tgt_mask,\n",
    "             memory_padding_mask=memory_padding_mask,\n",
    "             tgt_padding_mask=tgt_padding_mask # Pass target padding mask if needed by attention\n",
    "         )\n",
    "         return decoder_output # Shape: (tgt_seq_len, batch_size, d_model)\n",
    "\n",
    "    def forward(self, src: Tensor, tgt: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Full forward pass for training.\n",
    "\n",
    "        Args:\n",
    "            src (Tensor): Source sequence tensor, shape (batch_size, src_seq_len).\n",
    "            tgt (Tensor): Target sequence tensor (shifted right), shape (batch_size, tgt_seq_len).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output logits, shape (batch_size, tgt_seq_len, tgt_vocab_size).\n",
    "        \"\"\"\n",
    "        # Encode source\n",
    "        memory, src_padding_mask = self.encode(src) # memory: (src_seq_len, batch_size, d_model)\n",
    "\n",
    "        # Decode target\n",
    "        decoder_output = self.decode(tgt, memory, memory_padding_mask=src_padding_mask)\n",
    "        # decoder_output: (tgt_seq_len, batch_size, d_model)\n",
    "\n",
    "        # Final linear layer\n",
    "        output_logits = self.fc_out(decoder_output) # (tgt_seq_len, batch_size, tgt_vocab_size)\n",
    "\n",
    "        # Transpose back to (batch_size, tgt_seq_len, tgt_vocab_size)\n",
    "        return output_logits.transpose(0, 1)\n",
    "\n",
    "\n",
    "# --- Example Usage ---\n",
    "if __name__ == '__main__':\n",
    "    # Hyperparameters (smaller values for simplicity)\n",
    "    SRC_VOCAB_SIZE = 1000\n",
    "    TGT_VOCAB_SIZE = 1200\n",
    "    D_MODEL = 128       # Embedding dimension\n",
    "    D_FF = 256          # Feed-forward dimension\n",
    "    DROPOUT = 0.1\n",
    "    PAD_IDX = 0         # Padding index\n",
    "\n",
    "    BATCH_SIZE = 16\n",
    "    SRC_SEQ_LEN = 20\n",
    "    TGT_SEQ_LEN = 18\n",
    "\n",
    "    # Device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Create the simplified model\n",
    "    simple_transformer = SimpleTransformer(\n",
    "        d_model=D_MODEL,\n",
    "        d_ff=D_FF,\n",
    "        dropout=DROPOUT,\n",
    "        src_vocab_size=SRC_VOCAB_SIZE,\n",
    "        tgt_vocab_size=TGT_VOCAB_SIZE,\n",
    "        pad_idx=PAD_IDX\n",
    "    ).to(device)\n",
    "\n",
    "    # Generate dummy input data\n",
    "    src_input = torch.randint(1, SRC_VOCAB_SIZE, (BATCH_SIZE, SRC_SEQ_LEN), device=device)\n",
    "    tgt_input = torch.randint(1, TGT_VOCAB_SIZE, (BATCH_SIZE, TGT_SEQ_LEN), device=device)\n",
    "\n",
    "    # Add padding\n",
    "    src_input[0, -5:] = PAD_IDX\n",
    "    tgt_input[1, -3:] = PAD_IDX\n",
    "\n",
    "    print(f\"Source Input Shape: {src_input.shape}\")\n",
    "    print(f\"Target Input Shape: {tgt_input.shape}\")\n",
    "\n",
    "    # --- Forward Pass (Training) ---\n",
    "    output = simple_transformer(src_input, tgt_input)\n",
    "    print(f\"Output Shape (Training): {output.shape}\") # Should be (BATCH_SIZE, TGT_SEQ_LEN, TGT_VOCAB_SIZE)\n",
    "\n",
    "    # --- Basic Inference Example (Greedy) ---\n",
    "    print(\"\\n--- Basic Inference Example (Greedy) ---\")\n",
    "    simple_transformer.eval()\n",
    "\n",
    "    # Assume SOS token index is 1, EOS is 2\n",
    "    SOS_IDX = 1\n",
    "    EOS_IDX = 2\n",
    "    max_output_len = 15\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Encode source sequence once\n",
    "        memory_inf, src_padding_mask_inf = simple_transformer.encode(src_input)\n",
    "\n",
    "        # Start with SOS token for each sequence in the batch\n",
    "        tgt_sequence = torch.full((BATCH_SIZE, 1), SOS_IDX, dtype=torch.long, device=device) # (BATCH_SIZE, 1)\n",
    "\n",
    "        for _ in range(max_output_len):\n",
    "            # Decode the current target sequence\n",
    "            decoder_output_inf = simple_transformer.decode(\n",
    "                tgt_sequence, memory_inf, memory_padding_mask=src_padding_mask_inf\n",
    "            ) # Shape: (current_tgt_len, batch_size, d_model)\n",
    "\n",
    "            # Get logits for the *last* predicted token\n",
    "            last_token_logits = simple_transformer.fc_out(decoder_output_inf[-1, :, :]) # Shape: (batch_size, tgt_vocab_size)\n",
    "\n",
    "            # Greedy choice: pick token with highest logit\n",
    "            next_token = last_token_logits.argmax(dim=-1) # Shape: (batch_size)\n",
    "\n",
    "            # Append predicted token to the sequence\n",
    "            tgt_sequence = torch.cat([tgt_sequence, next_token.unsqueeze(1)], dim=1)\n",
    "\n",
    "            # Simple stopping condition: Check if EOS was generated for all sequences\n",
    "            # (A more robust implementation would handle finished sequences individually)\n",
    "            if (next_token == EOS_IDX).all():\n",
    "                 break\n",
    "\n",
    "    print(f\"Generated Target Sequence Shape: {tgt_sequence.shape}\")\n",
    "    print(f\"Example Generated Sequence (first in batch):\\n{tgt_sequence[0]}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
