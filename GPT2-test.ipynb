{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df20f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from transformers import (\n",
    "    GPT2Tokenizer,\n",
    "    GPT2LMHeadModel,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b579898f",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = './sumerian_gpt2_finetuned'    # Directory to save the fine-tuned model\n",
    "\n",
    "train_data = pd.read_csv('datasets/SumTablets_English_train.csv')\n",
    "test_data = pd.read_csv('datasets/SumTablets_English_train.csv')\n",
    "\n",
    "# Format the data for GPT-2:\n",
    "# We'll combine Sumerian and English with a separator.\n",
    "# GPT-2 will learn to generate the English part after seeing \"English: \".\n",
    "# The <|endoftext|> token is GPT-2's standard end-of-sequence token.\n",
    "formatted_texts = []\n",
    "for index, row in train_data.iterrows():\n",
    "    sumerian_texts = row['transliteration']\n",
    "    english_translations = row['translation']\n",
    "    if isinstance(sumerian_texts, str) and isinstance(english_translations, str):\n",
    "        sumerian_texts = sumerian_texts.replace('\\n', ' ')\n",
    "        english_translations = english_translations.replace('\\n', ' ')\n",
    "        formatted_texts.append(f\"Sumerian: {sumerian_texts}\\nEnglish: {english_translations}<|endoftext|>\")\n",
    "print(f\"Loaded {len(formatted_texts)} formatted examples.\")\n",
    "\n",
    "lengths = [len(text.split()) for text in formatted_texts]\n",
    "print(lengths)\n",
    "mean_length = np.mean(lengths)\n",
    "print(f\"Mean length of the texts: {mean_length} words\")\n",
    "print(f\"Percentage of texts longer than 528 words: {sum(length > 528 for length in lengths) / len(lengths) * 100:.2f}%\")\n",
    "\n",
    "# remove texts longer than 528 words\n",
    "formatted_texts = [text for text in formatted_texts if len(text.split()) <= 528]\n",
    "print(len(formatted_texts), \"texts after filtering by length.\")\n",
    "\n",
    "print(f\"\\nExample formatted text:\\n{formatted_texts[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9c203b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# load model from a checkpoint\n",
    "model = GPT2LMHeadModel.from_pretrained(f\"{OUTPUT_DIR}\").to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(f\"{OUTPUT_DIR}\")\n",
    "\n",
    "for index, row in test_data.iloc[:30,:].iterrows():\n",
    "    sumerian_texts = row['transliteration'].replace('\\n', ' ')\n",
    "    english_translations = row['translation'].replace('\\n', ' ')\n",
    "    prompt_text = f\"Sumerian: {sumerian_texts} \\nEnglish:\"\n",
    "\n",
    "    # --- Tokenizzazione dell'input ---\n",
    "    input_ids = tokenizer.encode(prompt_text, return_tensors='pt').to(device)\n",
    "\n",
    "    # --- Generazione del testo ---\n",
    "    print(\"Generazione del testo...\")\n",
    "    try:\n",
    "        output_sequences = model.generate(\n",
    "            input_ids=input_ids,    \n",
    "            max_length=200,             # Max length of prompt + generated text\n",
    "            temperature=0.2,            # Controls randomness. Lower is more deterministic.\n",
    "            top_k=40,                   # Considers the top K most probable tokens at each step.\n",
    "            top_p=0.9,                  # Nucleus sampling: considers tokens with cumulative probability >= P.\n",
    "            repetition_penalty=1,       # Penalizes repetition.\n",
    "            num_return_sequences=1,     # Number of different sequences to generate.\n",
    "            pad_token_id=tokenizer.eos_token_id, # Pad token ID for generation\n",
    "            no_repeat_ngram_size=3,     # Prevent 3-gram repetition\n",
    "            early_stopping=True,        # Stop when EOS is generated\n",
    "            length_penalty=1.0,         # Neutral - neither favor short nor long outputs\n",
    "            num_beams=3                 # Use beam search instead of sampling\n",
    "        )\n",
    "\n",
    "        print(f\"Testo di input: {prompt_text}\")\n",
    "        print(f\"Traduzione effettiva: {english_translations}\")\n",
    "        \n",
    "        # --- Decodifica e Stampa ---\n",
    "        for i, generated_sequence in enumerate(output_sequences):\n",
    "            text = tokenizer.decode(generated_sequence, skip_special_tokens=True)\n",
    "            print('Testo Generato:', text)\n",
    "            print('---')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Errore durante la generazione del testo: {e}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
