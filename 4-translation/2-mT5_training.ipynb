{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "195b3dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53844f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-31 10:30:44.554435: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748687445.478548    3455 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748687445.738190    3455 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1748687448.150198    3455 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748687448.150235    3455 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748687448.150237    3455 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748687448.150239    3455 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-31 10:30:48.417684: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from transformers import (\n",
    "    MT5ForConditionalGeneration,\n",
    "    MT5Tokenizer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from evaluate import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25fe5f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: google/mt5-small\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device being used: cuda\n",
      "Model loaded with 300.18M parameters\n",
      "Tokenizer vocabulary size: 250100\n"
     ]
    }
   ],
   "source": [
    "\n",
    "MODEL_NAME = \"google/mt5-small\"\n",
    "OUTPUT_DIR = \"./sumerian_mt5_model\"\n",
    "LOG_DIR = \"./sumerian_mt5_logs\"\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "# Fix the tokenizer initialization by using AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Device being used: {device}\")\n",
    "print(f\"Model loaded with {sum(p.numel() for p in model.parameters())/1e6:.2f}M parameters\")\n",
    "print(f\"Tokenizer vocabulary size: {len(tokenizer)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856b3276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Loaded 1907 training examples and 113 test examples\n",
      "22\n",
      "24\n",
      "19\n",
      "17\n",
      "22\n",
      "23\n",
      "59\n",
      "10\n",
      "22\n",
      "27\n",
      "18\n",
      "18\n",
      "48\n",
      "34\n",
      "31\n",
      "89\n",
      "38\n",
      "22\n",
      "23\n",
      "18\n",
      "21\n",
      "24\n",
      "16\n",
      "18\n",
      "40\n",
      "33\n",
      "23\n",
      "10\n",
      "11\n",
      "19\n",
      "34\n",
      "104\n",
      "52\n",
      "14\n",
      "24\n",
      "5\n",
      "19\n",
      "26\n",
      "29\n",
      "22\n",
      "67\n",
      "21\n",
      "17\n",
      "19\n",
      "16\n",
      "15\n",
      "56\n",
      "72\n",
      "31\n",
      "25\n",
      "18\n",
      "33\n",
      "15\n",
      "12\n",
      "9\n",
      "18\n",
      "8\n",
      "19\n",
      "50\n",
      "92\n",
      "37\n",
      "37\n",
      "30\n",
      "24\n",
      "115\n",
      "27\n",
      "31\n",
      "25\n",
      "25\n",
      "12\n",
      "20\n",
      "22\n",
      "33\n",
      "20\n",
      "21\n",
      "32\n",
      "20\n",
      "25\n",
      "30\n",
      "66\n",
      "20\n",
      "35\n",
      "253\n",
      "171\n",
      "129\n",
      "106\n",
      "104\n",
      "26\n",
      "105\n",
      "62\n",
      "170\n",
      "56\n",
      "19\n",
      "89\n",
      "96\n",
      "431\n",
      "3\n",
      "3\n",
      "4\n",
      "3\n",
      "3\n",
      "1\n",
      "6\n",
      "4\n",
      "5\n",
      "3\n",
      "4\n",
      "3\n",
      "3\n",
      "2\n",
      "4\n",
      "4\n",
      "5\n",
      "3\n",
      "9\n",
      "4\n",
      "4\n",
      "4\n",
      "14\n",
      "3\n",
      "6\n",
      "4\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "3\n",
      "2\n",
      "4\n",
      "4\n",
      "7\n",
      "9\n",
      "4\n",
      "4\n",
      "10\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "3\n",
      "9\n",
      "3\n",
      "10\n",
      "2\n",
      "4\n",
      "6\n",
      "14\n",
      "15\n",
      "14\n",
      "14\n",
      "13\n",
      "13\n",
      "16\n",
      "16\n",
      "16\n",
      "13\n",
      "14\n",
      "13\n",
      "16\n",
      "1\n",
      "12\n",
      "10\n",
      "6\n",
      "9\n",
      "10\n",
      "14\n",
      "17\n",
      "19\n",
      "17\n",
      "16\n",
      "11\n",
      "13\n",
      "13\n",
      "15\n",
      "16\n",
      "14\n",
      "15\n",
      "13\n",
      "16\n",
      "17\n",
      "17\n",
      "15\n",
      "15\n",
      "13\n",
      "13\n",
      "16\n",
      "15\n",
      "15\n",
      "13\n",
      "16\n",
      "15\n",
      "16\n",
      "15\n",
      "17\n",
      "14\n",
      "14\n",
      "12\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "16\n",
      "17\n",
      "15\n",
      "14\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "17\n",
      "16\n",
      "15\n",
      "13\n",
      "14\n",
      "13\n",
      "15\n",
      "13\n",
      "15\n",
      "13\n",
      "14\n",
      "14\n",
      "15\n",
      "14\n",
      "15\n",
      "16\n",
      "15\n",
      "15\n",
      "18\n",
      "14\n",
      "16\n",
      "15\n",
      "15\n",
      "15\n",
      "16\n",
      "15\n",
      "15\n",
      "16\n",
      "15\n",
      "17\n",
      "13\n",
      "15\n",
      "15\n",
      "14\n",
      "14\n",
      "14\n",
      "13\n",
      "12\n",
      "15\n",
      "16\n",
      "18\n",
      "13\n",
      "14\n",
      "15\n",
      "14\n",
      "15\n",
      "16\n",
      "14\n",
      "13\n",
      "33\n",
      "16\n",
      "12\n",
      "17\n",
      "13\n",
      "5\n",
      "11\n",
      "5\n",
      "3\n",
      "15\n",
      "14\n",
      "3\n",
      "3\n",
      "4\n",
      "4\n",
      "15\n",
      "14\n",
      "13\n",
      "14\n",
      "15\n",
      "14\n",
      "6\n",
      "30\n",
      "20\n",
      "20\n",
      "5\n",
      "21\n",
      "63\n",
      "22\n",
      "24\n",
      "125\n",
      "32\n",
      "14\n",
      "60\n",
      "41\n",
      "42\n",
      "36\n",
      "13\n",
      "9\n",
      "6\n",
      "11\n",
      "21\n",
      "32\n",
      "36\n",
      "36\n",
      "25\n",
      "21\n",
      "38\n",
      "159\n",
      "10\n",
      "16\n",
      "15\n",
      "35\n",
      "33\n",
      "17\n",
      "40\n",
      "19\n",
      "20\n",
      "20\n",
      "32\n",
      "18\n",
      "81\n",
      "16\n",
      "34\n",
      "45\n",
      "8\n",
      "69\n",
      "20\n",
      "26\n",
      "30\n",
      "37\n",
      "51\n",
      "31\n",
      "51\n",
      "48\n",
      "35\n",
      "42\n",
      "73\n",
      "35\n",
      "71\n",
      "27\n",
      "23\n",
      "72\n",
      "84\n",
      "41\n",
      "26\n",
      "106\n",
      "22\n",
      "36\n",
      "21\n",
      "25\n",
      "18\n",
      "38\n",
      "22\n",
      "20\n",
      "34\n",
      "18\n",
      "17\n",
      "24\n",
      "34\n",
      "24\n",
      "28\n",
      "14\n",
      "12\n",
      "22\n",
      "25\n",
      "37\n",
      "19\n",
      "19\n",
      "223\n",
      "15\n",
      "23\n",
      "14\n",
      "48\n",
      "158\n",
      "11\n",
      "64\n",
      "47\n",
      "53\n",
      "33\n",
      "26\n",
      "15\n",
      "13\n",
      "23\n",
      "12\n",
      "128\n",
      "18\n",
      "24\n",
      "7\n",
      "23\n",
      "23\n",
      "29\n",
      "12\n",
      "24\n",
      "185\n",
      "59\n",
      "29\n",
      "48\n",
      "97\n",
      "75\n",
      "76\n",
      "212\n",
      "180\n",
      "27\n",
      "11\n",
      "22\n",
      "35\n",
      "38\n",
      "11\n",
      "36\n",
      "17\n",
      "9\n",
      "7\n",
      "9\n",
      "23\n",
      "16\n",
      "26\n",
      "13\n",
      "11\n",
      "21\n",
      "23\n",
      "25\n",
      "33\n",
      "24\n",
      "27\n",
      "70\n",
      "25\n",
      "14\n",
      "17\n",
      "82\n",
      "26\n",
      "22\n",
      "17\n",
      "22\n",
      "31\n",
      "34\n",
      "31\n",
      "58\n",
      "15\n",
      "121\n",
      "4\n",
      "11\n",
      "15\n",
      "58\n",
      "12\n",
      "17\n",
      "13\n",
      "19\n",
      "5\n",
      "21\n",
      "46\n",
      "10\n",
      "78\n",
      "56\n",
      "10\n",
      "26\n",
      "22\n",
      "16\n",
      "33\n",
      "19\n",
      "18\n",
      "31\n",
      "15\n",
      "11\n",
      "52\n",
      "14\n",
      "9\n",
      "11\n",
      "12\n",
      "32\n",
      "105\n",
      "9\n",
      "23\n",
      "29\n",
      "29\n",
      "28\n",
      "30\n",
      "13\n",
      "5\n",
      "36\n",
      "30\n",
      "35\n",
      "20\n",
      "6\n",
      "7\n",
      "12\n",
      "23\n",
      "12\n",
      "10\n",
      "46\n",
      "18\n",
      "16\n",
      "64\n",
      "37\n",
      "28\n",
      "137\n",
      "198\n",
      "228\n",
      "319\n",
      "383\n",
      "21\n",
      "33\n",
      "18\n",
      "50\n",
      "94\n",
      "49\n",
      "46\n",
      "34\n",
      "19\n",
      "23\n",
      "9\n",
      "15\n",
      "47\n",
      "6\n",
      "26\n",
      "19\n",
      "12\n",
      "9\n",
      "13\n",
      "6\n",
      "15\n",
      "8\n",
      "10\n",
      "13\n",
      "12\n",
      "10\n",
      "15\n",
      "21\n",
      "9\n",
      "8\n",
      "7\n",
      "7\n",
      "8\n",
      "10\n",
      "12\n",
      "10\n",
      "6\n",
      "2\n",
      "10\n",
      "9\n",
      "10\n",
      "7\n",
      "13\n",
      "10\n",
      "10\n",
      "11\n",
      "10\n",
      "11\n",
      "8\n",
      "19\n",
      "16\n",
      "13\n",
      "8\n",
      "7\n",
      "20\n",
      "17\n",
      "28\n",
      "11\n",
      "17\n",
      "14\n",
      "10\n",
      "16\n",
      "18\n",
      "14\n",
      "17\n",
      "18\n",
      "17\n",
      "18\n",
      "11\n",
      "8\n",
      "8\n",
      "14\n",
      "26\n",
      "13\n",
      "17\n",
      "13\n",
      "13\n",
      "13\n",
      "21\n",
      "45\n",
      "7\n",
      "7\n",
      "6\n",
      "12\n",
      "10\n",
      "16\n",
      "53\n",
      "7\n",
      "20\n",
      "23\n",
      "16\n",
      "28\n",
      "31\n",
      "52\n",
      "59\n",
      "111\n",
      "19\n",
      "19\n",
      "19\n",
      "25\n",
      "11\n",
      "37\n",
      "27\n",
      "35\n",
      "32\n",
      "17\n",
      "15\n",
      "93\n",
      "20\n",
      "29\n",
      "432\n",
      "55\n",
      "80\n",
      "46\n",
      "9\n",
      "10\n",
      "18\n",
      "107\n",
      "91\n",
      "5\n",
      "28\n",
      "113\n",
      "291\n",
      "98\n",
      "14\n",
      "13\n",
      "29\n",
      "90\n",
      "42\n",
      "238\n",
      "222\n",
      "26\n",
      "47\n",
      "380\n",
      "376\n",
      "94\n",
      "346\n",
      "359\n",
      "335\n",
      "282\n",
      "277\n",
      "167\n",
      "49\n",
      "45\n",
      "17\n",
      "162\n",
      "37\n",
      "39\n",
      "92\n",
      "330\n",
      "16\n",
      "106\n",
      "107\n",
      "34\n",
      "234\n",
      "115\n",
      "17\n",
      "45\n",
      "110\n",
      "71\n",
      "117\n",
      "96\n",
      "175\n",
      "99\n",
      "90\n",
      "31\n",
      "29\n",
      "60\n",
      "47\n",
      "310\n",
      "385\n",
      "118\n",
      "100\n",
      "33\n",
      "202\n",
      "324\n",
      "45\n",
      "7\n",
      "77\n",
      "79\n",
      "97\n",
      "15\n",
      "16\n",
      "42\n",
      "12\n",
      "14\n",
      "92\n",
      "119\n",
      "20\n",
      "136\n",
      "18\n",
      "32\n",
      "17\n",
      "25\n",
      "14\n",
      "436\n",
      "73\n",
      "79\n",
      "5\n",
      "8\n",
      "8\n",
      "7\n",
      "8\n",
      "42\n",
      "4\n",
      "6\n",
      "12\n",
      "8\n",
      "9\n",
      "34\n",
      "4\n",
      "5\n",
      "6\n",
      "5\n",
      "3\n",
      "4\n",
      "5\n",
      "8\n",
      "7\n",
      "7\n",
      "6\n",
      "8\n",
      "16\n",
      "8\n",
      "4\n",
      "154\n",
      "60\n",
      "11\n",
      "4\n",
      "5\n",
      "13\n",
      "19\n",
      "13\n",
      "13\n",
      "13\n",
      "4\n",
      "14\n",
      "15\n",
      "12\n",
      "13\n",
      "17\n",
      "12\n",
      "15\n",
      "15\n",
      "7\n",
      "15\n",
      "16\n",
      "12\n",
      "15\n",
      "15\n",
      "7\n",
      "15\n",
      "13\n",
      "15\n",
      "10\n",
      "9\n",
      "15\n",
      "15\n",
      "15\n",
      "6\n",
      "14\n",
      "15\n",
      "15\n",
      "15\n",
      "13\n",
      "14\n",
      "14\n",
      "15\n",
      "15\n",
      "12\n",
      "14\n",
      "15\n",
      "15\n",
      "14\n",
      "11\n",
      "15\n",
      "9\n",
      "14\n",
      "13\n",
      "13\n",
      "13\n",
      "15\n",
      "15\n",
      "4\n",
      "15\n",
      "15\n",
      "16\n",
      "15\n",
      "10\n",
      "16\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "14\n",
      "15\n",
      "14\n",
      "14\n",
      "15\n",
      "13\n",
      "35\n",
      "16\n",
      "16\n",
      "15\n",
      "16\n",
      "15\n",
      "15\n",
      "14\n",
      "15\n",
      "17\n",
      "15\n",
      "10\n",
      "55\n",
      "45\n",
      "42\n",
      "13\n",
      "26\n",
      "56\n",
      "38\n",
      "19\n",
      "23\n",
      "74\n",
      "100\n",
      "39\n",
      "12\n",
      "16\n",
      "26\n",
      "61\n",
      "15\n",
      "35\n",
      "13\n",
      "42\n",
      "20\n",
      "39\n",
      "17\n",
      "25\n",
      "1966\n",
      "1005\n",
      "415\n",
      "33\n",
      "19\n",
      "56\n",
      "73\n",
      "50\n",
      "32\n",
      "39\n",
      "34\n",
      "54\n",
      "106\n",
      "6\n",
      "16\n",
      "17\n",
      "95\n",
      "42\n",
      "135\n",
      "14\n",
      "19\n",
      "31\n",
      "238\n",
      "731\n",
      "19\n",
      "23\n",
      "34\n",
      "23\n",
      "17\n",
      "19\n",
      "17\n",
      "18\n",
      "16\n",
      "10\n",
      "7\n",
      "9\n",
      "9\n",
      "15\n",
      "7\n",
      "4\n",
      "7\n",
      "5\n",
      "9\n",
      "11\n",
      "6\n",
      "5\n",
      "11\n",
      "7\n",
      "13\n",
      "7\n",
      "147\n",
      "27\n",
      "13\n",
      "41\n",
      "72\n",
      "18\n",
      "18\n",
      "33\n",
      "33\n",
      "11\n",
      "16\n",
      "11\n",
      "26\n",
      "16\n",
      "25\n",
      "75\n",
      "24\n",
      "29\n",
      "30\n",
      "22\n",
      "13\n",
      "11\n",
      "27\n",
      "8\n",
      "11\n",
      "9\n",
      "9\n",
      "4\n",
      "10\n",
      "13\n",
      "13\n",
      "12\n",
      "19\n",
      "9\n",
      "16\n",
      "12\n",
      "17\n",
      "40\n",
      "8\n",
      "17\n",
      "15\n",
      "28\n",
      "10\n",
      "18\n",
      "22\n",
      "23\n",
      "21\n",
      "14\n",
      "13\n",
      "18\n",
      "18\n",
      "18\n",
      "22\n",
      "12\n",
      "48\n",
      "13\n",
      "8\n",
      "24\n",
      "17\n",
      "119\n",
      "10\n",
      "20\n",
      "3\n",
      "16\n",
      "40\n",
      "183\n",
      "97\n",
      "82\n",
      "99\n",
      "80\n",
      "84\n",
      "116\n",
      "133\n",
      "55\n",
      "98\n",
      "90\n",
      "63\n",
      "46\n",
      "118\n",
      "241\n",
      "152\n",
      "88\n",
      "59\n",
      "132\n",
      "118\n",
      "121\n",
      "28\n",
      "10\n",
      "442\n",
      "20\n",
      "34\n",
      "31\n",
      "74\n",
      "66\n",
      "23\n",
      "21\n",
      "48\n",
      "115\n",
      "179\n",
      "121\n",
      "48\n",
      "198\n",
      "101\n",
      "91\n",
      "18\n",
      "29\n",
      "67\n",
      "360\n",
      "54\n",
      "24\n",
      "63\n",
      "90\n",
      "59\n",
      "84\n",
      "36\n",
      "30\n",
      "140\n",
      "19\n",
      "18\n",
      "88\n",
      "17\n",
      "28\n",
      "15\n",
      "19\n",
      "36\n",
      "16\n",
      "18\n",
      "16\n",
      "19\n",
      "17\n",
      "25\n",
      "14\n",
      "18\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "13\n",
      "19\n",
      "15\n",
      "23\n",
      "14\n",
      "22\n",
      "26\n",
      "22\n",
      "20\n",
      "61\n",
      "49\n",
      "11\n",
      "60\n",
      "41\n",
      "34\n",
      "22\n",
      "21\n",
      "22\n",
      "33\n",
      "27\n",
      "81\n",
      "57\n",
      "106\n",
      "21\n",
      "18\n",
      "145\n",
      "38\n",
      "40\n",
      "20\n",
      "15\n",
      "65\n",
      "88\n",
      "14\n",
      "232\n",
      "127\n",
      "53\n",
      "65\n",
      "46\n",
      "119\n",
      "38\n",
      "40\n",
      "23\n",
      "25\n",
      "27\n",
      "10\n",
      "25\n",
      "29\n",
      "54\n",
      "18\n",
      "11\n",
      "24\n",
      "16\n",
      "47\n",
      "17\n",
      "13\n",
      "15\n",
      "19\n",
      "22\n",
      "13\n",
      "19\n",
      "13\n",
      "17\n",
      "16\n",
      "18\n",
      "11\n",
      "11\n",
      "5\n",
      "10\n",
      "12\n",
      "8\n",
      "7\n",
      "7\n",
      "12\n",
      "15\n",
      "17\n",
      "20\n",
      "6\n",
      "88\n",
      "8\n",
      "12\n",
      "33\n",
      "9\n",
      "9\n",
      "18\n",
      "25\n",
      "8\n",
      "10\n",
      "14\n",
      "16\n",
      "6\n",
      "7\n",
      "10\n",
      "24\n",
      "12\n",
      "22\n",
      "17\n",
      "13\n",
      "12\n",
      "12\n",
      "7\n",
      "11\n",
      "10\n",
      "12\n",
      "9\n",
      "21\n",
      "17\n",
      "42\n",
      "15\n",
      "7\n",
      "24\n",
      "16\n",
      "12\n",
      "15\n",
      "11\n",
      "13\n",
      "8\n",
      "12\n",
      "13\n",
      "18\n",
      "19\n",
      "15\n",
      "15\n",
      "16\n",
      "11\n",
      "32\n",
      "10\n",
      "9\n",
      "26\n",
      "8\n",
      "20\n",
      "11\n",
      "25\n",
      "9\n",
      "30\n",
      "9\n",
      "25\n",
      "7\n",
      "21\n",
      "20\n",
      "11\n",
      "17\n",
      "29\n",
      "14\n",
      "80\n",
      "7\n",
      "16\n",
      "11\n",
      "15\n",
      "7\n",
      "8\n",
      "10\n",
      "6\n",
      "3\n",
      "2\n",
      "1\n",
      "4\n",
      "22\n",
      "12\n",
      "22\n",
      "17\n",
      "7\n",
      "14\n",
      "947\n",
      "12\n",
      "44\n",
      "22\n",
      "20\n",
      "96\n",
      "21\n",
      "43\n",
      "32\n",
      "22\n",
      "77\n",
      "7\n",
      "10\n",
      "20\n",
      "36\n",
      "20\n",
      "22\n",
      "19\n",
      "19\n",
      "10\n",
      "16\n",
      "13\n",
      "21\n",
      "9\n",
      "25\n",
      "38\n",
      "80\n",
      "15\n",
      "115\n",
      "208\n",
      "37\n",
      "16\n",
      "12\n",
      "21\n",
      "4\n",
      "27\n",
      "17\n",
      "9\n",
      "16\n",
      "24\n",
      "25\n",
      "15\n",
      "17\n",
      "389\n",
      "14\n",
      "17\n",
      "19\n",
      "18\n",
      "13\n",
      "3\n",
      "13\n",
      "226\n",
      "16\n",
      "434\n",
      "93\n",
      "30\n",
      "100\n",
      "34\n",
      "20\n",
      "97\n",
      "52\n",
      "25\n",
      "17\n",
      "42\n",
      "13\n",
      "16\n",
      "18\n",
      "17\n",
      "21\n",
      "10\n",
      "10\n",
      "24\n",
      "18\n",
      "9\n",
      "8\n",
      "25\n",
      "13\n",
      "16\n",
      "13\n",
      "14\n",
      "15\n",
      "18\n",
      "11\n",
      "19\n",
      "35\n",
      "25\n",
      "13\n",
      "16\n",
      "23\n",
      "8\n",
      "14\n",
      "21\n",
      "12\n",
      "30\n",
      "11\n",
      "16\n",
      "12\n",
      "14\n",
      "12\n",
      "10\n",
      "20\n",
      "12\n",
      "18\n",
      "34\n",
      "16\n",
      "18\n",
      "17\n",
      "12\n",
      "26\n",
      "11\n",
      "7\n",
      "24\n",
      "11\n",
      "16\n",
      "18\n",
      "24\n",
      "17\n",
      "15\n",
      "30\n",
      "17\n",
      "19\n",
      "27\n",
      "21\n",
      "7\n",
      "47\n",
      "51\n",
      "18\n",
      "25\n",
      "17\n",
      "12\n",
      "37\n",
      "5\n",
      "10\n",
      "39\n",
      "105\n",
      "108\n",
      "19\n",
      "30\n",
      "47\n",
      "13\n",
      "12\n",
      "13\n",
      "11\n",
      "9\n",
      "70\n",
      "16\n",
      "22\n",
      "16\n",
      "11\n",
      "28\n",
      "461\n",
      "20\n",
      "53\n",
      "13\n",
      "18\n",
      "30\n",
      "9\n",
      "23\n",
      "20\n",
      "28\n",
      "15\n",
      "21\n",
      "11\n",
      "39\n",
      "17\n",
      "20\n",
      "13\n",
      "60\n",
      "48\n",
      "28\n",
      "208\n",
      "52\n",
      "63\n",
      "18\n",
      "21\n",
      "16\n",
      "15\n",
      "12\n",
      "7\n",
      "86\n",
      "36\n",
      "16\n",
      "31\n",
      "12\n",
      "33\n",
      "12\n",
      "13\n",
      "30\n",
      "10\n",
      "24\n",
      "20\n",
      "17\n",
      "26\n",
      "10\n",
      "25\n",
      "15\n",
      "22\n",
      "27\n",
      "46\n",
      "18\n",
      "22\n",
      "3\n",
      "16\n",
      "18\n",
      "18\n",
      "27\n",
      "30\n",
      "26\n",
      "28\n",
      "437\n",
      "20\n",
      "36\n",
      "16\n",
      "34\n",
      "10\n",
      "12\n",
      "11\n",
      "12\n",
      "20\n",
      "14\n",
      "21\n",
      "6\n",
      "7\n",
      "24\n",
      "19\n",
      "21\n",
      "66\n",
      "20\n",
      "25\n",
      "19\n",
      "34\n",
      "29\n",
      "10\n",
      "9\n",
      "81\n",
      "295\n",
      "358\n",
      "305\n",
      "8\n",
      "20\n",
      "13\n",
      "13\n",
      "15\n",
      "43\n",
      "19\n",
      "12\n",
      "10\n",
      "15\n",
      "12\n",
      "12\n",
      "17\n",
      "14\n",
      "12\n",
      "17\n",
      "26\n",
      "16\n",
      "18\n",
      "18\n",
      "12\n",
      "21\n",
      "11\n",
      "15\n",
      "20\n",
      "14\n",
      "14\n",
      "11\n",
      "13\n",
      "69\n",
      "51\n",
      "82\n",
      "22\n",
      "25\n",
      "23\n",
      "28\n",
      "17\n",
      "24\n",
      "116\n",
      "24\n",
      "34\n",
      "16\n",
      "26\n",
      "69\n",
      "26\n",
      "20\n",
      "230\n",
      "22\n",
      "70\n",
      "116\n",
      "3138\n",
      "22\n",
      "35\n",
      "35\n",
      "573\n",
      "382\n",
      "13\n",
      "8\n",
      "19\n",
      "60\n",
      "12\n",
      "25\n",
      "23\n",
      "9\n",
      "8\n",
      "12\n",
      "19\n",
      "19\n",
      "10\n",
      "9\n",
      "17\n",
      "20\n",
      "29\n",
      "24\n",
      "71\n",
      "94\n",
      "51\n",
      "122\n",
      "53\n",
      "110\n",
      "61\n",
      "120\n",
      "103\n",
      "118\n",
      "140\n",
      "77\n",
      "72\n",
      "20\n",
      "584\n",
      "13\n",
      "26\n",
      "35\n",
      "44\n",
      "25\n",
      "29\n",
      "8\n",
      "20\n",
      "10\n",
      "7\n",
      "8\n",
      "9\n",
      "16\n",
      "17\n",
      "10\n",
      "19\n",
      "17\n",
      "11\n",
      "17\n",
      "15\n",
      "16\n",
      "9\n",
      "11\n",
      "14\n",
      "17\n",
      "12\n",
      "13\n",
      "19\n",
      "5\n",
      "18\n",
      "19\n",
      "27\n",
      "15\n",
      "21\n",
      "17\n",
      "13\n",
      "6\n",
      "21\n",
      "13\n",
      "11\n",
      "29\n",
      "9\n",
      "12\n",
      "19\n",
      "11\n",
      "8\n",
      "10\n",
      "13\n",
      "18\n",
      "11\n",
      "14\n",
      "6\n",
      "10\n",
      "65\n",
      "245\n",
      "159\n",
      "164\n",
      "7\n",
      "8\n",
      "6\n",
      "6\n",
      "7\n",
      "8\n",
      "8\n",
      "8\n",
      "5\n",
      "3\n",
      "4\n",
      "4\n",
      "9\n",
      "3\n",
      "5\n",
      "2\n",
      "5\n",
      "12\n",
      "8\n",
      "6\n",
      "8\n",
      "15\n",
      "21\n",
      "28\n",
      "22\n",
      "35\n",
      "20\n",
      "20\n",
      "27\n",
      "31\n",
      "42\n",
      "21\n",
      "30\n",
      "34\n",
      "356\n",
      "56\n",
      "16\n",
      "69\n",
      "13\n",
      "23\n",
      "32\n",
      "12\n",
      "21\n",
      "16\n",
      "33\n",
      "21\n",
      "23\n",
      "20\n",
      "25\n",
      "18\n",
      "23\n",
      "46\n",
      "20\n",
      "22\n",
      "20\n",
      "39\n",
      "9\n",
      "22\n",
      "48\n",
      "16\n",
      "38\n",
      "28\n",
      "3\n",
      "70\n",
      "20\n",
      "20\n",
      "16\n",
      "10\n",
      "14\n",
      "39\n",
      "14\n",
      "9\n",
      "16\n",
      "21\n",
      "21\n",
      "26\n",
      "23\n",
      "23\n",
      "14\n",
      "10\n",
      "11\n",
      "8\n",
      "23\n",
      "28\n",
      "20\n",
      "14\n",
      "3\n",
      "137\n",
      "19\n",
      "17\n",
      "33\n",
      "51\n",
      "21\n",
      "98\n",
      "70\n",
      "13\n",
      "12\n",
      "14\n",
      "39\n",
      "342\n",
      "39\n",
      "211\n",
      "259\n",
      "28\n",
      "35\n",
      "15\n",
      "22\n",
      "26\n",
      "11\n",
      "23\n",
      "11\n",
      "9\n",
      "11\n",
      "14\n",
      "16\n",
      "15\n",
      "8\n",
      "12\n",
      "20\n",
      "10\n",
      "20\n",
      "6\n",
      "19\n",
      "10\n",
      "13\n",
      "44\n",
      "15\n",
      "12\n",
      "50\n",
      "12\n",
      "29\n",
      "490\n",
      "29\n",
      "11\n",
      "22\n",
      "56\n",
      "11\n",
      "18\n",
      "8\n",
      "16\n",
      "15\n",
      "11\n",
      "7\n",
      "21\n",
      "6\n",
      "11\n",
      "5\n",
      "426\n",
      "16\n",
      "46\n",
      "6\n",
      "77\n",
      "25\n",
      "15\n",
      "3\n",
      "3\n",
      "19\n",
      "23\n",
      "15\n",
      "31\n",
      "30\n",
      "21\n",
      "12\n",
      "62\n",
      "13\n",
      "14\n",
      "23\n",
      "28\n",
      "30\n",
      "46\n",
      "20\n",
      "37\n",
      "61\n",
      "28\n",
      "25\n",
      "45\n",
      "15\n",
      "9\n",
      "14\n",
      "14\n",
      "5\n",
      "18\n",
      "15\n",
      "704\n",
      "31\n",
      "15\n",
      "17\n",
      "12\n",
      "27\n",
      "11\n",
      "19\n",
      "14\n",
      "20\n",
      "24\n",
      "37\n",
      "23\n",
      "31\n",
      "16\n",
      "19\n",
      "24\n",
      "23\n",
      "24\n",
      "26\n",
      "28\n",
      "54\n",
      "45\n",
      "19\n",
      "36\n",
      "53\n",
      "19\n",
      "17\n",
      "18\n",
      "32\n",
      "17\n",
      "34\n",
      "32\n",
      "19\n",
      "25\n",
      "8\n",
      "8\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "11\n",
      "9\n",
      "13\n",
      "83\n",
      "8\n",
      "14\n",
      "21\n",
      "26\n",
      "17\n",
      "15\n",
      "21\n",
      "12\n",
      "9\n",
      "23\n",
      "24\n",
      "24\n",
      "24\n",
      "13\n",
      "13\n",
      "42\n",
      "28\n",
      "9\n",
      "22\n",
      "7\n",
      "17\n",
      "79\n",
      "20\n",
      "8\n",
      "10\n",
      "15\n",
      "463\n",
      "233\n",
      "29\n",
      "16\n",
      "14\n",
      "8\n",
      "20\n",
      "27\n",
      "16\n",
      "13\n",
      "12\n",
      "12\n",
      "66\n",
      "11\n",
      "9\n",
      "17\n",
      "10\n",
      "47\n",
      "46\n",
      "43\n",
      "41\n",
      "26\n",
      "38\n",
      "5\n",
      "152\n",
      "10\n",
      "8\n",
      "17\n",
      "10\n",
      "13\n",
      "9\n",
      "13\n",
      "105\n",
      "13\n",
      "10\n",
      "15\n",
      "9\n",
      "7\n",
      "15\n",
      "35\n",
      "25\n",
      "22\n",
      "15\n",
      "25\n",
      "10\n",
      "410\n",
      "24\n",
      "3\n",
      "13\n",
      "87\n",
      "29\n",
      "15\n",
      "25\n",
      "23\n",
      "39\n",
      "11\n",
      "19\n",
      "27\n",
      "13\n",
      "21\n",
      "21\n",
      "10\n",
      "7\n",
      "10\n",
      "24\n",
      "74\n",
      "41\n",
      "9\n",
      "10\n",
      "75\n",
      "4\n",
      "10\n",
      "11\n",
      "57\n",
      "24\n",
      "68\n",
      "25\n",
      "110\n",
      "57\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data...\")\n",
    "train_data = pd.read_csv('../datasets/SumTablets_English_train.csv')\n",
    "\n",
    "# For evaluation, use a separate test set if available, otherwise split from train\n",
    "try:\n",
    "    test_data = pd.read_csv('../datasets/SumTablets_English_test.csv')\n",
    "    print(f\"Loaded {len(train_data)} training examples and {len(test_data)} test examples\")\n",
    "except:\n",
    "    print(\"No separate test file found. Will split from training data.\")\n",
    "    test_data = train_data\n",
    "\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "29ca05e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SOURCE_LENGTH = 256\n",
    "MAX_TARGET_LENGTH = 256\n",
    "\n",
    "class SumerianEnglishDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_source_len, max_target_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data\n",
    "        self.max_source_len = max_source_len\n",
    "        self.max_target_len = max_target_len\n",
    "        \n",
    "        # Filter out rows with missing data\n",
    "        self.filtered_data = []\n",
    "\n",
    "        for idx, row in data.iterrows():\n",
    "            if isinstance(row['transliteration'], str) and isinstance(row['translation'], str):\n",
    "                self.filtered_data.append({\n",
    "                    'sumerian': row['transliteration'].replace('\\n', ' '),\n",
    "                    'english': row['translation'].replace('\\n', ' ')\n",
    "                })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filtered_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        example = self.filtered_data[idx]\n",
    "        \n",
    "        # For MT5, we prepend a task prefix to clarify the task\n",
    "        source_text = f\"translate Sumerian to English: {example['sumerian']}\"\n",
    "        target_text = example['english']\n",
    "        \n",
    "        # Tokenize inputs\n",
    "        source_encoding = self.tokenizer(\n",
    "            source_text,\n",
    "            max_length=self.max_source_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Tokenize targets\n",
    "        target_encoding = self.tokenizer(\n",
    "            target_text,\n",
    "            max_length=self.max_target_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Replace padding token id's with -100 for loss calculation\n",
    "        target_ids = target_encoding[\"input_ids\"]\n",
    "        target_ids[target_ids == self.tokenizer.pad_token_id] = -100\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": source_encoding[\"input_ids\"].squeeze().numpy(),\n",
    "            \"attention_mask\": source_encoding[\"attention_mask\"].squeeze().numpy(),\n",
    "            \"labels\": target_ids.squeeze().numpy()\n",
    "        }\n",
    "\n",
    "# Create training dataset\n",
    "train_dataset = SumerianEnglishDataset(\n",
    "    train_data, \n",
    "    tokenizer, \n",
    "    max_source_len=MAX_SOURCE_LENGTH, \n",
    "    max_target_len=MAX_TARGET_LENGTH\n",
    ")\n",
    "\n",
    "# Create test dataset\n",
    "test_dataset = SumerianEnglishDataset(\n",
    "    test_data, \n",
    "    tokenizer, \n",
    "    max_source_len=MAX_SOURCE_LENGTH, \n",
    "    max_target_len=MAX_TARGET_LENGTH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "89dadce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 1714 training and 191 validation samples\n"
     ]
    }
   ],
   "source": [
    "TRAIN_VALID_SPLIT = 0.1\n",
    "\n",
    "# Split into training and validation sets\n",
    "if TRAIN_VALID_SPLIT > 0:\n",
    "    train_size = int((1 - TRAIN_VALID_SPLIT) * len(train_dataset))\n",
    "    valid_size = len(train_dataset) - train_size\n",
    "    train_dataset, eval_dataset = random_split(train_dataset, [train_size, valid_size])\n",
    "    print(f\"Split into {train_size} training and {valid_size} validation samples\")\n",
    "else:\n",
    "    train_dataset = train_dataset\n",
    "    eval_dataset = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dc6b2c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3455/1279589460.py:127: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='107' max='107' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [107/107 02:51, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Meteor</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/default/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/default/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/default/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample predictions (first 2):\n",
      "Pred: ''\n",
      "Label: '2 nannies, ... of the sukkalmaḫ, via Šeškalla, the household manager, booked out; year: “Šašrum was destroyed.”'\n",
      "---\n",
      "Pred: ''\n",
      "Label: '11 lambs, 1 billy goat, 3rd day, from Abbasaga Intaea accepted; month: “Piglet-feast,” year: “Šašru was destroyed;” (total:) 12.'\n",
      "---\n",
      "Warning: No valid (non-empty) prediction-label pairs found\n",
      "Saving model to ./sumerian_mt5_model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./sumerian_mt5_model/tokenizer_config.json',\n",
       " './sumerian_mt5_model/special_tokens_map.json',\n",
       " './sumerian_mt5_model/spiece.model',\n",
       " './sumerian_mt5_model/added_tokens.json',\n",
       " './sumerian_mt5_model/tokenizer.json')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training hyperparameters\n",
    "NUM_EPOCHS = 1\n",
    "LEARNING_RATE = 2e-5\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# --- Define evaluation metrics ---\n",
    "def compute_metrics(eval_preds):\n",
    "    bleu_metric = load(\"bleu\")\n",
    "    meteor_metric = load(\"meteor\")\n",
    "    rouge_metric = load(\"rouge\")\n",
    "    \n",
    "    preds, labels = eval_preds\n",
    "    \n",
    "    # Replace -100 with pad token id\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    \n",
    "    # Decode predictions and labels\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Clean up predictions and labels\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [label.strip() for label in decoded_labels]\n",
    "    \n",
    "    # Print some examples for debugging\n",
    "    print(\"\\nSample predictions (first 2):\")\n",
    "    for i in range(min(2, len(decoded_preds))):\n",
    "        print(f\"Pred: '{decoded_preds[i]}'\")\n",
    "        print(f\"Label: '{decoded_labels[i]}'\")\n",
    "        print(\"---\")\n",
    "    \n",
    "    # Check if we have any valid predictions/labels to work with\n",
    "    if not decoded_preds or not decoded_labels:\n",
    "        print(\"Warning: Empty predictions or labels\")\n",
    "        return {\n",
    "            \"bleu\": 0.0,\n",
    "            \"meteor\": 0.0, \n",
    "            \"rougeL\": 0.0,\n",
    "            \"gen_len\": 0.0\n",
    "        }\n",
    "    \n",
    "    # Ensure all predictions and labels have content (not empty strings)\n",
    "    valid_pairs = [(p, l) for p, l in zip(decoded_preds, decoded_labels) if p.strip() and l.strip()]\n",
    "    if not valid_pairs:\n",
    "        print(\"Warning: No valid (non-empty) prediction-label pairs found\")\n",
    "        return {\n",
    "            \"bleu\": 0.0,\n",
    "            \"meteor\": 0.0, \n",
    "            \"rougeL\": 0.0,\n",
    "            \"gen_len\": 0.0\n",
    "        }\n",
    "    \n",
    "    # Unzip the valid pairs\n",
    "    valid_preds, valid_labels = zip(*valid_pairs)\n",
    "    \n",
    "    # Format references for BLEU\n",
    "    references_for_bleu = [[label] for label in valid_labels]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    results = {}\n",
    "    \n",
    "    try:\n",
    "        # BLEU\n",
    "        bleu_results = bleu_metric.compute(predictions=valid_preds, references=references_for_bleu)\n",
    "        results[\"bleu\"] = bleu_results[\"bleu\"] if bleu_results else 0.0\n",
    "        \n",
    "        # METEOR\n",
    "        meteor_results = meteor_metric.compute(predictions=valid_preds, references=valid_labels)\n",
    "        results[\"meteor\"] = meteor_results[\"meteor\"] if meteor_results else 0.0\n",
    "        \n",
    "        # ROUGE\n",
    "        rouge_results = rouge_metric.compute(predictions=valid_preds, references=valid_labels)\n",
    "        results[\"rougeL\"] = rouge_results[\"rougeL\"] if rouge_results else 0.0\n",
    "        \n",
    "        # Add prediction length\n",
    "        pred_lens = [len(pred.split()) for pred in valid_preds]\n",
    "        results[\"gen_len\"] = np.mean(pred_lens) if pred_lens else 0.0\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error computing metrics: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        # Return zeros for all metrics if computation fails\n",
    "        return {\n",
    "            \"bleu\": 0.0,\n",
    "            \"meteor\": 0.0, \n",
    "            \"rougeL\": 0.0,\n",
    "            \"gen_len\": 0.0\n",
    "        }\n",
    "    \n",
    "    return {k: round(v, 4) if isinstance(v, float) else v for k, v in results.items()}\n",
    "    \n",
    "# --- Data collator ---\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=-100,\n",
    "    pad_to_multiple_of=8 if torch.cuda.is_available() else None\n",
    ")\n",
    "\n",
    "# --- 7. Training arguments ---\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    predict_with_generate=True,\n",
    "    logging_dir=LOG_DIR,\n",
    "    logging_steps=100,\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    gradient_accumulation_steps=2,\n",
    "    max_grad_norm=1.0,                      # gradient clipping\n",
    "    generation_max_length=MAX_TARGET_LENGTH,\n",
    "    report_to=\"tensorboard\",\n",
    "    warmup_steps=500,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\"\n",
    ")\n",
    "\n",
    "# --- 8. Initialize trainer ---\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# --- 9. Train the model ---\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "# --- 10. Save the model ---\n",
    "print(f\"Saving model to {OUTPUT_DIR}\")\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd12620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing on example data...\n",
      "\n",
      "Example 1:\n",
      "Sumerian:  ...guruš engar dumu-ni ...ur-mes 1(u) 1(diš) guruš ugula ur-lugal 8(diš) guruš ugula ab-ba-sag₁₀ 6(diš) guruš ugula lugal-ku₃-zu 3(diš) guruš ugula šeš-kal-la 2(diš) guruš ugula lugal-iti-da 4(diš) guruš ugula lu₂-dingir-ra 7(diš) guruš ugula ur-am₃-ma 4(diš) guruš ugula ur-e₂-nun-na  1(geš₂) guruš ugula al-la-igi-še₃-du gurum₂ u₄ 2(diš)-kam ki-su₇ ka-ma-ri₂ gub-ba giri₃ i₃-kal-la iti še-kar-ra-gal₂-la mu {d}šu{d}suen lugal uri₅-ma{ki}...da za-ab-ša-li{ki} mu-hul\n",
      "Actual Translation: n male laborers, plowman and his sons, foreman: Ur-mes, 11 male laborers, foreman: Ur-lugal, 8 male laborers, foreman: Abba-saga, 6 male laborers, foreman: Lugal-kuzu, 3 male laborers, foreman: Šeš-kalla, 2 male laborers, foreman: Lugal-itida, 4 male laborers, foreman: Lu-dingira, 7 male laborers, foreman: Ur-amma, 4 male laborers, foreman: Ur-enunna, 60 male laborers, foreman: Alla-palil; inspection of the second day, on the threshing floor Ka-ma-ri2 stationed, under charge of Ikalla, month: “Barley stored in the harbor,” year: “Šu-Suen, the king of Ur, destroyed the lands of Zabšali.”\n",
      "CUDA error: CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      ". Falling back to CPU.\n",
      "Error in translation: CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "MT5 Translation: Translation error: CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some...\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 2:\n",
      "Sumerian:  1(diš) udu gir-ru-um niga 2(diš) udu eme-gi-ra nita₂ ma₂-an-na unu{ki} iti-ta u₄ 2(u) 6(diš) ba-ra-zal  ki lugal-nir-ta giri₃ ba-qar-tum  iti {d}dumu-zi mu en {d}inana unu{ki} maš₂-e i₃-pa₃   {d}šu{d}suen lugal kal-ga lugal uri₅{ki}ma lugal an ub-da limmu₂-ba  wa-qar-tum nin₉-a-ni\n",
      "Actual Translation: 1 kirrum sheep, grain-fed, 2 emegi rams, for Heaven-barge of Uruk, of the month, the 26th day passed; from Lugal-nir, via Baqartum. month: “Dumuzi,” year: “The high-priestess of Inanna of Uruk by extispicy was chosen.” Šu-Suen, strong king, king of Ur, king of the four quarters: Waqartum, his sister.\n",
      "CUDA error: CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      ". Falling back to CPU.\n",
      "Error in translation: CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "MT5 Translation: Translation error: CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some...\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 3:\n",
      "Sumerian:  2(diš) udu-nita₂ kur-ra bar-gal₂ 1(diš) sila₄ nita₂ kur-ra bar-gal₂ ri-ri-ga  ki ur-ru-ta kišib₃ lu₂-kal-la iti {d}li₉-si₄ mu hu-uh₂-nu-ri{ki} ba-hul  lu₂-kal-la dub-sar dumu ur-e₁₁-e šuš₃\n",
      "Actual Translation: 2 male sheep of the mountain, with fleece, 1 male lamb of the mountain, with fleece, fallen; from Urru, under seal of Lukalla; month: “Lisi,” year: “Ḫuḫnuri was destroyed.” Lukalla, the scribe, son of Ur-E’e, chief livestock administrator.\n",
      "CUDA error: CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      ". Falling back to CPU.\n",
      "Error in translation: CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "MT5 Translation: Translation error: CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some...\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 4:\n",
      "Sumerian:  ...nin₉ ki aŋ₂ {d}dumu-zid-de₃ ...gur₃-ru ki aŋ₂ {d}dur₇-dur₇-e a...zid-de₃ šag₄-ga ri-a nin₉-ŋu₁₀ nam-nin-e tud-da {e₂}tur₃-e i₃ gara₂...mu-un-da-ab-si amaš-e i₃...un-da-an <unk>... an-eden-ne₂...a-ŋu₁₀ {d}ŋeštin-an-na me-en a ki-sikil...nam dumu banda₃{da}...na-nam unug{ki}ga...bi na-nam kul-aba₄{ki}...bi na-nam mu-ut-tin-na me-en ru-ru-a na-nam ki-sikil-e...šeš-a-na... igi mu-un-na-hur kiri₃ mu-un-na... ki lu₂-da nu-u₆-di haš₂-gal mu-na... e₂-eš₂-dam e₂-gal-la si ba-ni-in-sa₂ sukkal lu₂ e₂-gal-la-ta e₃-a ki-sikil-e en₃{en} mu-un-tar-re-en sukkal lu₂ e₂-gal-la-ta e₃-a ki-sikil-ra mu-un-na-ni-ib-gi₄-gi₄ šeš-zu nam-en-še₃ šu mu-un-ŋa₂-ŋa₂-a nam-en unug{ki}ga <unk> <unk>...e  me hal-hal-ne u išib-ne ki za-za-a-ne a-tu₅-a-tu₅-a-ne susbu₂{be₂}e-ne itud-da eš₃ gal-e e₃-ne muš₃ nu-tum₂-mu-ne šeš-zu nam-en-še₃ šu mu-un-ŋa₂-ŋa₂-a ud-bi-a imin he₂-en-na-me-eš imin he₂-en-na-me-eš šir₃ dug₄-dug₄ unug{ki}ga imin me-eš zabalam{ki} ad ša₄-ša₄ ninnu me-eš e-ne-ne an-na mul zu me-eš ki har-ra-an zu me-eš an-na mul zu <unk> il₂-la me-eš e-ne-ne šir₃ im-zi-zi-ne šir₃ im-ŋa₂-ŋa₂-ne šir₃-e saŋ-bi nu-mu-un-ne-pad₃-ne ad-da ki-gub-ba nu-mu-un-ŋa₂-ŋa₂ me-eš nin₉ banda₃{da} <unk> bi-a til₃-la gu₃ mu-un-na-de₂-e gaba ba-da-ab-gu-la gaba ba-da-ab...gu₃ mu-un... me-e-de₃ šir₃ im-zi-zi-ne e-ne-ne...ŋa₂-ŋa₂-ze₂-en ...bi-a nu-mu-un-til₃ ...ma-ka mu-un-til₃ ...ma kur kug za-gin₃-a-ka <unk> dim-ma-ba mu-un-til₃\n",
      "Actual Translation: ... beloved sister of Dumuzi Exuding/bearing ..., beloved of Durtur (Whose) seed was planted in the womb by the true ... My sister, born into ladyship She is able to ... fill the cattle pen with butter and cream In the sheepfold she is able to ... In the plain ... you are my ..., {geš}tinanna Oh, the young woman is indeed ... Your little ones/children ... are indeed ... She is indeed the ... of Unug She is indeed the ... of Kulaba You are ..., indeed she is ... The young lady ... for her brother Scratched her face, scratched(?) her nose Scratched her upper thighs, the place not seen (when) with a man, She made straight away to the tavern of(?) the palace The young woman, she(!) was inquiring of the minister who was leaving the palace The minister who was leaving the palace was answering the young woman Your brother is setting his hands upon(?) the en-ship The en-ship of Unug ... The ones dividing up the me, ten išib priests The ones bowing down, the lustration priests, the susbu priests The ones that leave(?) the great shrine monthly/in the moonlight(?), the unceasing ones, (in their presence?) your brother is setting his hands upon(?) the en-ship At that time there should be seven for him, there should be seven for him The song performers of Unug are seven In Zabalam there were fifty lamenters They know the stars in heaven, they know the roads (on) earth The ones that know the stars in heaven are “carrying/raising the middle\"(?) They were “raising” the song and “putting down” the song In (performing) the song they(?) could not find “its head” (beginning, soloist, director or (proper) tuning?) for themselves(?) It was them who were not “putting” (the song?) into the (proper) voice and “station\"(?) The little sister dwelling in the midst was speaking/singing? She was making (her) chest great (i.e., inhaling, or a technical musical term?), she was ... her chest, she was speaking/singing We will “raise” the song, and you(!) will “put down” the song ... did not live in ... ... lived in the ... of(?) ... She lived in the ..., the mountain of silver and lapis lazuli, in its ...\n",
      "CUDA error: CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      ". Falling back to CPU.\n",
      "Error in translation: CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "MT5 Translation: Translation error: CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some...\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 5:\n",
      "Sumerian:  <unk> nin dub-sar dumu šeš-kal-la\n",
      "Actual Translation: Šu-Suen, strong king, king of Ur: Aḫuni, cup-bearer, is your servant.\n",
      "CUDA error: CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      ". Falling back to CPU.\n",
      "Error in translation: CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "MT5 Translation: Translation error: CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some...\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(\"\\nTesting on example data...\")\n",
    "\n",
    "def generate_translation(sumerian_text):\n",
    "    # Clean and truncate input text to avoid potential issues\n",
    "    sumerian_text = sumerian_text.strip()\n",
    "    if len(sumerian_text) > 1000:  # Arbitrary limit to prevent very long inputs\n",
    "        sumerian_text = sumerian_text[:1000] + \"...\"\n",
    "    \n",
    "    input_text = f\"translate Sumerian to English: {sumerian_text}\"\n",
    "    \n",
    "    try:\n",
    "        # Process input with truncation to avoid sequence length issues\n",
    "        inputs = tokenizer(\n",
    "            input_text, \n",
    "            return_tensors=\"pt\", \n",
    "            padding=True, \n",
    "            truncation=True,\n",
    "            max_length=MAX_SOURCE_LENGTH\n",
    "        )\n",
    "        \n",
    "        # Move to CPU if CUDA issues persist\n",
    "        if torch.cuda.is_available():\n",
    "            try:\n",
    "                inputs = inputs.to(device)\n",
    "                \n",
    "                # Try with beam search (safer parameters)\n",
    "                outputs = model.generate(\n",
    "                    input_ids=inputs[\"input_ids\"],\n",
    "                    attention_mask=inputs[\"attention_mask\"],\n",
    "                    max_length=MAX_TARGET_LENGTH,\n",
    "                    min_length=5,\n",
    "                    num_beams=2,\n",
    "                    length_penalty=1.0,\n",
    "                    early_stopping=True,\n",
    "                    do_sample=False\n",
    "                )\n",
    "                \n",
    "            except RuntimeError as e:\n",
    "                print(f\"CUDA error: {e}. Falling back to CPU.\")\n",
    "                # Fall back to CPU\n",
    "                inputs = {k: v.cpu() for k, v in inputs.items()}\n",
    "                model.cpu()\n",
    "                \n",
    "                outputs = model.generate(\n",
    "                    input_ids=inputs[\"input_ids\"],\n",
    "                    attention_mask=inputs[\"attention_mask\"],\n",
    "                    max_length=MAX_TARGET_LENGTH,\n",
    "                    min_length=5,\n",
    "                    num_beams=2,\n",
    "                    do_sample=False\n",
    "                )\n",
    "                \n",
    "                # Move model back to the original device\n",
    "                model.to(device)\n",
    "        else:\n",
    "            # Already on CPU\n",
    "            outputs = model.generate(\n",
    "                input_ids=inputs[\"input_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"],\n",
    "                max_length=MAX_TARGET_LENGTH,\n",
    "                min_length=5,\n",
    "                num_beams=2,\n",
    "                do_sample=False\n",
    "            )\n",
    "        \n",
    "        translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in translation: {e}\")\n",
    "        translation = f\"Translation error: {str(e)[:100]}...\"\n",
    "        \n",
    "    return translation\n",
    "\n",
    "for i, row in test_data.head(5).iterrows():\n",
    "    if isinstance(row['transliteration'], str):\n",
    "        sumerian_text = row['transliteration'].replace('\\n', ' ')\n",
    "        actual_translation = row['translation'].replace('\\n', ' ') if isinstance(row['translation'], str) else \"N/A\"\n",
    "        \n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(f\"Sumerian: {sumerian_text}\")\n",
    "        print(f\"Actual Translation: {actual_translation}\")\n",
    "        \n",
    "        generated_translation = generate_translation(sumerian_text)\n",
    "        print(f\"MT5 Translation: {generated_translation}\")\n",
    "        print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azureml_py310_sdkv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
