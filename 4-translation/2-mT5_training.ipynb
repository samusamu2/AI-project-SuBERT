{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53844f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from transformers import (\n",
    "    MT5ForConditionalGeneration,\n",
    "    MT5Tokenizer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import GenerationConfig, EarlyStoppingCallback\n",
    "\n",
    "from datasets import load_dataset, Dataset as HFDataset\n",
    "from load_dataset import preprocess_dataset\n",
    "from compute_metrics import compute_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25fe5f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"google/mt5-small\"\n",
    "# Directory to save the fine-tuned model\n",
    "OUTPUT_DIR = \"./mt5_model\"\n",
    "# Directory for TensorBoard logs\n",
    "LOGGING_DIR = \"./mt5_logs\"\n",
    "\n",
    "# Some hyperparameters\n",
    "MAX_INPUT_LENGTH = 512\n",
    "MAX_TARGET_LENGTH = 512\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 1e-5\n",
    "NUM_TRAIN_EPOCHS = 100\n",
    "\n",
    "# check if dirs exist, if not create them\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(LOGGING_DIR, exist_ok=True)\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "model.to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "856b3276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1907 examples from ../datasets/SumTablets_English_train.csv\n",
      "Preprocessed dataset contains 1905 examples\n",
      "Loaded 107 examples from ../datasets/SumTablets_English_validation.csv\n",
      "Preprocessed dataset contains 107 examples\n",
      "Loaded 113 examples from ../datasets/SumTablets_English_test.csv\n",
      "Preprocessed dataset contains 113 examples\n"
     ]
    }
   ],
   "source": [
    "preprocessed_train = preprocess_dataset('../datasets/SumTablets_English_train.csv')\n",
    "preprocessed_val = preprocess_dataset('../datasets/SumTablets_English_validation.csv')\n",
    "preprocessed_test = preprocess_dataset('../datasets/SumTablets_English_test.csv')\n",
    "\n",
    "train_data = [{\n",
    "    'source': row['sumerian'],\n",
    "    'target': row['english']\n",
    "} for _, row in preprocessed_train.iterrows()]\n",
    "\n",
    "val_data = [{\n",
    "    'source': row['sumerian'],\n",
    "    'target': row['english']\n",
    "} for _, row in preprocessed_val.iterrows()]\n",
    "\n",
    "test_data = [{\n",
    "    'source': row['sumerian'],\n",
    "    'target': row['english']\n",
    "} for _, row in preprocessed_test.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df62b147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing datasets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05e3e3aabd0c44578bf1d080d3df28fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1905 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0921c16b0c4b43e0ac5a355508c6363e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/107 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    \"\"\"\n",
    "    Tokenizes the source (Sumerian) and target (English) texts.\n",
    "    \"\"\"\n",
    "    inputs = examples['source']\n",
    "    targets = examples['target']\n",
    "\n",
    "    # DEBUG: Print flag if any of inputs or targets are none\n",
    "    if any(x is None for x in inputs) or any(x is None for x in targets):\n",
    "        print(\"Warning: Found None values in inputs or targets. This may affect training.\")\n",
    "\n",
    "\n",
    "    model_inputs = tokenizer(inputs, max_length=MAX_INPUT_LENGTH, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    # Tokenize targets (English) using the newer approach\n",
    "    labels = tokenizer(text_target=targets, max_length=MAX_TARGET_LENGTH, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Convert lists to Hugging Face Dataset objects\n",
    "train_dataset = HFDataset.from_list(train_data)\n",
    "val_dataset = HFDataset.from_list(val_data)\n",
    "\n",
    "# Apply preprocessing to the datasets\n",
    "print(\"Tokenizing datasets...\")\n",
    "tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_val_dataset = val_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2859a758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of tokenized input:\n",
      "{'source': ' 1(u) la₂ 1(diš) udu u₄ 2(u) 8(diš)-kam ki ab-ba-sa₆-ga-ta na-lu₅ i₃-dab₅   iti <unk> bi₂-gu₇ mu en-unu₆-gal {d}inana unu{ki}ga ba-hun  1(u) la₂ 1(diš)', 'target': '9 rams, 28th day, from Abba-saga, Nalu accepted; month: “ubi-feast,” year: “Enunugal of Inanna of Uruk was installed;” (total:) 9 (rams).', 'input_ids': [333, 312, 273, 271, 283, 338, 333, 312, 720, 1166, 271, 259, 17278, 259, 273, 410, 356, 312, 273, 271, 630, 312, 720, 1166, 271, 264, 13555, 504, 1995, 264, 835, 264, 263, 262, 451, 264, 743, 264, 422, 294, 264, 1696, 428, 259, 266, 328, 264, 31256, 428, 259, 2650, 2, 837, 14528, 2871, 487, 890, 289, 264, 14031, 451, 264, 6362, 785, 285, 1354, 348, 1238, 18308, 596, 650, 1354, 743, 810, 264, 20544, 333, 312, 273, 271, 283, 338, 333, 312, 720, 1166, 271, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [774, 259, 2833, 263, 261, 1190, 807, 3117, 261, 702, 116285, 264, 39084, 261, 727, 1696, 12004, 345, 296, 11400, 267, 359, 40081, 264, 367, 82365, 5224, 3721, 267, 359, 6187, 14031, 6362, 304, 563, 6702, 304, 84809, 314, 639, 259, 58576, 296, 365, 274, 27277, 267, 271, 774, 274, 2833, 263, 483, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "print(\"Example of tokenized input:\")\n",
    "print(tokenized_train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6b2c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_85797/756632298.py:53: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred during training: CUDA out of memory. Tried to allocate 3.82 GiB. GPU 0 has a total capacity of 15.77 GiB of which 1.13 GiB is free. Including non-PyTorch memory, this process has 14.64 GiB memory in use. Of the allocated memory 14.15 GiB is allocated by PyTorch, and 121.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Set the training arguments for the Seq2SeqTrainer\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,                  # Directory to save the model\n",
    "\n",
    "    num_train_epochs=NUM_TRAIN_EPOCHS,      # Number of training epochs\n",
    "    per_device_train_batch_size=BATCH_SIZE, # Batch size for training\n",
    "    per_device_eval_batch_size=BATCH_SIZE,  # Batch size for evaluation\n",
    "\n",
    "    learning_rate=LEARNING_RATE,            # Learning rate for the optimizer\n",
    "    weight_decay=0.01,                      # Weight decay for regularization\n",
    "    warmup_ratio=0.1,                       # Warmup ratio for learning rate scheduler\n",
    "    gradient_accumulation_steps=1,          # Gradient accumulation steps to simulate larger batch sizes\n",
    "    lr_scheduler_type=\"cosine\",             # Use cosine learning rate scheduler\n",
    "    label_smoothing_factor=0.1,             # Label smoothing factor for better generalization\n",
    "    max_grad_norm=1.0,                      # gradient clipping\n",
    "\n",
    "    save_total_limit=1,                     # Only keep the last checkpoint\n",
    "    predict_with_generate=True,             # Enable generation during evaluation\n",
    "    report_to=\"tensorboard\",                # Report metrics to TensorBoard\n",
    "    logging_dir=LOGGING_DIR,                # Directory for TensorBoard logs\n",
    "    logging_steps=50,                       # Log every 50 steps\n",
    "\n",
    "    eval_strategy=\"epoch\",                  # Evaluate at the end of each epoch\n",
    "    save_strategy=\"epoch\",                  # Save model at the end of each epoch\n",
    "    load_best_model_at_end=True,            # Load the best model at the end of training\n",
    "    metric_for_best_model=\"meteor\",         # Metric to determine the best model\n",
    "    fp16=torch.cuda.is_available(),         # Use mixed precision training if GPU is available\n",
    ")\n",
    "\n",
    "# Set up generation configuration for the model\n",
    "generation_config = GenerationConfig(\n",
    "    max_length=MAX_TARGET_LENGTH,           # Maximum length of the generated sequences\n",
    "    early_stopping=True,                    # Stop generation when all beams reach the EOS token\n",
    "    num_beams=4,                            # Number of beams for beam search\n",
    "    no_repeat_ngram_size=3,                 # Prevent repetition of n-grams in the generated text\n",
    "    forced_bos_token_id=0,                  # Force the beginning of the sequence to be the BOS token\n",
    "    pad_token_id=tokenizer.pad_token_id,    # Padding token ID for the tokenizer\n",
    "    eos_token_id=tokenizer.eos_token_id,    # End of sequence token ID for the tokenizer\n",
    "    decoder_start_token_id=tokenizer.bos_token_id if tokenizer.bos_token_id is not None else tokenizer.eos_token_id   # Decoder start token ID for the model\n",
    ")\n",
    "model.generation_config = generation_config\n",
    "\n",
    "\n",
    "# Data collator for padding\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    padding=True,\n",
    "    max_length=MAX_INPUT_LENGTH\n",
    ")\n",
    "\n",
    "# Initialize the Seq2SeqTrainer with the model, training arguments, datasets, tokenizer, data collator, and metrics computation\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=lambda p: compute_metrics(p, tokenizer),        # Function to compute metrics during evaluation\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]    # Early stopping callback to prevent overfitting\n",
    "    )\n",
    "\n",
    "# Start the training process\n",
    "print(\"Starting model training...\")\n",
    "try:\n",
    "    trainer.train()\n",
    "    print(\"Training finished successfully!\")\n",
    "\n",
    "    # Save the final model and tokenizer\n",
    "    trainer.save_model(f\"{OUTPUT_DIR}/final_model\")\n",
    "    tokenizer.save_pretrained(f\"{OUTPUT_DIR}/final_model_tokenizer\")\n",
    "    print(f\"Final model saved to {OUTPUT_DIR}/final_model\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during training: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd12620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing on example data...\n",
      "\n",
      "Example 1:\n",
      "Sumerian:  ...guruš engar dumu-ni ...ur-mes 1(u) 1(diš) guruš ugula ur-lugal 8(diš) guruš ugula ab-ba-sag₁₀ 6(diš) guruš ugula lugal-ku₃-zu 3(diš) guruš ugula šeš-kal-la 2(diš) guruš ugula lugal-iti-da 4(diš) guruš ugula lu₂-dingir-ra 7(diš) guruš ugula ur-am₃-ma 4(diš) guruš ugula ur-e₂-nun-na  1(geš₂) guruš ugula al-la-igi-še₃-du gurum₂ u₄ 2(diš)-kam ki-su₇ ka-ma-ri₂ gub-ba giri₃ i₃-kal-la iti še-kar-ra-gal₂-la mu {d}šu{d}suen lugal uri₅-ma{ki}...da za-ab-ša-li{ki} mu-hul\n",
      "Actual Translation: n male laborers, plowman and his sons, foreman: Ur-mes, 11 male laborers, foreman: Ur-lugal, 8 male laborers, foreman: Abba-saga, 6 male laborers, foreman: Lugal-kuzu, 3 male laborers, foreman: Šeš-kalla, 2 male laborers, foreman: Lugal-itida, 4 male laborers, foreman: Lu-dingira, 7 male laborers, foreman: Ur-amma, 4 male laborers, foreman: Ur-enunna, 60 male laborers, foreman: Alla-palil; inspection of the second day, on the threshing floor Ka-ma-ri2 stationed, under charge of Ikalla, month: “Barley stored in the harbor,” year: “Šu-Suen, the king of Ur, destroyed the lands of Zabšali.”\n",
      "CUDA error: CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      ". Falling back to CPU.\n",
      "Error in translation: CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "MT5 Translation: Translation error: CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some...\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 2:\n",
      "Sumerian:  1(diš) udu gir-ru-um niga 2(diš) udu eme-gi-ra nita₂ ma₂-an-na unu{ki} iti-ta u₄ 2(u) 6(diš) ba-ra-zal  ki lugal-nir-ta giri₃ ba-qar-tum  iti {d}dumu-zi mu en {d}inana unu{ki} maš₂-e i₃-pa₃   {d}šu{d}suen lugal kal-ga lugal uri₅{ki}ma lugal an ub-da limmu₂-ba  wa-qar-tum nin₉-a-ni\n",
      "Actual Translation: 1 kirrum sheep, grain-fed, 2 emegi rams, for Heaven-barge of Uruk, of the month, the 26th day passed; from Lugal-nir, via Baqartum. month: “Dumuzi,” year: “The high-priestess of Inanna of Uruk by extispicy was chosen.” Šu-Suen, strong king, king of Ur, king of the four quarters: Waqartum, his sister.\n",
      "CUDA error: CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      ". Falling back to CPU.\n",
      "Error in translation: CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "MT5 Translation: Translation error: CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some...\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 3:\n",
      "Sumerian:  2(diš) udu-nita₂ kur-ra bar-gal₂ 1(diš) sila₄ nita₂ kur-ra bar-gal₂ ri-ri-ga  ki ur-ru-ta kišib₃ lu₂-kal-la iti {d}li₉-si₄ mu hu-uh₂-nu-ri{ki} ba-hul  lu₂-kal-la dub-sar dumu ur-e₁₁-e šuš₃\n",
      "Actual Translation: 2 male sheep of the mountain, with fleece, 1 male lamb of the mountain, with fleece, fallen; from Urru, under seal of Lukalla; month: “Lisi,” year: “Ḫuḫnuri was destroyed.” Lukalla, the scribe, son of Ur-E’e, chief livestock administrator.\n",
      "CUDA error: CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      ". Falling back to CPU.\n",
      "Error in translation: CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "MT5 Translation: Translation error: CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some...\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 4:\n",
      "Sumerian:  ...nin₉ ki aŋ₂ {d}dumu-zid-de₃ ...gur₃-ru ki aŋ₂ {d}dur₇-dur₇-e a...zid-de₃ šag₄-ga ri-a nin₉-ŋu₁₀ nam-nin-e tud-da {e₂}tur₃-e i₃ gara₂...mu-un-da-ab-si amaš-e i₃...un-da-an <unk>... an-eden-ne₂...a-ŋu₁₀ {d}ŋeštin-an-na me-en a ki-sikil...nam dumu banda₃{da}...na-nam unug{ki}ga...bi na-nam kul-aba₄{ki}...bi na-nam mu-ut-tin-na me-en ru-ru-a na-nam ki-sikil-e...šeš-a-na... igi mu-un-na-hur kiri₃ mu-un-na... ki lu₂-da nu-u₆-di haš₂-gal mu-na... e₂-eš₂-dam e₂-gal-la si ba-ni-in-sa₂ sukkal lu₂ e₂-gal-la-ta e₃-a ki-sikil-e en₃{en} mu-un-tar-re-en sukkal lu₂ e₂-gal-la-ta e₃-a ki-sikil-ra mu-un-na-ni-ib-gi₄-gi₄ šeš-zu nam-en-še₃ šu mu-un-ŋa₂-ŋa₂-a nam-en unug{ki}ga <unk> <unk>...e  me hal-hal-ne u išib-ne ki za-za-a-ne a-tu₅-a-tu₅-a-ne susbu₂{be₂}e-ne itud-da eš₃ gal-e e₃-ne muš₃ nu-tum₂-mu-ne šeš-zu nam-en-še₃ šu mu-un-ŋa₂-ŋa₂-a ud-bi-a imin he₂-en-na-me-eš imin he₂-en-na-me-eš šir₃ dug₄-dug₄ unug{ki}ga imin me-eš zabalam{ki} ad ša₄-ša₄ ninnu me-eš e-ne-ne an-na mul zu me-eš ki har-ra-an zu me-eš an-na mul zu <unk> il₂-la me-eš e-ne-ne šir₃ im-zi-zi-ne šir₃ im-ŋa₂-ŋa₂-ne šir₃-e saŋ-bi nu-mu-un-ne-pad₃-ne ad-da ki-gub-ba nu-mu-un-ŋa₂-ŋa₂ me-eš nin₉ banda₃{da} <unk> bi-a til₃-la gu₃ mu-un-na-de₂-e gaba ba-da-ab-gu-la gaba ba-da-ab...gu₃ mu-un... me-e-de₃ šir₃ im-zi-zi-ne e-ne-ne...ŋa₂-ŋa₂-ze₂-en ...bi-a nu-mu-un-til₃ ...ma-ka mu-un-til₃ ...ma kur kug za-gin₃-a-ka <unk> dim-ma-ba mu-un-til₃\n",
      "Actual Translation: ... beloved sister of Dumuzi Exuding/bearing ..., beloved of Durtur (Whose) seed was planted in the womb by the true ... My sister, born into ladyship She is able to ... fill the cattle pen with butter and cream In the sheepfold she is able to ... In the plain ... you are my ..., {geš}tinanna Oh, the young woman is indeed ... Your little ones/children ... are indeed ... She is indeed the ... of Unug She is indeed the ... of Kulaba You are ..., indeed she is ... The young lady ... for her brother Scratched her face, scratched(?) her nose Scratched her upper thighs, the place not seen (when) with a man, She made straight away to the tavern of(?) the palace The young woman, she(!) was inquiring of the minister who was leaving the palace The minister who was leaving the palace was answering the young woman Your brother is setting his hands upon(?) the en-ship The en-ship of Unug ... The ones dividing up the me, ten išib priests The ones bowing down, the lustration priests, the susbu priests The ones that leave(?) the great shrine monthly/in the moonlight(?), the unceasing ones, (in their presence?) your brother is setting his hands upon(?) the en-ship At that time there should be seven for him, there should be seven for him The song performers of Unug are seven In Zabalam there were fifty lamenters They know the stars in heaven, they know the roads (on) earth The ones that know the stars in heaven are “carrying/raising the middle\"(?) They were “raising” the song and “putting down” the song In (performing) the song they(?) could not find “its head” (beginning, soloist, director or (proper) tuning?) for themselves(?) It was them who were not “putting” (the song?) into the (proper) voice and “station\"(?) The little sister dwelling in the midst was speaking/singing? She was making (her) chest great (i.e., inhaling, or a technical musical term?), she was ... her chest, she was speaking/singing We will “raise” the song, and you(!) will “put down” the song ... did not live in ... ... lived in the ... of(?) ... She lived in the ..., the mountain of silver and lapis lazuli, in its ...\n",
      "CUDA error: CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      ". Falling back to CPU.\n",
      "Error in translation: CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "MT5 Translation: Translation error: CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some...\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 5:\n",
      "Sumerian:  <unk> nin dub-sar dumu šeš-kal-la\n",
      "Actual Translation: Šu-Suen, strong king, king of Ur: Aḫuni, cup-bearer, is your servant.\n",
      "CUDA error: CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      ". Falling back to CPU.\n",
      "Error in translation: CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "MT5 Translation: Translation error: CUDA error: device-side assert triggered\n",
      "CUDA kernel errors might be asynchronously reported at some...\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(\"\\nTesting on example data...\")\n",
    "\n",
    "def generate_translation(sumerian_text):\n",
    "    # Clean and truncate input text to avoid potential issues\n",
    "    sumerian_text = sumerian_text.strip()\n",
    "    if len(sumerian_text) > 1000:  # Arbitrary limit to prevent very long inputs\n",
    "        sumerian_text = sumerian_text[:1000] + \"...\"\n",
    "    \n",
    "    input_text = f\"translate Sumerian to English: {sumerian_text}\"\n",
    "    \n",
    "    try:\n",
    "        # Process input with truncation to avoid sequence length issues\n",
    "        inputs = tokenizer(\n",
    "            input_text, \n",
    "            return_tensors=\"pt\", \n",
    "            padding=True, \n",
    "            truncation=True,\n",
    "            max_length=MAX_SOURCE_LENGTH\n",
    "        )\n",
    "        \n",
    "        # Move to CPU if CUDA issues persist\n",
    "        if torch.cuda.is_available():\n",
    "            try:\n",
    "                inputs = inputs.to(device)\n",
    "                \n",
    "                # Try with beam search (safer parameters)\n",
    "                outputs = model.generate(\n",
    "                    input_ids=inputs[\"input_ids\"],\n",
    "                    attention_mask=inputs[\"attention_mask\"],\n",
    "                    max_length=MAX_TARGET_LENGTH,\n",
    "                    min_length=5,\n",
    "                    num_beams=2,\n",
    "                    length_penalty=1.0,\n",
    "                    early_stopping=True,\n",
    "                    do_sample=False\n",
    "                )\n",
    "                \n",
    "            except RuntimeError as e:\n",
    "                print(f\"CUDA error: {e}. Falling back to CPU.\")\n",
    "                # Fall back to CPU\n",
    "                inputs = {k: v.cpu() for k, v in inputs.items()}\n",
    "                model.cpu()\n",
    "                \n",
    "                outputs = model.generate(\n",
    "                    input_ids=inputs[\"input_ids\"],\n",
    "                    attention_mask=inputs[\"attention_mask\"],\n",
    "                    max_length=MAX_TARGET_LENGTH,\n",
    "                    min_length=5,\n",
    "                    num_beams=2,\n",
    "                    do_sample=False\n",
    "                )\n",
    "                \n",
    "                # Move model back to the original device\n",
    "                model.to(device)\n",
    "        else:\n",
    "            # Already on CPU\n",
    "            outputs = model.generate(\n",
    "                input_ids=inputs[\"input_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"],\n",
    "                max_length=MAX_TARGET_LENGTH,\n",
    "                min_length=5,\n",
    "                num_beams=2,\n",
    "                do_sample=False\n",
    "            )\n",
    "        \n",
    "        translation = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in translation: {e}\")\n",
    "        translation = f\"Translation error: {str(e)[:100]}...\"\n",
    "        \n",
    "    return translation\n",
    "\n",
    "for i, row in test_data.head(5).iterrows():\n",
    "    if isinstance(row['transliteration'], str):\n",
    "        sumerian_text = row['transliteration'].replace('\\n', ' ')\n",
    "        actual_translation = row['translation'].replace('\\n', ' ') if isinstance(row['translation'], str) else \"N/A\"\n",
    "        \n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(f\"Sumerian: {sumerian_text}\")\n",
    "        print(f\"Actual Translation: {actual_translation}\")\n",
    "        \n",
    "        generated_translation = generate_translation(sumerian_text)\n",
    "        print(f\"MT5 Translation: {generated_translation}\")\n",
    "        print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azureml_py310_sdkv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
