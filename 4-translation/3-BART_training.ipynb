{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf868bf4",
   "metadata": {},
   "source": [
    "# BART Model\n",
    "\n",
    "This notebook implements translation task using the BART (Bidirectional and Auto-Regressive Transformers). The implementation demonstrates how transfer learning can be applied to low-resource languages by fine-tuning a pre-trained language model on specialized domain data.\n",
    "\n",
    "Here are some key procedures you'll find along the notebook:\n",
    "- **Data Preparation:** load and preprocess Sumerian-English parallel texts from the SumTablets dataset\n",
    "- **Model Configuration:** set up a BART model (either base or large version) with appropriate parameters for the translation task\n",
    "- **Training Pipeline:** implement a complete training workflow using Hugging Face's Transformers library with:\n",
    "    - Dynamic tokenization of source and target texts\n",
    "    - Sequence-to-sequence training with teacher forcing\n",
    "    - Early stopping to prevent overfitting\n",
    "    - Learning rate scheduling and mixed precision training\n",
    "    - Evaluation metrics tracking (BLEU, METEOR, ROUGE)\n",
    "- **Translation Interface:** Provide functionality to translate new Sumerian texts using the fine-tuned model\n",
    "- **Model Saving:** save the trained model and facilitates deployment by backing up to OneDrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d43dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
    "from transformers import GenerationConfig, EarlyStoppingCallback\n",
    "\n",
    "from datasets import load_dataset, Dataset as HFDataset\n",
    "from load_dataset import preprocess_dataset\n",
    "from compute_metrics import compute_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b6308e",
   "metadata": {},
   "outputs": [],
   "source": [
    "large = False  # Set to True if using the large version of BART\n",
    "\n",
    "MODEL_NAME = \"facebook/bart-base\" if not large else \"facebook/bart-large\"\n",
    "# Directory to save the fine-tuned model\n",
    "OUTPUT_DIR = \"./bart_model\" if not large else \"./bart_large_model\"\n",
    "# Directory for TensorBoard logs\n",
    "LOGGING_DIR = \"./bart_logs\" if not large else \"./bart_large_logs\"\n",
    "\n",
    "# Some hyperparameters\n",
    "MAX_INPUT_LENGTH = 512\n",
    "MAX_TARGET_LENGTH = 512\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 1e-5\n",
    "NUM_TRAIN_EPOCHS = 100\n",
    "\n",
    "# check if dirs exist, if not create them\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(LOGGING_DIR, exist_ok=True)\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BartForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
    "model.to(device)\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "if not hasattr(model.config, \"decoder_start_token_id\") or model.config.decoder_start_token_id is None:\n",
    "    model.config.decoder_start_token_id = tokenizer.bos_token_id if tokenizer.bos_token_id is not None else tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf985e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1907 examples from ../datasets/SumTablets_English_train.csv\n",
      "Preprocessed dataset contains 1905 examples\n",
      "Loaded 107 examples from ../datasets/SumTablets_English_validation.csv\n",
      "Preprocessed dataset contains 107 examples\n",
      "Loaded 113 examples from ../datasets/SumTablets_English_test.csv\n",
      "Preprocessed dataset contains 113 examples\n"
     ]
    }
   ],
   "source": [
    "preprocessed_train = preprocess_dataset('../datasets/SumTablets_English_train.csv')\n",
    "preprocessed_val = preprocess_dataset('../datasets/SumTablets_English_validation.csv')\n",
    "preprocessed_test = preprocess_dataset('../datasets/SumTablets_English_test.csv')\n",
    "\n",
    "train_data = [{\n",
    "    'source': row['sumerian'],\n",
    "    'target': row['english']\n",
    "} for _, row in preprocessed_train.iterrows()]\n",
    "\n",
    "val_data = [{\n",
    "    'source': row['sumerian'],\n",
    "    'target': row['english']\n",
    "} for _, row in preprocessed_val.iterrows()]\n",
    "\n",
    "test_data = [{\n",
    "    'source': row['sumerian'],\n",
    "    'target': row['english']\n",
    "} for _, row in preprocessed_test.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbb777b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing datasets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce878f3f98d8416a94e8ece3102972df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1905 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fdf598386ab44abb2b07a173a4e40ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/107 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    \"\"\"\n",
    "    Tokenizes the source (Sumerian) and target (English) texts.\n",
    "    \"\"\"\n",
    "    inputs = examples['source']\n",
    "    targets = examples['target']\n",
    "\n",
    "    # DEBUG: Print flag if any of inputs or targets are none\n",
    "    if any(x is None for x in inputs) or any(x is None for x in targets):\n",
    "        print(\"Warning: Found None values in inputs or targets. This may affect training.\")\n",
    "\n",
    "\n",
    "    model_inputs = tokenizer(inputs, max_length=MAX_INPUT_LENGTH, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    # Tokenize targets (English) using the newer approach\n",
    "    labels = tokenizer(text_target=targets, max_length=MAX_TARGET_LENGTH, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Convert lists to Hugging Face Dataset objects\n",
    "train_dataset = HFDataset.from_list(train_data)\n",
    "val_dataset = HFDataset.from_list(val_data)\n",
    "\n",
    "# Apply preprocessing to the datasets\n",
    "print(\"Tokenizing datasets...\")\n",
    "tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_val_dataset = val_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc354f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of tokenized input:\n",
      "{'source': ' 1(u) la₂ 1(diš) udu u₄ 2(u) 8(diš)-kam ki ab-ba-sa₆-ga-ta na-lu₅ i₃-dab₅   iti <unk> bi₂-gu₇ mu en-unu₆-gal {d}inana unu{ki}ga ba-hun  1(u) la₂ 1(diš)', 'target': '9 rams, 28th day, from Abba-saga, Nalu accepted; month: “ubi-feast,” year: “Enunugal of Inanna of Uruk was installed;” (total:) 9 (rams).', 'input_ids': [0, 112, 1640, 257, 43, 897, 24987, 9264, 9264, 112, 1640, 7506, 4654, 43, 1717, 6588, 1717, 24987, 9264, 11936, 132, 1640, 257, 43, 290, 1640, 7506, 4654, 19281, 330, 424, 27651, 4091, 12, 3178, 12, 11146, 24987, 9264, 27819, 12, 2538, 12, 4349, 2750, 12, 6487, 24987, 9264, 5782, 939, 24987, 9264, 862, 12, 417, 873, 24987, 9264, 5782, 1437, 1437, 24, 118, 1437, 3, 4003, 24987, 9264, 9264, 12, 5521, 24987, 9264, 6382, 14701, 1177, 12, 879, 257, 24987, 9264, 27819, 12, 9487, 25522, 417, 24303, 179, 1113, 542, 257, 45152, 3144, 24303, 2538, 17279, 12, 18458, 1437, 112, 1640, 257, 43, 897, 24987, 9264, 9264, 112, 1640, 7506, 4654, 43, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [0, 466, 910, 7042, 6, 971, 212, 183, 6, 31, 2060, 3178, 12, 29, 6080, 6, 234, 337, 257, 3903, 131, 353, 35, 44, 48, 36384, 12, 7068, 1988, 6, 17, 46, 76, 35, 44, 48, 16040, 879, 39029, 9, 96, 4057, 9, 121, 2070, 330, 21, 5923, 131, 17, 46, 36, 30033, 35, 43, 361, 36, 27809, 322, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(\"Example of tokenized input:\")\n",
    "print(tokenized_train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1ff64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the training arguments for the Seq2SeqTrainer\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,                  # Directory to save the model checkpoints\n",
    "    \n",
    "    num_train_epochs=NUM_TRAIN_EPOCHS,      # Number of training epochs\n",
    "    per_device_train_batch_size=BATCH_SIZE, # Batch size for training\n",
    "    per_device_eval_batch_size=BATCH_SIZE,  # Batch size for evaluation\n",
    "    \n",
    "    learning_rate=LEARNING_RATE,            # Learning rate for the optimizer\n",
    "    weight_decay=0.01,                      # Weight decay for regularization\n",
    "    warmup_ratio=0.1,                       # Warmup ratio for learning rate scheduler\n",
    "    gradient_accumulation_steps=1,          # Gradient accumulation steps to simulate larger batch sizes\n",
    "    lr_scheduler_type=\"cosine\",             # Use cosine learning rate scheduler\n",
    "    label_smoothing_factor=0.1,             # Label smoothing factor for better generalization\n",
    "\n",
    "    save_total_limit=1,                     # Only keep the last checkpoint\n",
    "    predict_with_generate=True,             # Enable generation during evaluation\n",
    "    report_to=\"tensorboard\",                # Report metrics to TensorBoard\n",
    "    logging_dir=LOGGING_DIR,                # Directory for TensorBoard logs\n",
    "    logging_steps=50,                       # Log every 50 steps\n",
    "    \n",
    "    eval_strategy=\"epoch\",                  # Evaluate at the end of each epoch\n",
    "    save_strategy=\"epoch\",                  # Save model at the end of each epoch\n",
    "    load_best_model_at_end=True,            # Load the best model at the end of training\n",
    "    metric_for_best_model=\"meteor\",         # Metric to determine the best model\n",
    "    fp16=torch.cuda.is_available(),         # Use mixed precision training if GPU is available\n",
    ")\n",
    "\n",
    "# Set up generation configuration for the model\n",
    "generation_config = GenerationConfig(\n",
    "    max_length=MAX_TARGET_LENGTH,           # Maximum length of the generated sequences\n",
    "    early_stopping=True,                    # Stop generation when all beams reach the EOS token\n",
    "    num_beams=4,                            # Number of beams for beam search\n",
    "    no_repeat_ngram_size=3,                 # Prevent repetition of n-grams in the generated text\n",
    "    forced_bos_token_id=0,                  # Force the beginning of the sequence to be the BOS token\n",
    "    pad_token_id=tokenizer.pad_token_id,    # Padding token ID for the tokenizer\n",
    "    eos_token_id=tokenizer.eos_token_id,    # End of sequence token ID for the tokenizer\n",
    "    decoder_start_token_id=tokenizer.bos_token_id if tokenizer.bos_token_id is not None else tokenizer.eos_token_id   # Decoder start token ID for the model\n",
    ")\n",
    "model.generation_config = generation_config\n",
    "\n",
    "# Set DataCllator for Seq2Seq tasks to handle padding and batching\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "# Initialize the Seq2SeqTrainer with the model, training arguments, datasets, tokenizer, data collator, and metrics computation\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=lambda p: compute_metrics(p, tokenizer),        # Function to compute metrics during evaluation\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]    # Early stopping callback to prevent overfitting\n",
    "    )\n",
    "\n",
    "# Start the training process\n",
    "print(\"Starting model training...\")\n",
    "try:\n",
    "    trainer.train()\n",
    "    print(\"Training finished successfully!\")\n",
    "\n",
    "    # Save the final model and tokenizer\n",
    "    trainer.save_model(f\"{OUTPUT_DIR}/final_model\")\n",
    "    tokenizer.save_pretrained(f\"{OUTPUT_DIR}/final_model_tokenizer\")\n",
    "    print(f\"Final model saved to {OUTPUT_DIR}/final_model\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during training: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed4824c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BartForConditionalGeneration.from_pretrained(f\"{OUTPUT_DIR}/final_model\")\n",
    "tokenizer = BartTokenizer.from_pretrained(f\"{OUTPUT_DIR}/final_model_tokenizer\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def translate_sumerian_to_english(text, trained_model, trained_tokenizer, device):\n",
    "    \"\"\"\n",
    "    Translates a Sumerian text to English using the fine-tuned model.\n",
    "\n",
    "    Args:\n",
    "        text (str): The Sumerian text to translate.\n",
    "        trained_model (BartForConditionalGeneration): The fine-tuned BART model.\n",
    "        trained_tokenizer (BartTokenizer): The tokenizer used for the model.\n",
    "        device (torch.device): The device to run the model on (CPU or GPU).\n",
    "    Returns:\n",
    "        str: The translated English text.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    trained_model.eval()\n",
    "    trained_model.to(device)\n",
    "\n",
    "    # Prepare the input text\n",
    "    inputs = trained_tokenizer(text, return_tensors=\"pt\", max_length=MAX_INPUT_LENGTH, truncation=True, padding=True)\n",
    "    input_ids = inputs.input_ids.to(device)\n",
    "    attention_mask = inputs.attention_mask.to(device)\n",
    "\n",
    "    # Generate translation\n",
    "    with torch.no_grad():   # Disable gradient calculations for inference\n",
    "        outputs = trained_model.generate(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,      # Use attention mask to ignore padding tokens\n",
    "            max_length=MAX_TARGET_LENGTH + 2,   # +2 for start/end tokens\n",
    "            num_beams=5,                        # Beam search width\n",
    "            early_stopping=True                 # Stop when all beams reach the end token\n",
    "        )\n",
    "\n",
    "    # Decode the generated ids to text\n",
    "    translated_text = trained_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return translated_text\n",
    "\n",
    "testing_data = preprocess_dataset('../datasets/SumTablets_English_test.csv')\n",
    "\n",
    "for index, row in testing_data.iterrows():\n",
    "    sumerian_text = row['sumerian']\n",
    "    english_translation = translate_sumerian_to_english(sumerian_text, model, tokenizer, device)\n",
    "    true_english_translation = row['english']\n",
    "    \n",
    "    print(f\"Sumerian: {sumerian_text}\")\n",
    "    print(f\"Predicted English: {english_translation}\")\n",
    "    print(f\"True English: {true_english_translation}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bbd62a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys  \n",
    "sys.path.insert(1, '../utils')\n",
    "\n",
    "from rclone import update_folder_on_onedrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8f56f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating 'bart_large_model' on OneDrive with 'bart_large_model'...\n",
      "rclone command: rclone sync bart_large_model onedrive_bocconi:AI-project/bart_large_model -P\n",
      "SUCCESS: Folder updated successfully.\n",
      "Local folder 'bart_large_model' has been removed after successful update.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update_folder_on_onedrive(\"bart_large_model\", \"bart_large_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azureml_py310_sdkv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
