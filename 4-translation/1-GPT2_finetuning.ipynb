{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c5bc751",
   "metadata": {},
   "source": [
    "# GPT2 Model\n",
    "\n",
    "This notebook implements a machine translation system that finetunes GPT-2 Medium to translate Sumerian cuneiform text into English. Here's a breakdown of the key components:\n",
    "\n",
    "- **Model Setup:** Initializes GPT-2 Medium and configures tokenizer settings, ensuring proper padding token handling for the model.\n",
    "- **Data Preparation:** Loads Sumerian tablets dataset, formatting each example with a clear structure.\n",
    "- **Dataset Processing:**\n",
    "  - Filters out overly long sequences (>528 words)\n",
    "  - Implements custom SumerianEnglishDataset class with appropriate tokenization\n",
    "  - Splits data into training and validation sets\n",
    "- **Training Configuration:**\n",
    "  - Uses DataCollatorForLanguageModeling for causal language modeling\n",
    "  - Implements gradient checkpointing and mixed precision for memory efficiency\n",
    "  - Configures output directories for model checkpoints and logs\n",
    "- **Evaluation Metrics:** Implements custom evaluation with BLEU, METEOR, and ROUGE scores to measure translation quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a73d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from transformers import (\n",
    "    GPT2Tokenizer,\n",
    "    GPT2LMHeadModel,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129d7e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'gpt2-medium'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03d33c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modello caricato su: cuda\n",
      "Generazione del testo...\n",
      "\n",
      "--- Testo Generato 1 ---\n",
      "Once upon a time, in a land far, far away, there lived a wise man named Arathorn the Wise. A king of many faces and great stature with long hair on his head, he was known to everyone as \"the Great Sage.\"\n",
      "From that day forth, all who knew him were called gods by those around them. Even when they grew tired from their labors or lost power for any reason at all...they still kept up appearances while taking care not look like anything else! That's why you always see so much people wearing these masks today!\"\n",
      "\"And then what happened?\" I asked her gently. The voice sounded deep within my mind but it wasn't very clear…did she know? She must have felt something\n"
     ]
    }
   ],
   "source": [
    "# Test model is working\n",
    "prompt_text = \"Once upon a time, in a land far, far away,\"\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "input_ids = tokenizer.encode(prompt_text, return_tensors='pt').to(device)\n",
    "\n",
    "print(\"Trying generating text...\")\n",
    "try:\n",
    "    output_sequences = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=150,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        repetition_penalty=1.1,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        attention_mask=torch.ones_like(input_ids)  # Add explicit attention mask\n",
    "    )\n",
    "\n",
    "    for i, generated_sequence in enumerate(output_sequences):\n",
    "        text = tokenizer.decode(generated_sequence, skip_special_tokens=True)\n",
    "        print(f\"\\n--- Generated text {i+1} ---\")\n",
    "        print(text)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error when generating text: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adba3500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1905 formatted examples.\n",
      "[45, 51, 47, 39, 55, 73, 148, 23, 49, 72, 52, 45, 125, 82, 78, 223, 96, 57, 58, 49, 48, 55, 38, 52, 107, 97, 31, 27, 24, 48, 75, 116, 121, 39, 51, 15, 41, 65, 79, 52, 176, 46, 39, 41, 36, 35, 120, 184, 77, 63, 38, 106, 40, 28, 17, 42, 21, 47, 124, 224, 90, 92, 87, 57, 262, 66, 67, 63, 57, 31, 45, 51, 72, 44, 49, 78, 52, 78, 79, 138, 49, 102, 847, 630, 490, 368, 409, 77, 410, 264, 696, 184, 62, 338, 454, 1560, 9, 9, 11, 10, 12, 13, 16, 11, 14, 12, 11, 9, 9, 7, 11, 11, 16, 9, 22, 12, 12, 26, 30, 9, 16, 12, 11, 11, 11, 7, 9, 8, 11, 13, 20, 25, 11, 11, 27, 11, 11, 11, 12, 9, 22, 9, 24, 6, 11, 15, 31, 32, 31, 30, 30, 26, 33, 41, 37, 31, 35, 31, 38, 26, 29, 26, 17, 23, 27, 36, 39, 41, 35, 37, 25, 29, 31, 31, 32, 30, 32, 26, 33, 41, 33, 31, 31, 30, 29, 32, 32, 37, 29, 32, 32, 28, 32, 33, 31, 28, 26, 32, 32, 32, 32, 33, 30, 33, 32, 32, 28, 28, 39, 41, 39, 37, 31, 35, 31, 36, 31, 36, 31, 35, 34, 36, 34, 36, 39, 36, 36, 43, 34, 39, 36, 36, 36, 39, 36, 36, 39, 36, 39, 33, 36, 39, 36, 34, 34, 18, 29, 37, 38, 46, 34, 37, 40, 37, 38, 39, 37, 33, 86, 40, 29, 46, 31, 12, 27, 38, 35, 9, 9, 11, 11, 37, 31, 29, 31, 32, 35, 13, 75, 50, 49, 13, 48, 153, 53, 60, 313, 75, 45, 168, 104, 98, 83, 38, 25, 15, 30, 56, 74, 82, 89, 55, 44, 100, 376, 25, 35, 35, 78, 76, 37, 87, 41, 45, 52, 75, 45, 199, 41, 81, 105, 19, 169, 59, 62, 67, 73, 137, 67, 120, 116, 84, 107, 172, 82, 167, 72, 58, 172, 202, 107, 68, 279, 48, 82, 49, 56, 47, 82, 49, 43, 75, 41, 41, 57, 78, 64, 66, 34, 38, 62, 65, 95, 40, 51, 563, 40, 65, 32, 118, 346, 33, 154, 123, 131, 92, 63, 34, 29, 63, 33, 273, 41, 50, 17, 56, 57, 68, 31, 63, 398, 159, 61, 111, 202, 199, 205, 484, 436, 77, 32, 55, 78, 90, 21, 88, 37, 19, 22, 28, 51, 38, 76, 20, 18, 31, 35, 76, 77, 53, 55, 165, 57, 42, 50, 173, 66, 50, 47, 56, 75, 49, 45, 144, 37, 335, 9, 17, 22, 86, 21, 48, 28, 37, 10, 62, 110, 25, 194, 106, 17, 68, 34, 53, 83, 31, 29, 44, 23, 19, 72, 21, 16, 19, 19, 75, 240, 24, 65, 59, 70, 62, 75, 32, 11, 71, 78, 87, 62, 19, 17, 29, 60, 34, 18, 64, 27, 23, 173, 97, 73, 277, 438, 521, 693, 945, 49, 76, 40, 114, 235, 119, 111, 80, 47, 57, 38, 31, 118, 15, 62, 49, 34, 29, 33, 10, 41, 22, 21, 30, 29, 23, 40, 52, 25, 20, 16, 19, 19, 23, 32, 25, 14, 8, 26, 24, 23, 17, 29, 24, 26, 25, 28, 31, 20, 46, 41, 31, 18, 17, 55, 40, 72, 27, 44, 35, 25, 37, 49, 34, 41, 45, 38, 47, 29, 19, 21, 33, 31, 30, 43, 32, 27, 30, 51, 116, 17, 16, 19, 33, 23, 39, 107, 14, 58, 35, 47, 71, 73, 119, 139, 238, 60, 38, 41, 64, 27, 88, 70, 85, 82, 53, 46, 253, 50, 78, 1638, 178, 317, 172, 57, 45, 105, 364, 411, 36, 81, 447, 925, 341, 100, 42, 132, 435, 186, 889, 838, 138, 324, 1316, 1228, 443, 1139, 1109, 1117, 938, 1238, 583, 231, 225, 94, 527, 118, 173, 370, 1265, 86, 387, 396, 136, 745, 448, 90, 209, 365, 252, 381, 329, 504, 362, 352, 101, 98, 203, 188, 1230, 1221, 332, 332, 119, 623, 1071, 196, 25, 259, 281, 334, 53, 59, 153, 53, 50, 351, 426, 65, 614, 52, 178, 74, 130, 53, 1456, 273, 287, 19, 28, 27, 27, 35, 120, 38, 21, 61, 31, 34, 120, 14, 20, 19, 27, 12, 17, 19, 24, 26, 22, 17, 32, 65, 29, 21, 495, 220, 27, 11, 13, 31, 47, 31, 20, 36, 10, 30, 36, 29, 32, 34, 31, 36, 36, 19, 38, 40, 29, 31, 33, 18, 36, 31, 38, 23, 22, 32, 36, 36, 16, 35, 31, 36, 32, 31, 34, 36, 32, 36, 29, 32, 31, 32, 36, 26, 36, 24, 33, 31, 33, 31, 32, 36, 11, 36, 33, 38, 32, 26, 39, 38, 36, 36, 32, 36, 36, 36, 32, 31, 34, 93, 40, 39, 36, 33, 36, 37, 33, 32, 39, 36, 26, 115, 119, 113, 39, 72, 122, 100, 54, 60, 174, 270, 97, 28, 37, 58, 167, 55, 79, 34, 87, 58, 94, 44, 60, 5354, 3088, 1008, 95, 46, 146, 153, 128, 85, 99, 88, 120, 266, 16, 45, 41, 234, 113, 158, 32, 31, 37, 537, 1416, 43, 57, 66, 49, 40, 42, 30, 36, 36, 17, 11, 14, 16, 24, 15, 8, 12, 9, 15, 17, 12, 9, 16, 14, 20, 14, 332, 64, 34, 115, 166, 45, 61, 91, 46, 18, 26, 18, 39, 24, 36, 207, 37, 67, 47, 34, 21, 20, 37, 14, 19, 15, 16, 9, 17, 21, 22, 20, 31, 15, 23, 20, 26, 54, 15, 25, 25, 73, 17, 34, 54, 58, 46, 23, 29, 45, 42, 43, 55, 22, 114, 22, 23, 64, 42, 429, 26, 52, 11, 33, 105, 436, 214, 196, 217, 178, 186, 279, 277, 125, 218, 205, 142, 97, 248, 572, 337, 186, 122, 273, 249, 249, 82, 28, 1101, 45, 85, 74, 173, 184, 49, 55, 115, 291, 503, 292, 114, 444, 213, 210, 42, 67, 151, 871, 144, 57, 157, 259, 156, 168, 99, 78, 380, 43, 49, 182, 37, 64, 36, 41, 81, 37, 43, 37, 41, 41, 61, 32, 39, 34, 32, 34, 33, 34, 37, 34, 52, 33, 52, 56, 49, 52, 142, 113, 31, 129, 94, 80, 55, 51, 55, 79, 67, 199, 128, 252, 51, 37, 284, 117, 98, 48, 34, 157, 211, 33, 605, 299, 123, 163, 106, 285, 94, 80, 56, 68, 63, 27, 63, 73, 123, 51, 26, 53, 37, 119, 44, 33, 41, 43, 52, 46, 46, 31, 39, 49, 51, 27, 25, 12, 27, 25, 25, 19, 19, 33, 40, 59, 52, 18, 206, 17, 28, 78, 22, 23, 42, 62, 19, 26, 33, 38, 16, 18, 27, 57, 37, 41, 43, 32, 27, 28, 21, 31, 25, 31, 26, 51, 28, 99, 36, 18, 56, 38, 28, 45, 30, 29, 20, 27, 35, 46, 47, 43, 36, 45, 29, 76, 28, 22, 64, 16, 48, 27, 62, 22, 73, 24, 58, 22, 61, 57, 33, 48, 71, 34, 208, 33, 50, 44, 54, 27, 27, 30, 20, 9, 7, 4, 10, 65, 37, 68, 42, 19, 35, 2299, 27, 121, 58, 51, 270, 54, 113, 79, 66, 178, 15, 29, 59, 82, 50, 58, 41, 46, 26, 40, 34, 55, 26, 63, 97, 222, 36, 263, 459, 109, 39, 24, 51, 12, 63, 40, 23, 37, 54, 66, 39, 40, 933, 37, 45, 46, 41, 36, 11, 38, 507, 30, 1079, 341, 89, 264, 98, 59, 220, 129, 67, 53, 96, 21, 26, 43, 38, 52, 16, 22, 51, 33, 31, 14, 39, 21, 25, 21, 23, 32, 27, 19, 32, 57, 41, 22, 29, 35, 20, 21, 37, 19, 70, 18, 24, 19, 23, 18, 17, 30, 19, 26, 48, 24, 47, 26, 20, 40, 18, 12, 60, 18, 26, 29, 38, 39, 24, 40, 46, 28, 43, 30, 18, 117, 120, 27, 45, 32, 21, 51, 18, 16, 112, 258, 249, 52, 66, 93, 22, 20, 19, 34, 14, 157, 39, 53, 41, 30, 57, 1028, 55, 112, 20, 28, 75, 16, 33, 33, 64, 22, 32, 29, 88, 27, 29, 21, 139, 145, 63, 486, 142, 136, 29, 30, 27, 23, 18, 18, 203, 93, 25, 43, 20, 78, 19, 22, 41, 16, 61, 50, 26, 40, 16, 36, 46, 51, 66, 108, 43, 58, 9, 41, 43, 39, 61, 66, 59, 69, 1121, 30, 85, 25, 49, 18, 21, 19, 19, 30, 23, 50, 12, 14, 36, 28, 48, 87, 52, 69, 54, 90, 67, 27, 41, 194, 620, 800, 707, 14, 28, 20, 21, 21, 61, 36, 20, 18, 23, 19, 19, 26, 23, 20, 28, 46, 24, 27, 27, 20, 38, 18, 23, 33, 23, 28, 18, 28, 163, 154, 102, 34, 38, 33, 71, 26, 61, 282, 66, 78, 39, 62, 164, 57, 56, 550, 54, 154, 258, 8028, 54, 88, 135, 1312, 843, 31, 20, 47, 138, 33, 60, 56, 22, 22, 36, 50, 48, 26, 22, 29, 56, 60, 64, 186, 210, 112, 257, 118, 242, 162, 253, 217, 270, 290, 171, 185, 65, 1363, 51, 56, 77, 107, 57, 67, 22, 40, 23, 21, 25, 24, 43, 49, 28, 48, 41, 23, 42, 38, 39, 21, 35, 39, 39, 31, 32, 60, 15, 40, 48, 63, 38, 53, 43, 34, 16, 46, 38, 27, 67, 20, 32, 42, 30, 21, 24, 31, 43, 30, 36, 16, 36, 178, 585, 361, 395, 29, 30, 23, 22, 22, 22, 39, 22, 13, 9, 14, 14, 27, 9, 15, 7, 14, 38, 28, 17, 22, 36, 47, 60, 55, 80, 51, 57, 63, 71, 105, 51, 87, 70, 817, 185, 46, 154, 32, 51, 84, 32, 67, 41, 85, 49, 55, 47, 62, 53, 63, 112, 51, 56, 57, 103, 25, 54, 140, 40, 97, 68, 10, 235, 54, 53, 39, 29, 40, 106, 35, 24, 39, 52, 53, 63, 64, 41, 41, 18, 19, 17, 59, 73, 58, 24, 9, 443, 28, 43, 80, 122, 34, 220, 152, 21, 19, 21, 103, 752, 102, 480, 752, 45, 49, 30, 33, 38, 19, 38, 19, 17, 19, 24, 23, 22, 14, 20, 31, 23, 62, 12, 28, 17, 22, 58, 21, 22, 145, 29, 41, 1212, 71, 32, 53, 143, 18, 28, 14, 25, 23, 18, 13, 64, 11, 18, 10, 1298, 25, 112, 16, 104, 58, 25, 11, 11, 28, 33, 22, 40, 39, 30, 19, 140, 24, 22, 36, 74, 89, 140, 55, 93, 155, 85, 37, 105, 23, 15, 40, 22, 29, 47, 36, 1634, 78, 24, 40, 19, 66, 17, 51, 21, 32, 55, 102, 53, 74, 39, 46, 65, 51, 54, 58, 62, 119, 102, 45, 79, 134, 49, 40, 47, 79, 46, 67, 74, 51, 73, 22, 20, 23, 24, 24, 24, 24, 25, 25, 25, 25, 24, 25, 28, 23, 31, 188, 14, 30, 29, 37, 23, 40, 33, 18, 16, 42, 36, 34, 64, 19, 20, 62, 69, 19, 33, 14, 24, 196, 49, 21, 28, 23, 1080, 514, 42, 26, 39, 24, 52, 85, 48, 20, 20, 20, 177, 20, 15, 27, 17, 59, 115, 133, 86, 41, 96, 10, 375, 18, 14, 26, 17, 19, 17, 20, 273, 21, 17, 24, 16, 13, 22, 50, 64, 32, 23, 36, 17, 927, 41, 6, 30, 198, 42, 24, 39, 33, 90, 21, 28, 39, 20, 51, 30, 17, 11, 17, 36, 183, 63, 15, 17, 184, 9, 19, 17, 162, 49, 88, 54, 230, 179]\n",
      "Mean length of the texts: 103.45669291338582 words\n",
      "Percentage of texts longer than 528 words: 3.04%\n",
      "1847 texts after filtering by length.\n",
      "\n",
      "Example formatted text:\n",
      "Sumerian:  1(u) la₂ 1(diš) udu u₄ 2(u) 8(diš)-kam ki ab-ba-sa₆-ga-ta na-lu₅ i₃-dab₅   iti <unk> bi₂-gu₇ mu en-unu₆-gal {d}inana unu{ki}ga ba-hun  1(u) la₂ 1(diš)\n",
      "English: 9 rams, 28th day, from Abba-saga, Nalu accepted; month: “ubi-feast,” year: “Enunugal of Inanna of Uruk was installed;” (total:) 9 (rams).<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv('datasets/SumTablets_English_train.csv')\n",
    "test_data = pd.read_csv('datasets/SumTablets_English_train.csv')\n",
    "\n",
    "# Format the data for GPT-2:\n",
    "# We'll combine Sumerian and English with a separator.\n",
    "# GPT-2 will learn to generate the English part after seeing \"English: \".\n",
    "# The <|endoftext|> token is GPT-2's standard end-of-sequence token.\n",
    "formatted_texts = []\n",
    "for index, row in train_data.iterrows():\n",
    "    sumerian_texts = row['transliteration']\n",
    "    english_translations = row['translation']\n",
    "    if isinstance(sumerian_texts, str) and isinstance(english_translations, str):\n",
    "        sumerian_texts = sumerian_texts.replace('\\n', ' ')\n",
    "        english_translations = english_translations.replace('\\n', ' ')\n",
    "        formatted_texts.append(f\"Sumerian: {sumerian_texts}\\nEnglish: {english_translations}<|endoftext|>\")\n",
    "print(f\"Loaded {len(formatted_texts)} formatted examples.\")\n",
    "\n",
    "lengths = [len(text.split()) for text in formatted_texts]\n",
    "print(lengths)\n",
    "mean_length = np.mean(lengths)\n",
    "print(f\"Mean length of the texts: {mean_length} words\")\n",
    "print(f\"Percentage of texts longer than 528 words: {sum(length > 528 for length in lengths) / len(lengths) * 100:.2f}%\")\n",
    "\n",
    "# remove texts longer than 528 words\n",
    "formatted_texts = [text for text in formatted_texts if len(text.split()) <= 528]\n",
    "print(len(formatted_texts), \"texts after filtering by length.\")\n",
    "\n",
    "print(f\"\\nExample formatted text:\\n{formatted_texts[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170cf36a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split dataset into 1662 training samples and 185 validation samples.\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'gpt2-medium'\n",
    "OUTPUT_DIR = './sumerian_gpt2_finetuned'    # Directory to save the fine-tuned model\n",
    "LOG_DIR = './sumerian_gpt2_finetuned_logs'  # Directory for training logs\n",
    "\n",
    "# Create output and log directories if they don't exist\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "# Training hyperparameters (adjust these based on your dataset size and resources)\n",
    "NUM_EPOCHS = 5                         # Number of training epochs\n",
    "LEARNING_RATE = 3e-5                   # Learning rate\n",
    "WARMUP_RATIO = 0.1                     # Number of warmup steps for learning rate scheduler\n",
    "WEIGHT_DECAY = 0.01                    # Weight decay\n",
    "MAX_LENGTH = 528                       # Maximum sequence length for tokenizer\n",
    "TRAIN_VALID_SPLIT = 0.1                # Proportion of data to use for validation\n",
    "\n",
    "\n",
    "class SumerianEnglishDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = texts\n",
    "        self.max_length = max_length\n",
    "        self.encodings = []\n",
    "        for text in texts:\n",
    "            \n",
    "            # Tokenize the combined text\n",
    "            # truncation=True ensures that sequences longer than max_length are cut.\n",
    "            # padding='max_length' pads shorter sequences to max_length.\n",
    "            # return_tensors='pt' returns PyTorch tensors.\n",
    "            encoding = self.tokenizer(\n",
    "                text,\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,     # Truncate to max_length\n",
    "                padding=\"max_length\",           # Ensure all sequences have the same length for batching\n",
    "                return_attention_mask=True,     # Return attention masks\n",
    "                return_tensors='pt'             # Explicitly specify to return PyTorch tensors\n",
    "            )\n",
    "            \n",
    "            # For language modeling, the 'labels' are typically the same as 'input_ids'.\n",
    "            self.encodings.append({\n",
    "                \"input_ids\": encoding[\"input_ids\"].squeeze(),\n",
    "                \"attention_mask\": encoding[\"attention_mask\"].squeeze()\n",
    "            })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.encodings[idx]\n",
    "        # The labels are the input_ids and the model is trained to predict the next token in the sequence.\n",
    "        # The DataCollatorForLanguageModeling will shift them appropriately.\n",
    "        return {\"input_ids\": item[\"input_ids\"], \"attention_mask\": item[\"attention_mask\"], \"labels\": item[\"input_ids\"].clone()}\n",
    "\n",
    "# Create the full dataset\n",
    "full_dataset = SumerianEnglishDataset(formatted_texts, tokenizer, MAX_LENGTH)\n",
    "\n",
    "# Split into training and validation sets\n",
    "if TRAIN_VALID_SPLIT > 0:\n",
    "    num_train = int((1 - TRAIN_VALID_SPLIT) * len(full_dataset))\n",
    "    num_valid = len(full_dataset) - num_train\n",
    "    train_dataset, eval_dataset = random_split(full_dataset, [num_train, num_valid])\n",
    "    print(f\"Split dataset into {len(train_dataset)} training samples and {len(eval_dataset)} validation samples.\")\n",
    "else:\n",
    "    train_dataset = full_dataset\n",
    "    eval_dataset = None     # No validation\n",
    "    print(f\"Using all {len(train_dataset)} samples for training. No validation set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac23da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/default/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/default/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/default/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Shere are some debug messages in italian which where useful during development\n",
    "# Sorry for that\n",
    "\n",
    "from evaluate import load\n",
    "import numpy as np\n",
    "\n",
    "# load the evaluation metrics\n",
    "bleu_metric = load(\"bleu\")\n",
    "meteor_metric = load(\"meteor\")\n",
    "rouge_metric = load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "\n",
    "    print(f\"DEBUG: Tipo iniziale di preds: {type(preds)}\")\n",
    "    if hasattr(preds, 'shape'):\n",
    "        print(f\"DEBUG: Shape iniziale di preds: {preds.shape}\")\n",
    "    elif isinstance(preds, (list, tuple)):\n",
    "        print(f\"DEBUG: Lunghezza iniziale di preds: {len(preds)}\")\n",
    "\n",
    "    actual_token_ids = preds\n",
    "\n",
    "    # Case 1: If preds is a tuple, it might contain logits or actual token IDs.\n",
    "    if isinstance(preds, tuple):\n",
    "        print(\"DEBUG: preds è una tupla, prendo il primo elemento.\")\n",
    "        actual_token_ids = preds[0]\n",
    "\n",
    "    print(f\"DEBUG: Tipo di actual_token_ids dopo il check della tupla: {type(actual_token_ids)}\")\n",
    "    if hasattr(actual_token_ids, 'shape'):\n",
    "        print(f\"DEBUG: Shape di actual_token_ids dopo il check della tupla: {actual_token_ids.shape}\")\n",
    "\n",
    "\n",
    "    # Case 2: If preds is a numpy array or PyTorch tensor, we need to check its shape.\n",
    "    if isinstance(actual_token_ids, (np.ndarray, torch.Tensor)) and actual_token_ids.ndim == 3:\n",
    "        print(\"DEBUG: actual_token_ids sembrano logits, applico argmax.\")\n",
    "        if isinstance(actual_token_ids, torch.Tensor):\n",
    "            actual_token_ids = actual_token_ids.cpu().numpy()\n",
    "        actual_token_ids = np.argmax(actual_token_ids, axis=-1)\n",
    "        print(f\"DEBUG: Shape di actual_token_ids dopo argmax: {actual_token_ids.shape}\")\n",
    "\n",
    "    # Substitute -100 labels with pad_token_id\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "\n",
    "    try:\n",
    "        decoded_preds = tokenizer.batch_decode(actual_token_ids, skip_special_tokens=True)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    except Exception as e:\n",
    "        print(f\"ERRORE durante tokenizer.batch_decode:\")\n",
    "        print(f\"  Tipo di actual_token_ids: {type(actual_token_ids)}\")\n",
    "        if hasattr(actual_token_ids, 'shape'): print(f\"  Shape di actual_token_ids: {actual_token_ids.shape}\")\n",
    "        if hasattr(actual_token_ids, 'dtype'): print(f\"  Dtype di actual_token_ids: {actual_token_ids.dtype}\")\n",
    "        print(f\"  Esempio di un elemento in actual_token_ids (se lista/array): {actual_token_ids[0] if len(actual_token_ids)>0 else 'N/A'}\")\n",
    "        raise e\n",
    "\n",
    "\n",
    "    cleaned_preds = [pred.split(\"English:\")[-1].replace(\"<|endoftext|>\", \"\").strip() if \"English:\" in pred else pred.replace(\"<|endoftext|>\", \"\").strip() for pred in decoded_preds]\n",
    "    cleaned_labels = [label.split(\"English:\")[-1].replace(\"<|endoftext|>\", \"\").strip() if \"English:\" in label else label.replace(\"<|endoftext|>\", \"\").strip() for label in decoded_labels]\n",
    "\n",
    "    list_of_lists_labels = [[label] for label in cleaned_labels]\n",
    "    results = {}\n",
    "\n",
    "    try:\n",
    "        bleu_score_dict = bleu_metric.compute(predictions=cleaned_preds, references=list_of_lists_labels)\n",
    "        results[\"bleu\"] = bleu_score_dict.get(\"score\", bleu_score_dict.get(\"bleu\", 0.0))\n",
    "\n",
    "        meteor_score_dict = meteor_metric.compute(predictions=cleaned_preds, references=cleaned_labels)\n",
    "        results[\"meteor\"] = meteor_score_dict[\"meteor\"]\n",
    "\n",
    "        rouge_score_dict = rouge_metric.compute(predictions=cleaned_preds, references=cleaned_labels)\n",
    "        results[\"rougeL\"] = rouge_score_dict.get(\"rougeLsum\", rouge_score_dict.get(\"rougeL\", 0.0)) \n",
    "    except Exception as e:\n",
    "        print(f\"AVVISO: Errore nel calcolo di una metrica: {e}\")\n",
    "        print(f\"  cleaned_preds (primi 2): {cleaned_preds[:2]}\")\n",
    "        print(f\"  list_of_lists_labels (primi 2): {list_of_lists_labels[:2]}\")\n",
    "        # Imposta valori di default se il calcolo fallisce per non bloccare tutto\n",
    "        results[\"bleu\"] = results.get(\"bleu\", 0.0)\n",
    "        results[\"meteor\"] = results.get(\"meteor\", 0.0)\n",
    "        results[\"rougeL\"] = results.get(\"rougeL\", 0.0)\n",
    "\n",
    "    try:\n",
    "        prediction_lens = [len(tokenizer.encode(p, add_special_tokens=False)) for p in cleaned_preds]\n",
    "        results[\"gen_len\"] = np.mean(prediction_lens) if prediction_lens else 0.0\n",
    "    except Exception as e:\n",
    "        print(f\"AVVISO: Errore nel calcolo di gen_len: {e}\")\n",
    "        results[\"gen_len\"] = 0.0\n",
    "\n",
    "    return {k: round(v, 4) if isinstance(v, float) else v for k, v in results.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e5e295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the pad_token_id in the model configuration (important for generation and padding)\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "print(f\"Set model.config.pad_token_id to {tokenizer.pad_token_id}\")\n",
    "\n",
    "# The DataCollatorForLanguageModeling will automatically create batches and\n",
    "# shift the input_ids to create labels for causal language modeling (predicting the next token).\n",
    "# It also handles padding. `mlm=False` means we are doing Causal Language Modeling (CLM), not Masked Language Modeling (MLM).\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # Causal Language Modeling for GPT-2\n",
    ")\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    num_train_epochs=NUM_EPOCHS,                        # Total number of training epochs\n",
    "    per_device_train_batch_size=2,                      # Batch size per device during training\n",
    "    per_device_eval_batch_size=2,                       # Batch size for evaluation\n",
    "    eval_accumulation_steps=4                           # Number of steps to accumulate for evaluation (to save memory)\n",
    "    warmup_ratio=WARMUP_RATIO,                          # Warmup ratio for learning rate scheduler\n",
    "    weight_decay=WEIGHT_DECAY,                          # Strength of weight decay\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    gradient_checkpointing=True,                        # Enable gradient checkpointing to save memory\n",
    "\n",
    "    output_dir=OUTPUT_DIR,                              # Directory to save model checkpoints and outputs\n",
    "    logging_dir=LOG_DIR,                                # Directory for storing logs\n",
    "    \n",
    "    eval_strategy=\"epoch\" if eval_dataset else \"no\",    # Evaluate at the end of each epoch if eval_dataset exists\n",
    "    save_strategy=\"epoch\",                              # Save a checkpoint at the end of each epoch\n",
    "    \n",
    "    load_best_model_at_end=True if eval_dataset else False, # Load the best model found during training (based on eval loss)\n",
    "    metric_for_best_model=\"bleu\" if eval_dataset else None, # Metric to use for determining the best model\n",
    "    greater_is_better=True if eval_dataset else None,   # Whether a higher metric is better (for BLEU, it is)\n",
    "    fp16=torch.cuda.is_available(),                     # Use 16-bit (mixed) precision training if a GPU is available\n",
    "    report_to=\"tensorboard\",                            # Report metrics to TensorBoard\n",
    "    save_total_limit=2,                                 # Limit the total amount of checkpoints. Deletes the older checkpoints.\n",
    "    \n",
    "    gradient_accumulation_steps=2,                      # Gradient accumulation steps (if you want to simulate larger batch sizes)\n",
    "    lr_scheduler_type=\"linear\",                         # Learning rate scheduler type\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,  # Function to compute metrics during evaluation\n",
    ")\n",
    "\n",
    "print(\"Starting fine-tuning...\")\n",
    "try:\n",
    "    trainer.train()\n",
    "    print(\"Fine-tuning completed.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during training: {e}\")\n",
    "    raise e\n",
    "\n",
    "print(f\"Saving model to {OUTPUT_DIR}\")\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(f\"Model and tokenizer saved to {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed135c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Inference Example ---\n",
      "Prompt for generation: 'Sumerian: dingir inana za-me-en English:'\n",
      "Sumerian Input: dingir inana za-me-en\n",
      "Generated English: Dingira, beloved one of Zabala. Šulgi praised(?) him. ... year: “... .” Amar al-ŋu₁₀ was king. Zanin 1(banše). SIGMA ARAD(-Amar)-kiel 3/4 sila3 (silver): Dabin 2(barig) 4(ban2)... barzagal gur ki ur{d}nin <unk> ta  mu us₂-sa bad₃ mar-tu ba-du₈\n",
      "English text on the tablet reciting praise for Dingira; month : \"Dingira Festival\" , Year following that which destroyed Mar-tud and Babylonia The silver is credited to Ur-Ninkaya instead of Manzil it has been confirmed by means out of Karakalla Temple administrator Zubida received into his temple seal a copy of this document from Uzzah Inanna hereby gives her approval as wife of Adad At length he swore an oath not even touching anything with his mouth He did not swear before anyone like That man who does not know justice may be struck off From among men there are no rivals! After having sworn thus much indeed Lugaland went away greatly rejoiced Amongst them all they say\"(?). Zurita says these words because those people do what should never occur To tell lies or make up their own truth They have made known nothing apart themselves And when something good happens towards them It makes its enemy rejoice Stronger than any wall Its strength surpasses heaven itself Like a mighty bull coming down upon battle field A strong wind carries against you afar Casting terror onto your city Defeating troops advancing Towards Belgrade? Then turn back Yours faithfully will be done By my royal name I shall raise my voice once more My lord speak openly!\". When Enlil saw how badly Luzard had fallen behind His eyes were opened For Lulli stood firm Utilising both arms raising high above himself Conveyance given over pacification Offerings offerings at regular intervals Great shout resounded through the land Saying ?To Bazganthas word we give our life again' Big barge brought Elamite kingship forward Ending long Gilgamesh Saga 9 months elapsed between each day's extra days Shu-Suen son usurper Intimidating brother With treachery gone past Come after me If Ilum adhered steadfastly Towards hostility toward Azagaraza Will destroy yourself if you don't change tactics Within earshot\n",
      "\n",
      "Script finished.\n"
     ]
    }
   ],
   "source": [
    "# Inference Example (How to use the fine-tuned model)\n",
    "print(\"\\n--- Inference Example ---\")\n",
    "\n",
    "OUTPUT_DIR = 'sumerian_gpt2_finetuned'  # Directory where the fine-tuned model is saved\n",
    "MAX_LENGTH = 528  # Maximum length for generation, adjust as needed\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "fine_tuned_model = GPT2LMHeadModel.from_pretrained(OUTPUT_DIR)\n",
    "fine_tuned_tokenizer = GPT2Tokenizer.from_pretrained(OUTPUT_DIR)\n",
    "\n",
    "# Ensure the pad token is set for the loaded tokenizer (it should be saved, but good to double check)\n",
    "if fine_tuned_tokenizer.pad_token is None:\n",
    "    fine_tuned_tokenizer.pad_token = fine_tuned_tokenizer.eos_token\n",
    "    fine_tuned_model.config.pad_token_id = fine_tuned_tokenizer.eos_token_id\n",
    "\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "fine_tuned_model.to(device)\n",
    "fine_tuned_model.eval()\n",
    "\n",
    "# Example Sumerian transliteration to translate\n",
    "sumerian_prompt = \"dingir inana za-me-en\" # \"Goddess Inana, you are\"\n",
    "\n",
    "# Format the prompt exactly as done during training, up to the point where generation should start\n",
    "prompt_for_generation = f\"Sumerian: {sumerian_prompt.strip()} English:\"\n",
    "print(f\"Prompt for generation: '{prompt_for_generation}'\")\n",
    "\n",
    "# Tokenize the prompt\n",
    "input_ids = fine_tuned_tokenizer.encode(prompt_for_generation, return_tensors='pt').to(device)\n",
    "\n",
    "# Generate text\n",
    "# Adjust generation parameters as needed\n",
    "# max_new_tokens is often preferred over max_length for more control over the generated part\n",
    "# For this example, we'll use max_length relative to the prompt.\n",
    "output_sequences = fine_tuned_model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_length=MAX_LENGTH, # Max length of prompt + generated text\n",
    "    # max_new_tokens=50, # Alternative: specify only the number of new tokens to generate\n",
    "    temperature=0.7,          # Controls randomness. Lower is more deterministic.\n",
    "    top_k=50,                 # Considers the top K most probable tokens at each step.\n",
    "    top_p=0.95,               # Nucleus sampling: considers tokens with cumulative probability >= P.\n",
    "    repetition_penalty=1.2,   # Penalizes repetition.\n",
    "    num_return_sequences=1,   # Number of different sequences to generate.\n",
    "    pad_token_id=fine_tuned_tokenizer.eos_token_id # Crucial for generation\n",
    ")\n",
    "\n",
    "# Decode and print the generated text\n",
    "for generated_sequence in output_sequences:\n",
    "    full_text = fine_tuned_tokenizer.decode(generated_sequence, skip_special_tokens=False) # Keep special tokens initially for inspection\n",
    "    # Extract only the generated English part\n",
    "    # This depends on your prompt format. We look for text after \"English: \"\n",
    "    generated_english = full_text.split(prompt_for_generation)[-1]\n",
    "    # Remove the <|endoftext|> token if present at the end\n",
    "    generated_english = generated_english.replace(fine_tuned_tokenizer.eos_token, \"\").strip()\n",
    "\n",
    "    print(f\"Sumerian Input: {sumerian_prompt}\")\n",
    "    print(f\"Generated English: {generated_english}\")\n",
    "\n",
    "print(\"\\nScript finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4bb9adc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generazione del testo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testo di input: Sumerian:  1(u) la₂ 1(diš) udu u₄ 2(u) 8(diš)-kam ki ab-ba-sa₆-ga-ta na-lu₅ i₃-dab₅   iti <unk> bi₂-gu₇ mu en-unu₆-gal {d}inana unu{ki}ga ba-hun  1(u) la₂ 1(diš) \n",
      "English:\n",
      "Traduzione effettiva: 9 rams, 28th day, from Abba-saga, Nalu accepted; month: “ubi-feast,” year: “Enunugal of Inanna of Uruk was installed;” (total:) 9 (rams).\n",
      "Testo Generato: Sumerian:  1(u) la₂ 1(diš) udu u₄ 2(u) 8(diš)-kam ki ab-ba-sa₆-ga-ta na-lu₅ i₃-dab₅   iti <unk> bi₂-gu₇ mu en-unu₆-gal {d}inana unu{ki}ga ba-hun  1(u) la₂ 1(diš) \n",
      "English: 9 rams, 28th day, from Abba-saga, Nalu accepted; month: “Ubi-feast,” “Enunugal of Inanna in Uruk was installed;” (total:) 9 (rams). 1 rams. Šulgi. Foreman: Iddab. ARADmu, the\n",
      "---\n",
      "Generazione del testo...\n",
      "Testo di input: Sumerian:  3(diš) 1/2(diš) gin₂ 1(u) 5(diš) še ku₃-babbar ur₅-še₃ ur{d}en-lil₂-la₂-ta lugal-sa₆-ga u₃ ur{d}šu-mah šu ba-ti  iti ku₃ <unk> u₄ 2(u) 2(diš) ba-zal  mu si-ma-num₂{ki} ba-hul \n",
      "English:\n",
      "Traduzione effettiva: 3 1/2 shekels 15 grains of silver, for interest, from Ur-Enlila, Lugal-saga, and Ur-Šumaḫ, received. month: “KuŠIM,” the 22th day passed, year: “Simanum was destroyed.”\n",
      "Testo Generato: Sumerian:  3(diš) 1/2(diš) gin₂ 1(u) 5(diš) še ku₃-babbar ur₅-še₃ ur{d}en-lil₂-la₂-ta lugal-sa₆-ga u₃ ur{d}šu-mah šu ba-ti  iti ku₃ <unk> u₄ 2(u) 2(diš) ba-zal  mu si-ma-num₂{ki} ba-hul \n",
      "English: (For) 3 1/3 shekels 15 grains silver, (for) Ur-Enlila, Lugal-saga, and Ur-Šumaḫ received; month: “Kušing,” 22nd day\n",
      "---\n",
      "Generazione del testo...\n",
      "Testo di input: Sumerian:  6(diš) murgu₂ peš 2(geš₂) 4(u) 5(diš) {geš}umbin ma₂ 6(diš) {geš}u₃-suh₅ {geš}a-da-še₃ ki lu₂-kal-la-ta  mar-sa-aš kišib₃ lu₂-sa₆-i₃-zu mu hu-uh₂-nu-ri{ki} ba-hul  \n",
      "English:\n",
      "Traduzione effettiva: 6 date palm spines, 165 boat ribs(?), 6 pine trees for ada-planks, from Lukalla, to the boat house, under seal of Lu-sa-izu. year: “Ḫuḫnuri was destroyed.”\n",
      "Testo Generato: Sumerian:  6(diš) murgu₂ peš 2(geš₂) 4(u) 5(diš) {geš}umbin ma₂ 6(diš) {geš}u₃-suh₅ {geš}a-da-še₃ ki lu₂-kal-la-ta  mar-sa-aš kišib₃ lu₂-sa₆-i₃-zu mu hu-uh₂-nu-ri{ki} ba-hul  \n",
      "English: 6 ewes, 25 billy goats, full grown, 65 fat-tailed sheep, 60 billy goat kids, from Lukalla, did Lu-sa’izu receive; year: “Ḫuḫnuri was destroyed.” Lu\n",
      "---\n",
      "Generazione del testo...\n",
      "Testo di input: Sumerian:  1/3(diš) kuš gu₄ 1(diš) sa gu₄ 2(diš) kuš udu a i₃-ri₂-na 1(u) gin₂ še-gin₂  ...  mu si-ma-num₂{ki} \n",
      "English:\n",
      "Traduzione effettiva: 1/3 oxen hide, 1 (bundle of) oxen sinews, 2 sheep skins “(soaked) with madder,” 10 shekels of glue, year: “Simanum.”\n",
      "Testo Generato: Sumerian:  1/3(diš) kuš gu₄ 1(diš) sa gu₄ 2(diš) kuš udu a i₃-ri₂-na 1(u) gin₂ še-gin₂  ...  mu si-ma-num₂{ki} \n",
      "English: 1 jug wort, 1 billy goat, 2 jennies, suckling, new-borns; 10 shekels of silver, for Iramnum; year: “Simanum.” 20th (year). Shu-Suen, the king, Great-Stele erected. Shu-Mingira, scribe, son of Ur-mes(?). xxx . . .\n",
      "SIG-nu-muš-da dub-sar: šu-Muri₅, the chief accountant,\n",
      "---\n",
      "Generazione del testo...\n",
      "Testo di input: Sumerian:  da-da u₃-na-a-du₁₁ 3(gešʾu) sa gi <unk> giri₃-ni-i₃-sa₆-ra  he₂-na-ab-šum₂-mu    {d}...gi nita kal-ga lugal uri₅{ki}ma lugal an ub-da limmu₂-ba  ... ensi₂ umma{ki} <unk> zu \n",
      "English:\n",
      "Traduzione effettiva: To Dada, say: “1800 bundles of fire-reeds, to(?) Girini-isa, let him give.” Šulgi, strong man, the king of Ur, king of the four corners: Ur-Lisi, governor, of Umma, is your servant.\n",
      "Testo Generato: Sumerian:  da-da u₃-na-a-du₁₁ 3(gešʾu) sa gi <unk> giri₃-ni-i₃-sa₆-ra  he₂-na-ab-šum₂-mu    {d}...gi nita kal-ga lugal uri₅{ki}ma lugal an ub-da limmu₂-ba  ... ensi₂ umma{ki} <unk> zu \n",
      "English: To Dada speak: 30 bundles of fodder reed, via Inanna-Ṭabšumu, may he give! May he not delay! ...-gi, strong man, governor of Umma, is your servant. Ur-Lisi, the scribe, son of Ur-Dumuzi, is\n",
      "---\n",
      "Generazione del testo...\n",
      "Testo di input: Sumerian:  ...6(diš) geme₂ 3(ban₂) ...1(diš)-še₃ šu ur₃ zar₃ tab... ...ša₃ {geš}ma-nu ...da-da-ga kišib₃ lugal-e₂-mah-e iti še-kar-ra-gal₂{ga₂}la  mu en {d}nanna...e i₃-pa₃   lugal-e₂-mah... dub-sar dumu lugal-ku₃-ga-ni \n",
      "English:\n",
      "Traduzione effettiva: n + 6 female laborers, 3 ban2, for one day, to gather and pile up the sheaves, on the field Willow, under charge of Dadaga, under seal of Lugal-emaḫe; month: “Barley-stored-in-the-harbor,” year: “The en-priestess of Nanna was chosen by means of the goat.” Lugal-emaḫe, scribe, son of Lugal-kugani.\n",
      "Testo Generato: Sumerian:  ...6(diš) geme₂ 3(ban₂) ...1(diš)-še₃ šu ur₃ zar₃ tab... ...ša₃ {geš}ma-nu ...da-da-ga kišib₃ lugal-e₂-mah-e iti še-kar-ra-gal₂{ga₂}la  mu en {d}nanna...e i₃-pa₃   lugal-e₂-mah... dub-sar dumu lugal-ku₃-ga-ni \n",
      "English: n + 6 female laborers, 3 ban2 each, for 3 months, barley rations of the field Willow, ...; under seal of Lugal-emaḫe; month “Bar\n",
      "---\n",
      "Generazione del testo...\n",
      "Errore durante la generazione del testo: Input length of input_ids is 363, but `max_length` is set to 200. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.\n",
      "Generazione del testo...\n",
      "Testo di input: Sumerian:  1(u) 4(diš) gu₄ geš ki na-sa₆-ta e₂-u₆-e i₃-dab₅  iti ki-siki{d}nin-a-zu  \n",
      "English:\n",
      "Traduzione effettiva: 14 plough oxen from Nasa E’u’e received; month “Ki-siki of Ninazu.”\n",
      "Testo Generato: Sumerian:  1(u) 4(diš) gu₄ geš ki na-sa₆-ta e₂-u₆-e i₃-dab₅  iti ki-siki{d}nin-a-zu  \n",
      "English: 14 plough oxen from Nasa E’u’e accepted; month “kisiki of Ninazu,” year: “Kissing-Oxen was fashioned.” (total:) 14 oxen. ARAD-Ninazu herdsman. Sealed tablet of ARAD. Ninazua herdsman(?). ARAD(NENE). ARAD(-of-Uruk). Year : “... .” ARAD, herdsman of Ur-Šu-Suen(?). Sealed Tablet of ARad. ARad(of)-of-Ur.\n",
      "---\n",
      "Generazione del testo...\n",
      "Testo di input: Sumerian:  1(diš) am gu₄ 1(diš) amar <unk> am u₄ 2(u) 1(diš)-kam ki ab-ba-sa₆-ga-ta lu₂-dingir-ra  i₃-dab₅  iti šu-eš₅-ša mu {d}amar{d}suen lugal-e ur-bi₂-lum{ki} mu-hul  2(diš) \n",
      "English:\n",
      "Traduzione effettiva: 1 wild ox, 1 wild heifer calf, 21st day, from Abbasaga did Lu-dingir receive; month “šu’eša,” year: “Amar-Suen, the king, Urbilum destroyed.” (total:) 2 (cattle).\n",
      "Testo Generato: Sumerian:  1(diš) am gu₄ 1(diš) amar <unk> am u₄ 2(u) 1(diš)-kam ki ab-ba-sa₆-ga-ta lu₂-dingir-ra  i₃-dab₅  iti šu-eš₅-ša mu {d}amar{d}suen lugal-e ur-bi₂-lum{ki} mu-hul  2(diš) \n",
      "English: 1 bull calf, 1 full-grown billy, 21st day; from Abbasaga did Lu-dingira accept; month: “šu’ešša,” year “Amar-Suen, the king, Urbilum destroyed;” (total:) 2. 1. Amargaba\n",
      "---\n",
      "Generazione del testo...\n",
      "Testo di input: Sumerian:  2(geš₂) 2(u) 4(diš) gu₄ ab₂ hi-a 5(gešʾu) 8(geš₂) 3(u) 3(diš) udu maš₂ hi-a šu-šum₂-ma ki {d}en-lil₂-zi-ša₃-gal₂-ta ba-zi iti ezem{d}šu{d}suen  ...  ...   ...{d}suen ...ga ...uri₅...ma ...  ... ... \n",
      "English:\n",
      "Traduzione effettiva: 144 various oxen and cows, 3513 various sheep and goats, —the delivered— from (the account of) Enlil-zišagal booked out; month “Festival of Šu-Suen.” ... ... ... ... ... ...-Suen, strong king(?), king of Ur, king of the four regions: ... ... ... ...\n",
      "Testo Generato: Sumerian:  2(geš₂) 2(u) 4(diš) gu₄ ab₂ hi-a 5(gešʾu) 8(geš₂) 3(u) 3(diš) udu maš₂ hi-a šu-šum₂-ma ki {d}en-lil₂-zi-ša₃-gal₂-ta ba-zi iti ezem{d}šu{d}suen  ...  ...   ...{d}suen ...ga ...uri₅...ma ...  ... ... \n",
      "English: 144 oxen and cows, 35 sheep, barley-fed, from Enlil-zišagal did Bazi accept; month “Festival of Šu-Suen,” year: “Š\n",
      "---\n",
      "Generazione del testo...\n",
      "Testo di input: Sumerian:  2(u) {geš}il₂ kun-zi-da a-pi₄-sal₄{ki}še₃ geme₂ uš-bar-e {geš}il₂ ...ib₂-il₂ ki ku₃-ga-ni-ta  šeš-sag₁₀...ba-ti  mu {d}amar-suen lugal  šeš... dub... dumu lugal-gu₃... \n",
      "English:\n",
      "Traduzione effettiva: 20 corvée baskets: to the weir of Apisal, did the weaving female laborers the corvée baskets ... carry; from Kugani did Šeš-saga receive; year: “Amar-Suen is king.” Šeš-saga, scribe, son of Lugal-Gudea.\n",
      "Testo Generato: Sumerian:  2(u) {geš}il₂ kun-zi-da a-pi₄-sal₄{ki}še₃ geme₂ uš-bar-e {geš}il₂ ...ib₂-il₂ ki ku₃-ga-ni-ta  šeš-sag₁₀...ba-ti  mu {d}amar-suen lugal  šeš... dub... dumu lugal-gu₃... \n",
      "English: 20 shekels silver, from the account of Apisal to the debit account of Ušbar, the copper worker, from Kugani received; year: “Amar-Suen is king.” Šešsaga, scribe, son of Lugal-Gudea. �\n",
      "---\n",
      "Generazione del testo...\n",
      "Testo di input: Sumerian:  1(diš) udu-nita₂ bar-gal₂ ba-uš₂ ki lu₂{d}utu-ta kišib₃ lu₂-kal-la  iti pa₄-u₂-e  mu hu-uh₂-nu-ri{ki} ba-hul  lu₂-kal-la dub-sar dumu ur-e₁₁-e šuš₃ \n",
      "English:\n",
      "Traduzione effettiva: 1 ram, with fleece, slaughtered, from Lu-Utu. under seal of Lukalla; month “Pa’ue,” year: “Ḫuḫnuri was destroyed.” Lukalla, scribe, son of Ur-E’e, chief livestock administrator.\n",
      "Testo Generato: Sumerian:  1(diš) udu-nita₂ bar-gal₂ ba-uš₂ ki lu₂{d}utu-ta kišib₃ lu₂-kal-la  iti pa₄-u₂-e  mu hu-uh₂-nu-ri{ki} ba-hul  lu₂-kal-la dub-sar dumu ur-e₁₁-e šuš₃ \n",
      "English: 1 sheep, barley-fed, slaughtered, from Lu-Utu, under seal of Lukalla; month “Pa’u’e,” year: “Ḫuḫnuri was destroyed.” Lukalla, scribe, son of Ur-Enki, chief cattle manager\n",
      "---\n",
      "Generazione del testo...\n",
      "Errore durante la generazione del testo: Input length of input_ids is 235, but `max_length` is set to 200. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.\n",
      "Generazione del testo...\n",
      "Testo di input: Sumerian:  2(aš) 3(barig) 5(ban₂) 8(diš) 1/3(diš) sila₃ ninda šu gur zi-ga šu-nir ki in-sa₆-sa₆-ta kišib₃ ensi₂-ka  iti pa₄-u₂-e  mu {d}šu{d}suen lugal   ...šu{d}suen lugal kal-ga lugal uri₅{ki}ma lugal an ub-da limmu₂-ba  a-a-kal-la ensi₂ umma{ki} <unk> zu \n",
      "English:\n",
      "Traduzione effettiva: 2 gur 3 barig 5 ban2 8 1/3 sila3 ŠU-bread booked out for the emblem, from Insasa. under seal of the governor; month “Pa’ue,” year: “Šū-Suen is king.” Šū-Suen, strong king, king of Ur, king of the four regions: Ayakalla, governor of Umma, is your servant.\n",
      "Testo Generato: Sumerian:  2(aš) 3(barig) 5(ban₂) 8(diš) 1/3(diš) sila₃ ninda šu gur zi-ga šu-nir ki in-sa₆-sa₆-ta kišib₃ ensi₂-ka  iti pa₄-u₂-e  mu {d}šu{d}suen lugal   ...šu{d}suen lugal kal-ga lugal uri₅{ki}ma lugal an ub-da limmu₂-ba  a-a-kal-la ensi₂ umma{ki} <unk> zu \n",
      "English: 2 gur 3 barig, 5 ban2 8 1/2 sila3 Šu-g\n",
      "---\n",
      "Generazione del testo...\n",
      "Testo di input: Sumerian:  5(ban₂) 6(diš) sila₃ dabin u₄ 2(u) 8(diš)-kam ki lu₂-dingir-ra-ta kišib₃ ensi₂  iti dal  mu ma₂ {d}en-ki ba-ab-du₈   {d}šu{d}suen lugal kal-ga lugal uri₅{ki}ma lugal an ub-da limmu₂-ba  a-a-kal... ensi₂ umma{ki} <unk> zu \n",
      "English:\n",
      "Traduzione effettiva: 5 ban2 6 sila3 barley flour on the 28th day; from Lu-dingira. under seal of the governor; month “Flight,” year: “The boat of Enki was caulked.” Šū-Suen, strong king, king of Ur, king of the four regions: Ayakalla, the governor of Umma, is your servant.\n",
      "Testo Generato: Sumerian:  5(ban₂) 6(diš) sila₃ dabin u₄ 2(u) 8(diš)-kam ki lu₂-dingir-ra-ta kišib₃ ensi₂  iti dal  mu ma₂ {d}en-ki ba-ab-du₈   {d}šu{d}suen lugal kal-ga lugal uri₅{ki}ma lugal an ub-da limmu₂-ba  a-a-kal... ensi₂ umma{ki} <unk> zu \n",
      "English: 5 ban2 6 sila3 of dabin(-flour), on the 28th day, from Lu-dingira, under seal of the governor; month: “Flight,” year: �\n",
      "---\n",
      "Generazione del testo...\n",
      "Errore durante la generazione del testo: Input length of input_ids is 440, but `max_length` is set to 200. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.\n",
      "Generazione del testo...\n",
      "Errore durante la generazione del testo: Input length of input_ids is 203, but `max_length` is set to 200. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.\n",
      "Generazione del testo...\n",
      "Testo di input: Sumerian:  1(u) 5(diš) guruš u₄ 1(diš)-še₃ ka-giri₃-da bar-la₂ dub-la₂{d}utu gub-ba ugula lugal-iti-da  kišib₃ šeš-a-ni  mu {d}šu{d}suen lugal-e na-ru₂-mah mu-du₃  šeš-a-ni dub-sar dumu da-da \n",
      "English:\n",
      "Traduzione effettiva: (1) 15 male laborer days on duty at the basin of the Dubla-Utu (canal) stationed; foreman: Lugal-itida, under seal of Šešani; year: “Šū-Suen, the king, Big Stele erected.” Šešani, scribe, son of Dada.\n",
      "Testo Generato: Sumerian:  1(u) 5(diš) guruš u₄ 1(diš)-še₃ ka-giri₃-da bar-la₂ dub-la₂{d}utu gub-ba ugula lugal-iti-da  kišib₃ šeš-a-ni  mu {d}šu{d}suen lugal-e na-ru₂-mah mu-du₃  šeš-a-ni dub-sar dumu da-da \n",
      "English: 15 laborer workdays, at the threshing floor of the boatyard stationed, foreman: Lugal-itida, under seal of Šešani; year: “Šu-Suen, the king, Big Stele for Enmah built.” �\n",
      "---\n",
      "Generazione del testo...\n",
      "Testo di input: Sumerian:  1(u) gun₂ pa {geš}<unk> e₂-kikken₂ ba-ba sag₁₀ ki e₂-ur₂-bi-du₁₀-ta na-ba-sa₂ šu ba-ti  kišib₃ ku₃{d}nin-ur₄-ra ša₃ bala-a  mu {d}i-bi₂{d}suen lugal  ku₃{d}nin-ur₄-ra dub... dumu na... \n",
      "English:\n",
      "Traduzione effettiva: 10 talents branches of willow (for) the mill of fine porridge from E’urbidu did Naba-sa receive; under seal of Ku-Ninura. part of the bala; year: “Ibbi-Suen is king.” Ku-Ninura, scribe, son of Na-silim.\n",
      "Testo Generato: Sumerian:  1(u) gun₂ pa {geš}<unk> e₂-kikken₂ ba-ba sag₁₀ ki e₂-ur₂-bi-du₁₀-ta na-ba-sa₂ šu ba-ti  kišib₃ ku₃{d}nin-ur₄-ra ša₃ bala-a  mu {d}i-bi₂{d}suen lugal  ku₃{d}nin-ur₄-ra dub... dumu na... \n",
      "English: 10 talents of poplar leaves. From E’urbidu. Nabaš received. Sealed tablet of Ku-Ninura. Year: “Ibbi-Suen is king.�\n",
      "---\n",
      "Generazione del testo...\n",
      "Testo di input: Sumerian:  5(geš₂) 2(u) 5(diš) udu 4(u) 4(diš) sila₄ 2(geš₂) 2(u) 4(diš) maš₂-gal 1(u) 1(diš) maš₂ u₄ 3(diš)-kam ki ab-ba-sa₆-ga-ta \n",
      "English:\n",
      "Traduzione effettiva: 337 rams, 34 male lambs, 147 billy goats, 11 male kids, 3rd day, from Abbasaga did Inta’e’a accept; month “Festival-of-Šulgi,” year: “Šašrum was destroyed.” (total :) 529 small cattle.\n",
      "Testo Generato: Sumerian:  5(geš₂) 2(u) 5(diš) udu 4(u) 4(diš) sila₄ 2(geš₂) 2(u) 4(diš) maš₂-gal 1(u) 1(diš) maš₂ u₄ 3(diš)-kam ki ab-ba-sa₆-ga-ta \n",
      "English: 55 sheep, 44 ewes, 4 lambs, 21 nanny goats, 21st day, from Abbasaga did Inta’e’a accept; month “Piglet-feast,” year: “The priest of Puzriš-Dumuzida was installed.” (total:) 55 small cattle, 44 large cattle. Abba-saga, the cattle manager. Year: ”The priest\n",
      "---\n",
      "Generazione del testo...\n",
      "Testo di input: Sumerian:  2(diš) sila₄ ga 1(diš) kir₁₁ ga u₃-tu-da e₂-udu-ka  en-dingir-mu i₃-dab₅ u₄ 1(u) la₂ 1(diš)-kam  iti šu-eš-ša mu en-mah-gal-an-na en {d}nanna ba-hun \n",
      "English:\n",
      "Traduzione effettiva: 2 male lambs, suckling, 1 female lamb, suckling, new-borns in the sheephouse, Endingirmu accepted; 9th day; month: “šuešša,” year: “Enmaḫgalana, en-priestess of Nanna, was installed.”\n",
      "Testo Generato: Sumerian:  2(diš) sila₄ ga 1(diš) kir₁₁ ga u₃-tu-da e₂-udu-ka  en-dingir-mu i₃-dab₅ u₄ 1(u) la₂ 1(diš)-kam  iti šu-eš-ša mu en-mah-gal-an-na en {d}nanna ba-hun \n",
      "English: 2 suckling lambs, 1 suckling ewe lamb, newborns; Endingirmu accepted; 9th day; month: “šu’ešša,” year: ‟Enmaḫgalana, en-priestess of Nanna, was hired.“(’Seal of) Nanna.” (total:)\n",
      "---\n",
      "Generazione del testo...\n",
      "Testo di input: Sumerian:  1(geš₂) 2(u) 4(barig) še gur še geš e₃-a a-ša₃ u-gir₄{ki} ki ur{d}šara₂-ta  kišib₃ {d}šul-gi-mu-dah  iti nesag mu si-mu-ru-um{ki} ba-hul  {d}šara₂-kam dub-sar {d}šul-gi-mu-dah <unk> zu \n",
      "English:\n",
      "Traduzione effettiva: 80 gur 4 barig barley, barley winnowed with a stick, field of Ugir, from Ur-Šara Seal of Šulgi-mudaḫ Month: nesag year: “Simurum was destroyed.” Šara-kam, scribe: Šulgi-mudaḫ, your servant.\n",
      "Testo Generato: Sumerian:  1(geš₂) 2(u) 4(barig) še gur še geš e₃-a a-ša₃ u-gir₄{ki} ki ur{d}šara₂-ta  kišib₃ {d}šul-gi-mu-dah  iti nesag mu si-mu-ru-um{ki} ba-hul  {d}šara₂-kam dub-sar {d}šul-gi-mu-dah <unk> zu \n",
      "English: 144 gur 4 barig barley, fodder of grain-fed sheep of the ugir, from Ur-Šara, under seal of Šulgi-mudaḫ; month: “First-fruits,” year “Simur\n",
      "---\n",
      "Generazione del testo...\n",
      "Testo di input: Sumerian:  1(gešʾu) 3(u) 3(diš) ad₆(|LU₂.LAGAB×U|) udu maš₂ hi-a ki na-ra-am-i₃-li₂-ta ur-nigar{gar}  šu ba-ti  ...da₃-gu₇ ...us₂-sa ki-maš{ki}...ur₅-ti{ki} ba-hul \n",
      "English:\n",
      "Traduzione effettiva: 633 carcasses, various sheep and goats, from Naram-ilī did Ur-nigar receive; month “Gezelle-feast,” year after: “Kimaš and Ḫurti were destroyed.”\n",
      "Testo Generato: Sumerian:  1(gešʾu) 3(u) 3(diš) ad₆(|LU₂.LAGAB×U|) udu maš₂ hi-a ki na-ra-am-i₃-li₂-ta ur-nigar{gar}  šu ba-ti  ...da₃-gu₇ ...us₂-sa ki-maš{ki}...ur₅-ti{ki} ba-hul \n",
      "English: 1333 carcasses, various sheep, grass-fed, from Naram-ilī did Ur-niĝar receive; month “Gezelle-feast,” year after: “Kimaš and Ḫurti were destroyed.” Ur-Tidu, scribe, child of Ur-Šara\n",
      "---\n",
      "Generazione del testo...\n",
      "Testo di input: Sumerian:  pisan dub-ba gaba-ri kišib₃ lu₂ nig₂-dab₅-ke₄-ne ugu₂-a ga₂-ga₂-de₃ kišib₃-bi ur{d}nun-gal ...  ... ...gal₂ giri₃...{d}ba-ba₆ u₃...zi iti ezem{d}šu{d}suen ...{d}... \n",
      "English:\n",
      "Traduzione effettiva: Basket-of-tablets: copies, sealed documents, men of the takes, in the debits to be placed’s the sealed documents of Ur-Nungal, ... ... are here; via ...-Baba, and ...-zi, month “Festival-of-Šu-Suen,” year: “... .”\n",
      "Testo Generato: Sumerian:  pisan dub-ba gaba-ri kišib₃ lu₂ nig₂-dab₅-ke₄-ne ugu₂-a ga₂-ga₂-de₃ kišib₃-bi ur{d}nun-gal ...  ... ...gal₂ giri₃...{d}ba-ba₆ u₃...zi iti ezem{d}šu{d}suen ...{d}... \n",
      "English: Basket-of-tablets: sealed documents of conveyance, ... of Uruk-gave, sealed documents ... of Ur-Nungal, ... ... are here; via ...-Baba, and ...-zi, month: “Festival of Šu-Suen,” year: �\n",
      "---\n",
      "Generazione del testo...\n",
      "Errore durante la generazione del testo: Input length of input_ids is 273, but `max_length` is set to 200. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.\n",
      "Generazione del testo...\n",
      "Errore durante la generazione del testo: Input length of input_ids is 289, but `max_length` is set to 200. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.\n",
      "Generazione del testo...\n",
      "Testo di input: Sumerian:  pisan dub-ba ...<unk> <unk> ...<unk> ku₆ še ...{d}inanna-ta <unk> ...ab ...<unk> ...  ... ...ba ...{d}suen še...<unk> {geš}eb ...gal₂ ...1(diš) mu ...ka₃-li₂-šar₃-ri₂ ...geš-i₃ a-ga-de₃{ki} \n",
      "English:\n",
      "Traduzione effettiva: Basket-of-tablets: xxx xxx xxx xxx xxx\n",
      "Testo Generato: Sumerian:  pisan dub-ba ...<unk> <unk> ...<unk> ku₆ še ...{d}inanna-ta <unk> ...ab ...<unk> ...  ... ...ba ...{d}suen še...<unk> {geš}eb ...gal₂ ...1(diš) mu ...ka₃-li₂-šar₃-ri₂ ...geš-i₃ a-ga-de₃{ki} \n",
      "English: Basket-of-tablets: ... ... ... in the ‘house,’ ... Abi-iliya, ... ... and ... Baba ... ... are here; year: “... .” Abigail, ..., ...-going,” (a period of) 1 year ... . . . xxx xxx are here.\n",
      "---\n",
      "Generazione del testo...\n",
      "Testo di input: Sumerian:  pisan dub-ba im di-til-la {d}šara₂-kam ensi₂-ka  i₃-gal₂ mu ša-aš-ru{ki} ba-hul \n",
      "English:\n",
      "Traduzione effettiva: Basket-of-tablets: tablets, completed legal cases of Šarakam, the governor, are here; year: “Šašru was destroyed.”\n",
      "Testo Generato: Sumerian:  pisan dub-ba im di-til-la {d}šara₂-kam ensi₂-ka  i₃-gal₂ mu ša-aš-ru{ki} ba-hul \n",
      "English: Basket-of-tablets: tablets, judgments, of Šarakam, the governor, are here; year: “Šašru was destroyed.” are here. Šakkan, the scribe, son of Ur-Šara. Year: \"Šaszrum was destroyed\" (Amar-Suen 9). Ša’arum, the chief cattle manager. Year following: \"The Amorite wall was erected\" . Amar-Sin-dingir(š) ..., ... . household manager of ... . Year following that: \" ... was built\" . 1(aš)\n",
      "---\n",
      "Generazione del testo...\n",
      "Testo di input: Sumerian:  pisan dub-ba sag nig₂-gur₁₁ ma-ni ša₃ gir₂-su{ki} i₃-gal₂   mu ki-maš{ki} ba-hul \n",
      "English:\n",
      "Traduzione effettiva: Basket-of-tablets: debits, Mani, in Girsu, are here; year: “Kimaš was destroyed.”\n",
      "Testo Generato: Sumerian:  pisan dub-ba sag nig₂-gur₁₁ ma-ni ša₃ gir₂-su{ki} i₃-gal₂   mu ki-maš{ki} ba-hul \n",
      "English: Basket-of-tablets: debits and credits of Mani, in Girsu, are here; year: “Kimaš was destroyed.” are here. Mu, child of Ḫassan, animal fattener of Šara. Year:  “King KA-ḫassan built the Amorite wall(?).” Amar-Suen, strong man, the king of Ur, king of heaven with its four corners. Amar-Šu-il, scribe, son of Ur-mes(?). Amar-GIRgal, cattle manager. Amar Šu\n",
      "---\n",
      "Generazione del testo...\n",
      "Testo di input: Sumerian:  pisan dub-ba nig₂-ka₉ ak i-din-er₃-ra iti še-sag₁₁-ku₅-ta iti ezem{d}me-ki-gal₂-še₃ ...1(u) 2(diš)-kam  mu {d}šu{d}suen lugal uri₅{ki}ma-ke₄ ma-da za-ab-ša-li{ki} mu-hul i₃-gal₂ \n",
      "English:\n",
      "Traduzione effettiva: Basket-of-tablets: accounts of Idin-Erra, from month “Harvest” to month “Festival-of-Mekigal,” (a period of) 12 months, year: “Šu-Suen, king of Ur, the lands of Zabšali destroyed,” are here.\n",
      "Testo Generato: Sumerian:  pisan dub-ba nig₂-ka₉ ak i-din-er₃-ra iti še-sag₁₁-ku₅-ta iti ezem{d}me-ki-gal₂-še₃ ...1(u) 2(diš)-kam  mu {d}šu{d}suen lugal uri₅{ki}ma-ke₄ ma-da za-ab-ša-li{ki} mu-hul i₃-gal₂ \n",
      "English: Basket-of-tablets: accounts of Idin-Erra, from month “Harvest,” to (a period of) 12 months, year: “Šu-Suen, king of Ur, the lands of Zabš\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# load model from a checkpoint\n",
    "model = GPT2LMHeadModel.from_pretrained(f\"{OUTPUT_DIR}\").to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(f\"{OUTPUT_DIR}\")\n",
    "\n",
    "for index, row in test_data.iloc[:30,:].iterrows():\n",
    "    sumerian_texts = row['transliteration'].replace('\\n', ' ')\n",
    "    english_translations = row['translation'].replace('\\n', ' ')\n",
    "    prompt_text = f\"Sumerian: {sumerian_texts} \\nEnglish:\"\n",
    "\n",
    "    # --- Tokenizzazione dell'input ---\n",
    "    input_ids = tokenizer.encode(prompt_text, return_tensors='pt').to(device)\n",
    "\n",
    "    # --- Generazione del testo ---\n",
    "    print(\"Generazione del testo...\")\n",
    "    try:\n",
    "        output_sequences = model.generate(\n",
    "            input_ids=input_ids,    \n",
    "            max_length=200,             # Max length of prompt + generated text\n",
    "            temperature=0.2,            # Controls randomness. Lower is more deterministic.\n",
    "            top_k=40,                   # Considers the top K most probable tokens at each step.\n",
    "            top_p=0.9,                  # Nucleus sampling: considers tokens with cumulative probability >= P.\n",
    "            repetition_penalty=1,       # Penalizes repetition.\n",
    "            num_return_sequences=1,     # Number of different sequences to generate.\n",
    "            pad_token_id=tokenizer.eos_token_id, # Pad token ID for generation\n",
    "            no_repeat_ngram_size=3,     # Prevent 3-gram repetition\n",
    "            early_stopping=True,        # Stop when EOS is generated\n",
    "            length_penalty=1.0,         # Neutral - neither favor short nor long outputs\n",
    "            num_beams=3                 # Use beam search instead of sampling\n",
    "        )\n",
    "\n",
    "        print(f\"Testo di input: {prompt_text}\")\n",
    "        print(f\"Traduzione effettiva: {english_translations}\")\n",
    "        \n",
    "        # --- Decodifica e Stampa ---\n",
    "        for i, generated_sequence in enumerate(output_sequences):\n",
    "            text = tokenizer.decode(generated_sequence, skip_special_tokens=True)\n",
    "            print('Testo Generato:', text)\n",
    "            print('---')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Errore durante la generazione del testo: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d05237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking if the destination folder 'sumerian_gpt2_finetuned' already exists on OneDrive...\n",
      "Moving 'sumerian_gpt2_finetuned' to OneDrive: 'onedrive_bocconi:AI-project/sumerian_gpt2_finetuned'...\n",
      "rclone command: rclone move sumerian_gpt2_finetuned onedrive_bocconi:AI-project/sumerian_gpt2_finetuned --create-empty-src-dirs -P\n",
      "SUCCESS: Folder 'sumerian_gpt2_finetuned' moved successfully to OneDrive.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys  \n",
    "sys.path.insert(1, '../utils')\n",
    "\n",
    "from rclone import move_folder_to_onedrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ca2e933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking if the destination folder 'sumerian_gpt2_finetuned_logs' already exists on OneDrive...\n",
      "Moving 'sumerian_gpt2_finetuned_logs' to OneDrive: 'onedrive_bocconi:AI-project/sumerian_gpt2_finetuned_logs'...\n",
      "rclone command: rclone move sumerian_gpt2_finetuned_logs onedrive_bocconi:AI-project/sumerian_gpt2_finetuned_logs --create-empty-src-dirs -P\n",
      "SUCCESS: Folder 'sumerian_gpt2_finetuned_logs' moved successfully to OneDrive.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "move_folder_to_onedrive('sumerian_gpt2_finetuned_logs')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
